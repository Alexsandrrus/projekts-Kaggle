{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91721,"databundleVersionId":13760552,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/klyushnik/s5-e10-let-s-some-blend-0-05554?scriptVersionId=268024523\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:27:21.341224Z","iopub.execute_input":"2025-10-11T14:27:21.341597Z","iopub.status.idle":"2025-10-11T14:27:21.668153Z","shell.execute_reply.started":"2025-10-11T14:27:21.341572Z","shell.execute_reply":"2025-10-11T14:27:21.667228Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Welcome!","metadata":{}},{"cell_type":"markdown","source":"![mindtitan_artificial_intelligence_road_traffic_accident_prediction_model-1.jpg](https://mindtitan.com/wp-content/uploads/2021/06/mindtitan_artificial_intelligence_road_traffic_accident_prediction_model-1.jpg)","metadata":{}},{"cell_type":"markdown","source":"**Welcome to my code! Here I will present my vision and solution for the dataset, where the primary metric is Root Mean Squared Error.**\n\n**Enjoy watching, and please vote!**\n\n**My GitHub: https://github.com/Alexsandrrus!**","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport matplotlib\nfrom matplotlib.pyplot import figure\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.optimize import minimize\nfrom scipy.stats import mstats\nfrom scipy.stats.mstats import winsorize\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import (\n    LabelEncoder,\n    QuantileTransformer,\n    StandardScaler,\n    PowerTransformer,\n    MaxAbsScaler,\n    MinMaxScaler,\n    RobustScaler,\n    PolynomialFeatures,\n    OrdinalEncoder,\n    OneHotEncoder,\n    FunctionTransformer,\n    KBinsDiscretizer,\n)\nfrom sklearn.feature_selection import (\n    VarianceThreshold,\n    SelectKBest,\n    f_regression,\n    SequentialFeatureSelector,\n    SelectFromModel\n)\nfrom sklearn.model_selection import (\n    StratifiedKFold,\n    KFold,\n    StratifiedGroupKFold,\n    RepeatedStratifiedKFold,\n    RepeatedKFold,\n    cross_validate,\n    train_test_split,\n    TimeSeriesSplit,\n    cross_val_score\n)\nfrom sklearn.linear_model import (\n    SGDOneClassSVM,\n    LinearRegression,\n    Ridge,\n    Lasso,\n    ElasticNet,\n)\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import (\n    HistGradientBoostingRegressor,\n    ExtraTreesRegressor,\n    GradientBoostingRegressor,\n    IsolationForest,\n    BaggingRegressor,\n    RandomForestRegressor,\n    AdaBoostRegressor\n)\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error, \n    r2_score,\n    make_scorer\n)\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import clone_model\nimport keras\nfrom keras_tuner import RandomSearch\nfrom keras import layers\nfrom keras.layers import (\n    BatchNormalization,\n    Flatten,\n    Dense,\n    Dropout,\n    Activation,\n)\nfrom tensorflow.keras.models import Sequential\nfrom keras import backend as K\nimport keras_tuner\nfrom keras_tuner import Hyperband\nfrom functools import partial\n\nimport optuna\nfrom optuna.samplers import CmaEsSampler\nfrom optuna.pruners import MedianPruner\nimport optuna.visualization as vis\n\nfrom catboost import CatBoostRegressor\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingRegressor, StackingCVRegressor\nfrom category_encoders import TargetEncoder, MEstimateEncoder\n#from cuml.preprocessing import TargetEncoder\n\nimport requests\nimport holidays\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nfrom category_encoders import CatBoostEncoder, LeaveOneOutEncoder\n\nimport warnings\nimport re\nimport time\nimport logging\nfrom functools import partial\nfrom itertools import combinations\nfrom IPython.display import Image\n\nfrom functools import partial\n\n# Visualization settings\nplt.style.use('ggplot')\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (12, 8)\nsns.set_context(\"notebook\", font_scale=1.2)\nsns.set_style(\"whitegrid\")\n\n# Pandas settings\npd.options.mode.chained_assignment = None\n\n# Logging configuration\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Warnings configuration\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:27:21.669717Z","iopub.execute_input":"2025-10-11T14:27:21.670437Z","iopub.status.idle":"2025-10-11T14:27:46.942825Z","shell.execute_reply.started":"2025-10-11T14:27:21.670414Z","shell.execute_reply":"2025-10-11T14:27:46.941925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_numerical_features(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        sns.histplot(df[feature], bins=30, kde=True, ax=axes[i], color='skyblue', edgecolor='black')\n        axes[i].set_title(f'Distribution of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel(feature, fontsize=14)\n        axes[i].set_ylabel('Frequency', fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n\n        mean_value = df[feature].mean()\n        axes[i].axvline(mean_value, color='red', linestyle='--', label='Mean')\n        axes[i].legend()\n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_numerical_boxplots(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        sns.boxplot(x=df[feature], ax=axes[i], color='lightgreen')\n        axes[i].set_title(f'Boxplot of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel(feature, fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n\n        median_value = df[feature].median()\n        axes[i].axvline(median_value, color='orange', linestyle='--', label='Median')\n        axes[i].legend()\n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_qq_plot(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        stats.probplot(df[feature], dist=\"norm\", plot=axes[i])\n        axes[i].set_title(f'QQ Plot of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel('Theoretical Quantiles', fontsize=14)\n        axes[i].set_ylabel('Sample Quantiles', fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha= 0.7)  \n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_correlation_matrix(df, method='spearman'):\n    num_df = df.select_dtypes(include=[np.number])\n    \n    corr = num_df.corr(method=method)\n    plt.figure(figsize=(14, 10))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8}, linewidths=.5)\n    plt.title(f'Correlation Matrix ({method.capitalize()} Correlation)', fontsize=18, fontweight='bold')\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.show()\n\ndef plot_pairplot(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    sns.pairplot(df[num_features], diag_kind='kde', plot_kws={'alpha': 0.6, 'edgecolor': 'k'}, height=2.5)\n    plt.suptitle('Pairplot of Numerical Features', y=1.02, fontsize=18, fontweight='bold')\n    plt.show()\n\ndef plot_categorical_features(df, ncols=2, top_n=None):\n    cat_features = df.select_dtypes(include=[object]).columns\n    nrows = (len(cat_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(cat_features):\n        if top_n is not None:\n            top_categories = df[feature].value_counts().nlargest(top_n).index\n            sns.countplot(data=df[df[feature].isin(top_categories)], y=feature, ax=axes[i], palette='viridis', order=top_categories)\n        else:\n            sns.countplot(data=df, y=feature, ax=axes[i], palette='viridis')\n        \n        axes[i].set_title(f'Count of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel('Count', fontsize=14)\n        axes[i].set_ylabel(feature, fontsize=14)\n        axes[i].tick_params(axis='y', rotation=0)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n    for j in range(i + 1, len(axes)):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.show()\n\ndef PolynomialFeatures_labeled(input_df,power):\n   \n    poly = preprocessing.PolynomialFeatures(power)\n    output_nparray = poly.fit_transform(input_df)\n    powers_nparray = poly.powers_\n\n    input_feature_names = list(input_df.columns)\n    target_feature_names = [\"Constant Term\"]\n    for feature_distillation in powers_nparray[1:]:\n        intermediary_label = \"\"\n        final_label = \"\"\n        for i in range(len(input_feature_names)):\n            if feature_distillation[i] == 0:\n                continue\n            else:\n                variable = input_feature_names[i]\n                power = feature_distillation[i]\n                intermediary_label = \"%s+%d\" % (variable,power)\n                if final_label == \"\":         #If the final label isn't yet specified\n                    final_label = intermediary_label\n                else:\n                    final_label = final_label + \"x\" + intermediary_label\n        target_feature_names.append(final_label)\n    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n    return output_df\n\ndef variance_threshold(df,th):\n    var_thres=VarianceThreshold(threshold=th)\n    var_thres.fit(df)\n    new_cols = var_thres.get_support()\n    return df.iloc[:,new_cols]\n   \ndef optimize_memory_usage(df, print_size=True):\n    \"\"\"\n    Optimizes memory usage in a DataFrame by downcasting numeric columns.\n\n    Parameters:\n        df (pd.DataFrame): The DataFrame to optimize.\n        print_size (bool): If True, prints memory usage before and after optimization.\n\n    Returns:\n        pd.DataFrame: The optimized DataFrame.\n    \"\"\"\n    # Types for optimization.\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    \n    # Memory usage size before optimize (Mb).\n    before_size = df.memory_usage().sum() / 1024**2\n    \n    for column in df.columns:\n        column_type = df[column].dtype\n        \n        if column_type in numerics:\n            try:\n                if str(column_type).startswith('int'):\n                    df[column] = pd.to_numeric(df[column], downcast='integer')\n                else:\n                    df[column] = pd.to_numeric(df[column], downcast='float')\n                logger.info(f\"Optimized column {column}: {column_type} -> {df[column].dtype}\")\n            except Exception as e:\n                logger.error(f\"Failed to optimize column {column}: {e}\")\n    \n    # Memory usage size after optimize (Mb).\n    after_size = df.memory_usage().sum() / 1024**2\n    \n    if print_size:\n        print(\n            'Memory usage size: before {:5.4f} Mb - after {:5.4f} Mb ({:.1f}%).'.format(\n                before_size, after_size, 100 * (before_size - after_size) / before_size\n            )\n        )\n    \n    return df\n\ndef categorize_variable(df, column, labels):\n    \n    if len(labels) != 3:\n        raise ValueError(\"3 type\")\n    \n    bins = [-float('inf'), \n            df[column].quantile(0.25), \n            df[column].quantile(0.75), \n            float('inf')]\n    \n    df[f'{column}_group'] = pd.cut(df[column], bins=bins, labels=labels)\n    return df\n\ndef replace_outliers_with_mean(df, threshold=3):\n\n    df_clean = df.copy()\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    \n    for col in numeric_cols:\n        \n        z_scores = np.abs(stats.zscore(df[col], nan_policy='omit')) \n        \n        mean_val = df[col][z_scores <= threshold].mean()\n        \n        df_clean[col] = np.where(z_scores > threshold, mean_val, df[col])\n        \n    return df_clean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:27:46.94377Z","iopub.execute_input":"2025-10-11T14:27:46.944572Z","iopub.status.idle":"2025-10-11T14:27:46.974185Z","shell.execute_reply.started":"2025-10-11T14:27:46.944549Z","shell.execute_reply":"2025-10-11T14:27:46.973275Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s5e10/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e10/test.csv')\n\ndisplay(train.shape, test.shape)\ndisplay(train.info(), test.info())\n\ntest = test.drop(['id'], axis =1)\ntrain = train.drop(['id'], axis =1)\n\ndisplay(train.describe().T)\ndisplay(test.describe().T)\n\nduplicates = train.duplicated()\nprint(f\"Number of duplicates: {duplicates.sum()}\")\n\nduplicates = test.duplicated()\nprint(f\"Number of duplicates: {duplicates.sum()}\")\n\ntrain = train.drop_duplicates()\n\nfor col in train.columns:\n    pct_missing = np.mean(train[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\ndisplay(train.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:27:46.976301Z","iopub.execute_input":"2025-10-11T14:27:46.976568Z","iopub.status.idle":"2025-10-11T14:27:49.500798Z","shell.execute_reply.started":"2025-10-11T14:27:46.976548Z","shell.execute_reply":"2025-10-11T14:27:49.500098Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optimize memory","metadata":{}},{"cell_type":"code","source":"train = optimize_memory_usage(train)\ntest = optimize_memory_usage(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:27:49.501639Z","iopub.execute_input":"2025-10-11T14:27:49.501962Z","iopub.status.idle":"2025-10-11T14:27:49.584499Z","shell.execute_reply.started":"2025-10-11T14:27:49.501935Z","shell.execute_reply":"2025-10-11T14:27:49.583347Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## Numerical feature","metadata":{}},{"cell_type":"code","source":"plot_numerical_features(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:27:49.585544Z","iopub.execute_input":"2025-10-11T14:27:49.585869Z","iopub.status.idle":"2025-10-11T14:28:01.910626Z","shell.execute_reply.started":"2025-10-11T14:27:49.585841Z","shell.execute_reply":"2025-10-11T14:28:01.909425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_numerical_boxplots(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:01.911525Z","iopub.execute_input":"2025-10-11T14:28:01.911965Z","iopub.status.idle":"2025-10-11T14:28:03.398731Z","shell.execute_reply.started":"2025-10-11T14:28:01.911935Z","shell.execute_reply":"2025-10-11T14:28:03.397643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_qq_plot(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:03.400147Z","iopub.execute_input":"2025-10-11T14:28:03.400543Z","iopub.status.idle":"2025-10-11T14:28:10.974025Z","shell.execute_reply.started":"2025-10-11T14:28:03.400508Z","shell.execute_reply":"2025-10-11T14:28:10.972987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_pairplot(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:10.975257Z","iopub.execute_input":"2025-10-11T14:28:10.976047Z","iopub.status.idle":"2025-10-11T14:28:50.468671Z","shell.execute_reply.started":"2025-10-11T14:28:10.976022Z","shell.execute_reply":"2025-10-11T14:28:50.467792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_correlation_matrix(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:50.471751Z","iopub.execute_input":"2025-10-11T14:28:50.472056Z","iopub.status.idle":"2025-10-11T14:28:51.135829Z","shell.execute_reply.started":"2025-10-11T14:28:50.472036Z","shell.execute_reply":"2025-10-11T14:28:51.134972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = sm.add_constant(train.select_dtypes(include=[np.number]).iloc [:, 1:])\n\nVIFs = pd.DataFrame()\nVIFs['Variable'] = X.columns\nVIFs['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nprint(VIFs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:51.13688Z","iopub.execute_input":"2025-10-11T14:28:51.137267Z","iopub.status.idle":"2025-10-11T14:28:51.652112Z","shell.execute_reply.started":"2025-10-11T14:28:51.137246Z","shell.execute_reply":"2025-10-11T14:28:51.651255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Categorical features","metadata":{}},{"cell_type":"code","source":"plot_categorical_features(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:51.652941Z","iopub.execute_input":"2025-10-11T14:28:51.653242Z","iopub.status.idle":"2025-10-11T14:28:53.670755Z","shell.execute_reply.started":"2025-10-11T14:28:51.653217Z","shell.execute_reply":"2025-10-11T14:28:53.66977Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# New features","metadata":{}},{"cell_type":"code","source":"def create_features(df):\n    df = df.copy()\n    \n    df['traffic_intensity'] = df['num_lanes'] * df['speed_limit'] / 10\n    df['road_complexity'] = df['curvature'] * df['num_lanes']\n    df['speed_curve_risk'] = df['curvature'] * (df['speed_limit'] / 50)\n    \n    df['poor_visibility'] = ((df['weather'].isin(['rainy', 'foggy', 'snowy'])) & \n                            (df['lighting'].isin(['dim', 'dark']))).astype(int)\n    \n    df['high_speed_curve'] = ((df['speed_limit'] > 60) & (df['curvature'] > 0.7)).astype(int)\n    df['adverse_conditions'] = (df['poor_visibility'] | df['high_speed_curve']).astype(int)\n    \n    return df\n\ntrain = create_features(train)\ntest = create_features(test)\n\ndisplay(train.shape, test.shape)\ndisplay(train.info(), test.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:53.671738Z","iopub.execute_input":"2025-10-11T14:28:53.67201Z","iopub.status.idle":"2025-10-11T14:28:53.951714Z","shell.execute_reply.started":"2025-10-11T14:28:53.671992Z","shell.execute_reply":"2025-10-11T14:28:53.951058Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def encode_time(time_str):\n    time_mapping = {'morning': 6, 'afternoon': 12, 'evening': 18, 'night': 0}\n    time_val = time_mapping[time_str]\n    \n    sin_time = np.sin(2 * np.pi * time_val / 24)\n    cos_time = np.cos(2 * np.pi * time_val / 24)\n    return sin_time, cos_time\n\ntrain['time_sin'], train['time_cos'] = zip(*train['time_of_day'].apply(encode_time))\ntest['time_sin'], test['time_cos'] = zip(*test['time_of_day'].apply(encode_time))\n\ndisplay(train.shape, test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:53.952455Z","iopub.execute_input":"2025-10-11T14:28:53.952736Z","iopub.status.idle":"2025-10-11T14:28:57.583753Z","shell.execute_reply.started":"2025-10-11T14:28:53.952715Z","shell.execute_reply":"2025-10-11T14:28:57.58289Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_target_encoding(train_df, test_df, categorical_cols):\n    for col in categorical_cols:\n        target_mean = train_df.groupby(col)['accident_risk'].mean()\n        \n        train_df[f'{col}_target_enc'] = train_df[col].map(target_mean)\n        \n        test_df[f'{col}_target_enc'] = test_df[col].map(target_mean)\n        \n        overall_mean = train_df['accident_risk'].mean()\n        test_df[f'{col}_target_enc'].fillna(overall_mean, inplace=True)\n    \n    return train_df, test_df\n\ncategorical_cols = ['road_type', 'lighting', 'weather', 'time_of_day']\ntrain, test = add_target_encoding(train, test, categorical_cols)\n\ndisplay(train.shape, test.shape)\ndisplay(train.info(), test.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:57.58472Z","iopub.execute_input":"2025-10-11T14:28:57.585029Z","iopub.status.idle":"2025-10-11T14:28:58.068981Z","shell.execute_reply.started":"2025-10-11T14:28:57.58501Z","shell.execute_reply":"2025-10-11T14:28:58.068259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_statistical_features(train_df, test_df, group_cols):\n    for col in group_cols:\n        stats = train_df.groupby(col).agg({\n            'accident_risk': ['mean', 'std', 'count']\n        }).round(4)\n        stats.columns = [f'{col}_risk_mean', f'{col}_risk_std', f'{col}_count']\n        stats = stats.reset_index()\n        \n        train_df = train_df.merge(stats, on=col, how='left')\n        \n        test_df = test_df.merge(stats, on=col, how='left')\n        \n        overall_stats = train_df['accident_risk'].agg(['mean', 'std']).values\n        test_df[f'{col}_risk_mean'].fillna(overall_stats[0], inplace=True)\n        test_df[f'{col}_risk_std'].fillna(overall_stats[1], inplace=True)\n        test_df[f'{col}_count'].fillna(len(train_df), inplace=True)\n    \n    return train_df, test_df\n\ngroup_cols = ['road_type', 'weather', 'lighting', 'num_lanes']\ntrain, test = add_statistical_features(train, test, group_cols)\n\ndisplay(train.shape, test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:58.069891Z","iopub.execute_input":"2025-10-11T14:28:58.070184Z","iopub.status.idle":"2025-10-11T14:28:58.792355Z","shell.execute_reply.started":"2025-10-11T14:28:58.070155Z","shell.execute_reply":"2025-10-11T14:28:58.791602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_interaction_features(df):\n    df = df.copy()\n    \n    df['road_weather'] = df['road_type'] + '_' + df['weather']\n    df['lighting_weather'] = df['lighting'] + '_' + df['weather']\n    df['time_weather'] = df['time_of_day'] + '_' + df['weather']\n    \n    df['lanes_speed_interaction'] = df['num_lanes'] * df['speed_limit']\n    df['curve_speed_ratio'] = df['curvature'] / (df['speed_limit'] + 1)  # +1 чтобы избежать деления на 0\n    \n    return df\n\ntrain = add_interaction_features(train)\ntest = add_interaction_features(test)\n\ndisplay(train.shape, test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:58.793203Z","iopub.execute_input":"2025-10-11T14:28:58.793445Z","iopub.status.idle":"2025-10-11T14:28:59.295162Z","shell.execute_reply.started":"2025-10-11T14:28:58.793425Z","shell.execute_reply":"2025-10-11T14:28:59.294416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_binned_features(df):\n    df = df.copy()\n    \n    speed_bins = [0, 30, 50, 60, 70]\n    speed_labels = ['low', 'medium', 'high', 'very_high']\n    df['speed_binned'] = pd.cut(df['speed_limit'], bins=speed_bins, labels=speed_labels)\n    \n    curvature_bins = [0, 0.3, 0.6, 1.0]\n    curvature_labels = ['straight', 'moderate', 'curvy']\n    df['curvature_binned'] = pd.cut(df['curvature'], bins=curvature_bins, labels=curvature_labels)\n    \n    return df\n\ntrain = add_binned_features(train)\ntest = add_binned_features(test)\n\ndisplay(train.shape, test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:59.296155Z","iopub.execute_input":"2025-10-11T14:28:59.29648Z","iopub.status.idle":"2025-10-11T14:28:59.523764Z","shell.execute_reply.started":"2025-10-11T14:28:59.296453Z","shell.execute_reply":"2025-10-11T14:28:59.523088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The final dataset","metadata":{}},{"cell_type":"code","source":"train_cols = set(train.columns)\ntest_cols = set(test.columns)\n\nprint(\"Columns in test but not in train:\", test_cols - train_cols)\nprint(\"Columns in train but not in test:\", train_cols - test_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:59.524414Z","iopub.execute_input":"2025-10-11T14:28:59.524609Z","iopub.status.idle":"2025-10-11T14:28:59.529858Z","shell.execute_reply.started":"2025-10-11T14:28:59.524594Z","shell.execute_reply":"2025-10-11T14:28:59.52903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:59.530833Z","iopub.execute_input":"2025-10-11T14:28:59.531224Z","iopub.status.idle":"2025-10-11T14:28:59.56195Z","shell.execute_reply.started":"2025-10-11T14:28:59.531199Z","shell.execute_reply":"2025-10-11T14:28:59.561178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:59.56296Z","iopub.execute_input":"2025-10-11T14:28:59.563289Z","iopub.status.idle":"2025-10-11T14:28:59.797417Z","shell.execute_reply.started":"2025-10-11T14:28:59.56326Z","shell.execute_reply":"2025-10-11T14:28:59.796451Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"object_category_cols = train.select_dtypes(include=['object', 'category']).columns.tolist()\nprint(object_category_cols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:59.798426Z","iopub.execute_input":"2025-10-11T14:28:59.798714Z","iopub.status.idle":"2025-10-11T14:28:59.840328Z","shell.execute_reply.started":"2025-10-11T14:28:59.798687Z","shell.execute_reply":"2025-10-11T14:28:59.83941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in object_category_cols:\n        le = LabelEncoder()\n        combined = pd.concat([train[col], test[col]], axis=0)\n        le.fit(combined)\n        \n        train[col] = le.transform(train[col])\n        test[col] = le.transform(test[col])\n\ndisplay(train.shape, test.shape)\ndisplay(train.info(), test.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:28:59.841249Z","iopub.execute_input":"2025-10-11T14:28:59.841592Z","iopub.status.idle":"2025-10-11T14:29:01.209805Z","shell.execute_reply.started":"2025-10-11T14:28:59.841572Z","shell.execute_reply":"2025-10-11T14:29:01.208957Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Split data & threshold","metadata":{}},{"cell_type":"code","source":"X = train.drop(columns=['accident_risk'])\ny = train['accident_risk']\n\n#not today\nX = variance_threshold(X,0.01)\nlist_name = (X.columns)\ntest = test[list_name]\n\ndisplay(X.shape, y.shape, test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:29:01.210733Z","iopub.execute_input":"2025-10-11T14:29:01.211029Z","iopub.status.idle":"2025-10-11T14:29:01.678368Z","shell.execute_reply.started":"2025-10-11T14:29:01.21101Z","shell.execute_reply":"2025-10-11T14:29:01.677479Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Optuna","metadata":{}},{"cell_type":"code","source":"def optimize_catboost_regression(X, y, n_trials=40, cv=5):\n  \n    def rmse_scorer(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n    \n    rmse_score = make_scorer(rmse_scorer, greater_is_better=False)\n    \n    def objective(trial):\n        params = {\n            'iterations': trial.suggest_int('iterations', 100, 1000),\n            'depth': trial.suggest_int('depth', 4, 10),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n            'border_count': trial.suggest_int('border_count', 32, 255),\n            'random_strength': trial.suggest_float('random_strength', 0, 2),\n            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n            'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise']),\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n            'loss_function': 'RMSE',\n            'eval_metric': 'RMSE',\n            'task_type': 'GPU', \n            'verbose': False,\n            'early_stopping_rounds': 100\n        }\n\n        model = CatBoostRegressor(**params)\n        \n        \n        scores = cross_val_score(model, X, y, cv=cv, \n                               scoring=rmse_score, n_jobs=1)\n        \n       \n        return scores.mean()\n    \n    study = optuna.create_study(direction='maximize')  \n    study.optimize(objective, n_trials=n_trials)\n    \n    return study\n\ncatboost_studies = []\nfor i in range(3):\n    print(f\"\\nRunning CatBoost Regression optimization {i+1}/3\")\n    study = optimize_catboost_regression(X, y, n_trials=40)\n    catboost_studies.append(study)\n    print(f\"Best trial {i+1}:\")\n    print(f\"  Value (-RMSE): {study.best_value:.5f}\")\n    print(f\"  Actual RMSE: {-study.best_value:.5f}\")  \n    print(f\"  Params: {study.best_params}\")\n\n\ncatboost_rmse_params = []\n\nfor i, study in enumerate(catboost_studies):\n    params = study.best_params.copy()\n    params['loss_function'] = 'RMSE'\n    params['eval_metric'] = 'RMSE'\n    params['verbose'] = False\n    catboost_rmse_params.append(params)\n    print(f\"\\nBest parameters for model {i+1}:\")\n    for key, value in params.items():\n        print(f\"  {key}: {value}\")\n\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"OPTIMIZATION SUMMARY\")\nprint(\"=\"*50)\nfor i, (study, params) in enumerate(zip(catboost_studies, catboost_rmse_params)):\n    print(f\"Model {i+1}: RMSE = {-study.best_value:.5f}\")\n\nconfig_0 = {\n  'iterations': 537,\n  'depth': 8,\n  'learning_rate': 0.017842162028402962,\n  'l2_leaf_reg': 7.370412244865911,\n  'border_count': 191,\n  'random_strength': 1.1087121186242526,\n  'bagging_temperature': 0.6239080010677365,\n  'grow_policy': 'Depthwise',\n  'min_data_in_leaf': 27,\n  'loss_function': 'RMSE',\n  'eval_metric': 'RMSE',\n  'verbose': False\n}\n\nconfig_1 = {\n  'iterations': 950,\n  'depth': 8,\n  'learning_rate': 0.018571742536927362,\n  'l2_leaf_reg': 4.524357457928256,\n  'border_count': 153,\n  'random_strength': 0.4862818740678978,\n  'bagging_temperature': 0.4014679810273516,\n  'grow_policy': 'Depthwise',\n  'min_data_in_leaf': 25,\n  'loss_function': 'RMSE',\n  'eval_metric': 'RMSE',\n  'verbose': False\n}\n\nconfig_2 = {\n  'iterations': 301,\n  'depth': 8,\n  'learning_rate': 0.04484782966119428,\n  'l2_leaf_reg': 2.8119862594836165,\n  'border_count': 102,\n  'random_strength': 0.8375799223678608,\n  'bagging_temperature': 0.874858527606345,\n  'grow_policy': 'Depthwise',\n  'min_data_in_leaf': 3,\n  'loss_function': 'RMSE',\n  'eval_metric': 'RMSE',\n  'verbose': False\n\n}\n\nconfig_3 = {\n    \n  'iterations': 180,\n  'depth': 8,\n  'learning_rate': 0.06730149449365055,\n  'l2_leaf_reg': 7.448233402386369,\n  'border_count': 131,\n  'random_strength': 0.5274500030545808,\n  'bagging_temperature': 0.5049369701743718,\n  'grow_policy': 'Depthwise',\n  'min_data_in_leaf': 47,\n  'loss_function': 'RMSE',\n  'eval_metric': 'RMSE',\n  'verbose': False\n    \n}\n\ncatboost_rmse_params.append(config_0)\ncatboost_rmse_params.append(config_1)\ncatboost_rmse_params.append(config_2)\ncatboost_rmse_params.append(config_3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize_xgboost_regression(X, y, n_trials=40, cv=5):\n   \n    def rmse_scorer(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n    \n    rmse_score = make_scorer(rmse_scorer, greater_is_better=False)\n    \n    def objective(trial):\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n            'gamma': trial.suggest_float('gamma', 0, 1),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 2),\n            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n            'max_delta_step': trial.suggest_int('max_delta_step', 0, 5),\n            'eval_metric': 'rmse',\n            'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n            'max_leaves': trial.suggest_int('max_leaves', 32, 256),\n            'max_bin': trial.suggest_int('max_bin', 128, 256),\n            'tree_method': 'gpu_hist',\n            'predictor': 'gpu_predictor',\n            'sampling_method': trial.suggest_categorical('sampling_method', ['uniform', 'gradient_based'])\n        }\n        \n        model = xgb.XGBRegressor(**params)\n        \n        scores = cross_val_score(model, X, y, cv=cv, \n                               scoring=rmse_score, n_jobs=1)\n        \n        return scores.mean()\n    \n    study = optuna.create_study(direction='maximize')  \n    study.optimize(objective, n_trials=n_trials)\n    \n    return study\n\n\nxgb_studies = []\nfor i in range(3):\n    print(f\"\\nRunning XGBoost Regression optimization {i+1}/3\")\n    study = optimize_xgboost_regression(X, y, n_trials=40)\n    xgb_studies.append(study)\n    print(f\"Best trial {i+1}:\")\n    print(f\"  Value (-RMSE): {study.best_value:.5f}\")\n    print(f\"  Actual RMSE: {-study.best_value:.5f}\")  \n    print(f\"  Params: {study.best_params}\")\n\nxgb_rmse_params = []\nfor i, study in enumerate(xgb_studies):\n    params = study.best_params.copy()\n    params.update({\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'eval_metric': 'rmse'\n    })\n    xgb_rmse_params.append(params)\n    print(f\"\\nXGBoost config {i+1}:\")\n    for key, value in params.items():\n        print(f\"  {key}: {value}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"XGBOOST OPTIMIZATION SUMMARY\")\nprint(\"=\"*50)\nfor i, (study, params) in enumerate(zip(xgb_studies, xgb_rmse_params)):\n    print(f\"Model {i+1}: RMSE = {-study.best_value:.5f}\")\n\nconfig_0 = {\n    \n  'n_estimators': 760,\n  'max_depth': 10,\n  'learning_rate': 0.013795917955898896,\n  'subsample': 0.7267734213015471,\n  'colsample_bytree': 0.711424372302688,\n  'gamma': 0.004090103222241464,\n  'min_child_weight': 8,\n  'reg_lambda': 0.3471752674396913,\n  'reg_alpha': 0.6283614659091622,\n  'max_delta_step': 2,\n  'grow_policy': 'lossguide',\n  'max_leaves': 215,\n  'max_bin': 243,\n  'sampling_method': 'gradient_based',\n  'tree_method': 'gpu_hist',\n  'predictor': 'gpu_predictor',\n  'eval_metric': 'rmse',\n    \n}\n\nconfig_2 = {\n    \n  'n_estimators': 396,\n  'max_depth': 12,\n  'learning_rate': 0.02054786311244503,\n  'subsample': 0.8006661157443362,\n  'colsample_bytree': 0.926330943646608,\n  'gamma': 0.020881547833851988,\n  'min_child_weight': 2,\n  'reg_lambda': 0.9947304284610006,\n  'reg_alpha': 0.8364558915124849,\n  'max_delta_step': 5,\n  'grow_policy': 'lossguide',\n  'max_leaves': 148,\n  'max_bin': 219,\n  'sampling_method': 'uniform',\n  'tree_method': 'gpu_hist',\n  'predictor': 'gpu_predictor',\n  'eval_metric': 'rmse',\n    \n}\n\nconfig_2 = {\n    \n  'n_estimators': 551,\n  'max_depth': 6,\n  'learning_rate': 0.0503127818403867,\n  'subsample': 0.6017365244797307,\n  'colsample_bytree': 0.7777128943498162,\n  'gamma': 0.0018954277087720513,\n  'min_child_weight': 2,\n  'reg_lambda': 0.8471823203750417,\n  'reg_alpha': 0.6262587504812429,\n  'max_delta_step': 5,\n  'grow_policy': 'depthwise',\n  'max_leaves': 59,\n  'max_bin': 130,\n  'sampling_method': 'gradient_based',\n  'tree_method': 'gpu_hist',\n  'predictor': 'gpu_predictor',\n  'eval_metric': 'rmse',\n}\n\nconfig_3 = {\n\n  'n_estimators': 328,\n  'max_depth': 11,\n  'learning_rate': 0.031634147945890305,\n  'subsample': 0.6900450298316714,\n  'colsample_bytree': 0.7809160542619278,\n  'gamma': 0.0005155545903676355,\n  'min_child_weight': 7,\n  'reg_lambda': 1.6398728691452662,\n  'reg_alpha': 0.4243594149698391,\n  'max_delta_step': 2,\n  'grow_policy': 'depthwise',\n  'max_leaves': 197,\n  'max_bin': 169,\n  'sampling_method': 'uniform',\n  'tree_method': 'gpu_hist',\n  'predictor': 'gpu_predictor',\n  'eval_metric': 'rmse',\n}\n\nxgb_rmse_params.append(config_0)\nxgb_rmse_params.append(config_1)\nxgb_rmse_params.append(config_2)\nxgb_rmse_params.append(config_3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize_lightgbm_regression(X, y, n_trials=40, cv=5):\n    \n    def rmse_scorer(y_true, y_pred):\n        return np.sqrt(mean_squared_error(y_true, y_pred))\n    \n    rmse_score = make_scorer(rmse_scorer, greater_is_better=False)\n    \n    def objective(trial):\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n            'num_leaves': trial.suggest_int('num_leaves', 20, 128),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n            'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 0.1),\n            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 0, 10),\n            'lambda_l1': trial.suggest_float('lambda_l1', 0, 1),\n            'lambda_l2': trial.suggest_float('lambda_l2', 0, 1),\n            'min_split_gain': trial.suggest_float('min_split_gain', 0, 0.2),\n            'path_smooth': trial.suggest_float('path_smooth', 0, 1),\n            'max_bin': trial.suggest_int('max_bin', 64, 255),\n            'extra_trees': trial.suggest_categorical('extra_trees', [True, False]),\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'objective': 'regression',\n            'metric': 'rmse',\n            'verbose': -1\n        }\n        \n        model = LGBMRegressor(**params)\n        \n        scores = cross_val_score(model, X, y, cv=cv, \n                               scoring=rmse_score, n_jobs=1)\n        \n        return scores.mean()\n    \n    study = optuna.create_study(direction='maximize') \n    study.optimize(objective, n_trials=n_trials)\n    \n    return study\n\n\nlgbm_studies = []\nfor i in range(3):\n    print(f\"\\nRunning LightGBM Regression optimization {i+1}/3\")\n    study = optimize_lightgbm_regression(X, y, n_trials=40)\n    lgbm_studies.append(study)\n    print(f\"Best trial {i+1}:\")\n    print(f\"  Value (-RMSE): {study.best_value:.5f}\")\n    print(f\"  Actual RMSE: {-study.best_value:.5f}\")  \n    print(f\"  Params: {study.best_params}\")\n\n\nlgbm_rmse_params = []\nfor i, study in enumerate(lgbm_studies):\n    params = study.best_params.copy()\n    params.update({\n        'device': 'gpu',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'verbose': -1\n    })\n    lgbm_rmse_params.append(params)\n    print(f\"\\nLightGBM config {i+1}:\")\n    for key, value in params.items():\n        print(f\"  {key}: {value}\")\n\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"LIGHTGBM OPTIMIZATION SUMMARY\")\nprint(\"=\"*50)\nfor i, (study, params) in enumerate(zip(lgbm_studies, lgbm_rmse_params)):\n    print(f\"Model {i+1}: RMSE = {-study.best_value:.5f}\")\n\nconfig_0 = {\n    \n  'n_estimators': 475,\n  'max_depth': 6,\n  'learning_rate': 0.02609566703640587,\n  'num_leaves': 126,\n  'min_child_samples': 10,\n  'min_child_weight': 0.08696250243662902,\n  'feature_fraction': 0.6040602458875166,\n  'bagging_fraction': 0.8208724518623647,\n  'bagging_freq': 3,\n  'lambda_l1': 0.361298537906501,\n  'lambda_l2': 0.3227120302947219,\n  'min_split_gain': 0.0001271057985094303,\n  'path_smooth': 0.02245717701896824,\n  'max_bin': 243,\n  'extra_trees': False,\n  'device': 'gpu',\n  'objective': 'regression',\n  'metric': 'rmse',\n  'verbose': -1,\n\n}\n\nconfig_1 = {\n    \n  'n_estimators': 729,\n  'max_depth': 11,\n  'learning_rate': 0.019579347467589887,\n  'num_leaves': 51,\n  'min_child_samples': 36,\n  'min_child_weight': 0.07029393408032038,\n  'feature_fraction': 0.8046195627507132,\n  'bagging_fraction': 0.9548143084700726,\n  'bagging_freq': 3,\n  'lambda_l1': 0.08652123482408915,\n  'lambda_l2': 0.11178179927007909,\n  'min_split_gain': 0.000188284436315936,\n  'path_smooth': 0.5054225516315782,\n  'max_bin': 182,\n  'extra_trees': False,\n  'device': 'gpu',\n  'objective': 'regression',\n  'metric': 'rmse',\n  'verbose': -1\n\n}\n\nconfig_2 = {\n    \n  'n_estimators': 830,\n  'max_depth': 10,\n  'learning_rate': 0.032717852791327814,\n  'num_leaves': 58,\n  'min_child_samples': 29,\n  'min_child_weight': 0.022821245946439715,\n  'feature_fraction': 0.9419990343992585,\n  'bagging_fraction': 0.9090572699720074,\n  'bagging_freq': 6,\n  'lambda_l1': 0.16396068014180826,\n  'lambda_l2': 0.6153088612059665,\n  'min_split_gain': 0.000978174955847403,\n  'path_smooth': 0.7739653148026253,\n  'max_bin': 114,\n  'extra_trees': False,\n  'device': 'gpu',\n  'objective': 'regression',\n  'metric': 'rmse',\n  'verbose': -1\n\n}\n\nconfig_3 = {\n\n  'n_estimators': 475,\n  'max_depth': 6,\n  'learning_rate': 0.02609566703640587,\n  'num_leaves': 126,\n  'min_child_samples': 10,\n  'min_child_weight': 0.08696250243662902,\n  'feature_fraction': 0.6040602458875166,\n  'bagging_fraction': 0.8208724518623647,\n  'bagging_freq': 3,\n  'lambda_l1': 0.361298537906501,\n  'lambda_l2': 0.3227120302947219,\n  'min_split_gain': 0.0001271057985094303,\n  'path_smooth': 0.02245717701896824,\n  'max_bin': 243,\n  'extra_trees': False,\n  'device': 'gpu',\n  'objective': 'regression',\n  'metric': 'rmse',\n  'verbose': -1,\n}\n\nlgbm_rmse_params.append(config_0)\nlgbm_rmse_params.append(config_1)\nlgbm_rmse_params.append(config_2)\nlgbm_rmse_params.append(config_3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"gb_params_list = [\n        {'n_estimators': 200, 'learning_rate': 0.1, 'max_depth': 5, \n         'random_state': 42, 'subsample': 0.8},\n        {'n_estimators': 300, 'learning_rate': 0.05, 'max_depth': 7, \n         'random_state': 42, 'subsample': 0.9},\n        {'n_estimators': 150, 'learning_rate': 0.2, 'max_depth': 3, \n         'random_state': 42, 'subsample': 0.7},\n       {'n_estimators': 100, 'learning_rate': 0.02, 'max_depth': 2, \n         'random_state': 42, 'subsample': 0.7}\n    ]\n\net_params_list = [\n        {'n_estimators': 200, 'max_depth': 15, 'min_samples_split': 5, \n         'random_state': 42, 'n_jobs': -1},\n        {'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 2, \n         'random_state': 42, 'n_jobs': -1},\n        {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 2, \n         'random_state': 42, 'n_jobs': -1}\n    ]\n\nhgb_params_list = [\n        {'max_iter': 200, 'learning_rate': 0.1, 'max_depth': 10, \n         'random_state': 42, 'l2_regularization': 0.1},\n        {'max_iter': 300, 'learning_rate': 0.05, 'max_depth': 15, \n         'random_state': 42, 'l2_regularization': 0.01},\n        {'max_iter': 100, 'learning_rate': 0.1, 'max_depth': 5, \n         'random_state': 42, 'l2_regularization': 0.01}\n    ]\n\nada_params_list = [\n        {'n_estimators': 100, 'learning_rate': 0.1, 'random_state': 42},\n        {'n_estimators': 200, 'learning_rate': 0.05, 'random_state': 42},\n        {'n_estimators': 300, 'learning_rate': 0.01, 'random_state': 42}\n    ]\n\nbagging_params = [\n        {'n_estimators': 20, 'random_state': 42, 'n_jobs': -1},\n        {'n_estimators': 50, 'random_state': 42, 'n_jobs': -1},\n        {'n_estimators': 100, 'random_state': 42, 'n_jobs': -1}\n    ]\n\nlinear_models = [\n        ('ridge', Ridge(alpha=1.0, random_state=42)),\n        ('lasso', Lasso(alpha=0.1, random_state=42)),\n        ('elastic', ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42))\n    ]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_ensemble(X, y, test, n_folds=5):\n    FOLDS = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    all_oof = {}\n    all_predictions = {}\n    models = []\n\n    for i, params in enumerate(catboost_rmse_params, 1):\n        models.append((f'cat_{i}', CatBoostRegressor(**params)))\n    \n    for i, params in enumerate(xgb_rmse_params, 1):\n        models.append((f'xgb_{i}', xgb.XGBRegressor(**params, n_jobs=1)))\n    \n    for i, params in enumerate(lgbm_rmse_params, 1):\n        models.append((f'lgb_{i}', LGBMRegressor(**params, n_jobs=1)))\n\n    for i, params in enumerate(gb_params_list, 1):\n        models.append((f'gb_{i}', GradientBoostingRegressor(**params)))\n\n    for i, params in enumerate(et_params_list, 1):\n        models.append((f'et_{i}', ExtraTreesRegressor(**params)))\n\n    for i, params in enumerate(hgb_params_list, 1):\n        models.append((f'hgb_{i}', HistGradientBoostingRegressor(**params)))\n\n    for i, params in enumerate(ada_params_list, 1):\n        models.append((f'ada_{i}', AdaBoostRegressor(**params)))\n\n    for i, params in enumerate(bagging_params, 1):\n        models.append((f'bag_{i}', BaggingRegressor(**params)))\n\n    for name, model in linear_models:\n        models.append((f'linear_{name}', model))\n    \n    for name, model in models:\n        try:\n            print(f\"\\nTraining {name}...\")\n            oof = np.zeros(len(X))\n            pred = np.zeros(len(test))\n            \n            for fold, (trn_idx, val_idx) in enumerate(FOLDS.split(X, y)):\n                X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n                X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n                \n                model.fit(X_train, y_train)\n                oof[val_idx] = model.predict(X_val)\n                pred += model.predict(test) / FOLDS.n_splits\n                \n                fold_rmse = np.sqrt(mean_squared_error(y_val, oof[val_idx]))\n                print(f'{name} - Fold {fold} RMSE: {fold_rmse:.4f}')\n            \n            all_oof[name] = oof\n            all_predictions[name] = pred\n            \n            full_rmse = np.sqrt(mean_squared_error(y, oof))\n            print(f'{name} - Full OOF RMSE: {full_rmse:.4f}')\n            \n        except Exception as e:\n            print(f\"Error training {name}: {str(e)}\")\n            continue\n    \n    oof_df = pd.DataFrame(all_oof)\n    predictions_df = pd.DataFrame(all_predictions)\n    \n    oof_df['target'] = y.values\n    \n    model_info = {\n        'model_names': [name for name, _ in models],\n        'num_models': len(models),\n        'features_used': list(X.columns),\n        'used_log_transform': False\n    }\n    \n    return oof_df, predictions_df, model_info\n\noof_results, test_predictions, model_info = create_ensemble(X, y, test)\n    \noof_results.to_csv('oof_predictions.csv', index=False)\ntest_predictions.to_csv('test_predictions.csv', index=False)\n\nprint(\"\\nModeling completed successfully!\")\nprint(f\"Trained {model_info['num_models']} models\")\nprint(\"OOF predictions shape:\", oof_results.shape)\nprint(\"Test predictions shape:\", test_predictions.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-11T14:29:01.67945Z","iopub.execute_input":"2025-10-11T14:29:01.679748Z","execution_failed":"2025-10-12T12:58:05.936Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Mix & blend","metadata":{}},{"cell_type":"code","source":"def create_optimal_ensemble(oof_results, test_predictions, y):\n   \n    oof_predictions = oof_results.drop(['target'], axis=1, errors='ignore')\n    y_true = oof_results['target'] if 'target' in oof_results else y\n    \n    ridge = Ridge(alpha=1.0)\n    ridge.fit(oof_predictions, y_true)\n    ridge_pred = ridge.predict(oof_predictions)\n    ridge_rmse = np.sqrt(mean_squared_error(y_true, ridge_pred))\n    \n    def objective(weights):\n        weighted_pred = np.dot(oof_predictions.values, weights)\n        return np.sqrt(mean_squared_error(y_true, weighted_pred))\n    \n    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n    bounds = [(0, 1)] * len(oof_predictions.columns)\n    initial_weights = np.ones(len(oof_predictions.columns)) / len(oof_predictions.columns)\n    \n    result = minimize(objective, initial_weights, \n                     method='SLSQP', bounds=bounds, constraints=constraints)\n    optimized_weights = result.x\n    optimized_pred = np.dot(oof_predictions.values, optimized_weights)\n    optimized_rmse = np.sqrt(mean_squared_error(y_true, optimized_pred))\n    \n    if ridge_rmse <= optimized_rmse:\n        print(f\"Using Ridge (RMSE: {ridge_rmse:.4f})\")\n        test_pred = ridge.predict(test_predictions)\n        weights = ridge.coef_\n    else:\n        print(f\"Using Optimized Weights (RMSE: {optimized_rmse:.4f})\")\n        test_pred = np.dot(test_predictions.values, optimized_weights)\n        weights = optimized_weights\n    \n    weights = weights / weights.sum()\n    \n    return test_pred, weights, min(ridge_rmse, optimized_rmse)\n\ntest_pred, weights, best_rmse = create_optimal_ensemble(oof_results, test_predictions, y)\n\nprint(f\"\\nBest Ensemble RMSE: {best_rmse:.4f}\")\nprint(\"Final weights:\")\nfor model, weight in zip(test_predictions.columns, weights):\n    print(f\"  {model}: {weight:.4f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-12T12:58:05.936Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv('/kaggle/input/playground-series-s5e10/sample_submission.csv')\nsample['accident_risk'] = test_pred\nsample.to_csv('submission.csv', index=False)\nsample.head(10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-10-12T12:58:05.936Z"}},"outputs":[],"execution_count":null}]}