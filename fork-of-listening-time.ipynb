{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91715,"databundleVersionId":11351736,"sourceType":"competition"}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/klyushnik/fork-of-listening-time-12-70619?scriptVersionId=235486448\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.258Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Description","metadata":{}},{"cell_type":"markdown","source":"**My name is Alexander and I present to your attention my vision of the dataset, model selection and results.**\n\n**Your Goal: Your task it to predict listening time of a podcast episode.**\n\n![06-Perfect-length-768x402.png](https://wearesoundmedia.com/wp-content/uploads/2022/02/06-Perfect-length-768x402.png)\n","metadata":{}},{"cell_type":"markdown","source":"## Root Mean Squared Error (RMSE)\n\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n\n$$\nRMSE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2}\n$$\n\nwhere \\(\\hat{y}_i\\) is the predicted value and \\(y_i\\) is the original value for each instance \\(i\\).","metadata":{}},{"cell_type":"markdown","source":"Welcome to my github - https://github.com/Alexsandrrus","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport matplotlib\nfrom matplotlib.pyplot import figure\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.optimize import minimize\nfrom scipy.stats import mstats\nfrom scipy.stats.mstats import winsorize\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import (\n    LabelEncoder,\n    QuantileTransformer,\n    StandardScaler,\n    PowerTransformer,\n    MaxAbsScaler,\n    MinMaxScaler,\n    RobustScaler,\n    PolynomialFeatures,\n    OrdinalEncoder,\n    OneHotEncoder,\n    FunctionTransformer,\n    KBinsDiscretizer,\n)\nfrom sklearn.feature_selection import (\n    VarianceThreshold,\n    SelectKBest,\n    f_regression,\n    SequentialFeatureSelector,\n)\nfrom sklearn.model_selection import (\n    StratifiedKFold,\n    KFold,\n    StratifiedGroupKFold,\n    RepeatedStratifiedKFold,\n    RepeatedKFold,\n    cross_validate,\n    train_test_split,\n    TimeSeriesSplit,\n)\nfrom sklearn.linear_model import (\n    SGDOneClassSVM,\n    LinearRegression,\n    Ridge,\n    Lasso,\n    ElasticNet,\n)\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import (\n    HistGradientBoostingRegressor,\n    ExtraTreesRegressor,\n    GradientBoostingRegressor,\n    IsolationForest,\n    BaggingRegressor,\n    RandomForestRegressor,\n)\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error, \n    r2_score\n)\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import clone_model\nimport keras\nfrom keras_tuner import RandomSearch\nfrom keras import layers\nfrom keras.layers import (\n    BatchNormalization,\n    Flatten,\n    Dense,\n    Dropout,\n    Activation,\n)\nfrom tensorflow.keras.models import Sequential\nfrom keras import backend as K\nimport keras_tuner\nfrom keras_tuner import Hyperband\nfrom functools import partial\n\nimport optuna\nfrom optuna.samplers import CmaEsSampler\nfrom optuna.pruners import MedianPruner\nimport optuna.visualization as vis\n\nfrom catboost import CatBoostRegressor\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingRegressor, StackingCVRegressor\nfrom category_encoders import TargetEncoder, MEstimateEncoder\n#from cuml.preprocessing import TargetEncoder\n\nimport requests\nimport holidays\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nimport warnings\nimport re\nimport time\nimport logging\nfrom functools import partial\nfrom itertools import combinations\nfrom IPython.display import Image\n\nfrom functools import partial\n\n# Visualization settings\nplt.style.use('ggplot')\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (12, 8)\nsns.set_context(\"notebook\", font_scale=1.2)\nsns.set_style(\"whitegrid\")\n\n# Pandas settings\npd.options.mode.chained_assignment = None\n\n# Logging configuration\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Warnings configuration\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.26Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# My setting","metadata":{}},{"cell_type":"code","source":"def plot_numerical_features(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        sns.histplot(df[feature], bins=30, kde=True, ax=axes[i], color='skyblue', edgecolor='black')\n        axes[i].set_title(f'Distribution of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel(feature, fontsize=14)\n        axes[i].set_ylabel('Frequency', fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n\n        mean_value = df[feature].mean()\n        axes[i].axvline(mean_value, color='red', linestyle='--', label='Mean')\n        axes[i].legend()\n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_numerical_boxplots(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        sns.boxplot(x=df[feature], ax=axes[i], color='lightgreen')\n        axes[i].set_title(f'Boxplot of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel(feature, fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n\n        median_value = df[feature].median()\n        axes[i].axvline(median_value, color='orange', linestyle='--', label='Median')\n        axes[i].legend()\n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_qq_plot(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        stats.probplot(df[feature], dist=\"norm\", plot=axes[i])\n        axes[i].set_title(f'QQ Plot of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel('Theoretical Quantiles', fontsize=14)\n        axes[i].set_ylabel('Sample Quantiles', fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha= 0.7)  \n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_correlation_matrix(df, method='spearman'):\n    num_df = df.select_dtypes(include=[np.number])\n    \n    corr = num_df.corr(method=method)\n    plt.figure(figsize=(14, 10))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8}, linewidths=.5)\n    plt.title(f'Correlation Matrix ({method.capitalize()} Correlation)', fontsize=18, fontweight='bold')\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.show()\n\ndef plot_pairplot(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    sns.pairplot(df[num_features], diag_kind='kde', plot_kws={'alpha': 0.6, 'edgecolor': 'k'}, height=2.5)\n    plt.suptitle('Pairplot of Numerical Features', y=1.02, fontsize=18, fontweight='bold')\n    plt.show()\n\ndef plot_categorical_features(df, ncols=2, top_n=None):\n    cat_features = df.select_dtypes(include=[object]).columns\n    nrows = (len(cat_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(cat_features):\n        if top_n is not None:\n            top_categories = df[feature].value_counts().nlargest(top_n).index\n            sns.countplot(data=df[df[feature].isin(top_categories)], y=feature, ax=axes[i], palette='viridis', order=top_categories)\n        else:\n            sns.countplot(data=df, y=feature, ax=axes[i], palette='viridis')\n        \n        axes[i].set_title(f'Count of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel('Count', fontsize=14)\n        axes[i].set_ylabel(feature, fontsize=14)\n        axes[i].tick_params(axis='y', rotation=0)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n    for j in range(i + 1, len(axes)):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.show()\n\ndef PolynomialFeatures_labeled(input_df,power):\n   \n    poly = preprocessing.PolynomialFeatures(power)\n    output_nparray = poly.fit_transform(input_df)\n    powers_nparray = poly.powers_\n\n    input_feature_names = list(input_df.columns)\n    target_feature_names = [\"Constant Term\"]\n    for feature_distillation in powers_nparray[1:]:\n        intermediary_label = \"\"\n        final_label = \"\"\n        for i in range(len(input_feature_names)):\n            if feature_distillation[i] == 0:\n                continue\n            else:\n                variable = input_feature_names[i]\n                power = feature_distillation[i]\n                intermediary_label = \"%s+%d\" % (variable,power)\n                if final_label == \"\":         #If the final label isn't yet specified\n                    final_label = intermediary_label\n                else:\n                    final_label = final_label + \"x\" + intermediary_label\n        target_feature_names.append(final_label)\n    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n    return output_df\n\ndef variance_threshold(df,th):\n    var_thres=VarianceThreshold(threshold=th)\n    var_thres.fit(df)\n    new_cols = var_thres.get_support()\n    return df.iloc[:,new_cols]\n   \ndef optimize_memory_usage(df, print_size=True):\n    \"\"\"\n    Optimizes memory usage in a DataFrame by downcasting numeric columns.\n\n    Parameters:\n        df (pd.DataFrame): The DataFrame to optimize.\n        print_size (bool): If True, prints memory usage before and after optimization.\n\n    Returns:\n        pd.DataFrame: The optimized DataFrame.\n    \"\"\"\n    # Types for optimization.\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    \n    # Memory usage size before optimize (Mb).\n    before_size = df.memory_usage().sum() / 1024**2\n    \n    for column in df.columns:\n        column_type = df[column].dtype\n        \n        if column_type in numerics:\n            try:\n                if str(column_type).startswith('int'):\n                    df[column] = pd.to_numeric(df[column], downcast='integer')\n                else:\n                    df[column] = pd.to_numeric(df[column], downcast='float')\n                logger.info(f\"Optimized column {column}: {column_type} -> {df[column].dtype}\")\n            except Exception as e:\n                logger.error(f\"Failed to optimize column {column}: {e}\")\n    \n    # Memory usage size after optimize (Mb).\n    after_size = df.memory_usage().sum() / 1024**2\n    \n    if print_size:\n        print(\n            'Memory usage size: before {:5.4f} Mb - after {:5.4f} Mb ({:.1f}%).'.format(\n                before_size, after_size, 100 * (before_size - after_size) / before_size\n            )\n        )\n    \n    return df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.26Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"## Load","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s5e4/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e4/test.csv')\n\ntrain.shape, test.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.26Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Info","metadata":{}},{"cell_type":"code","source":"test = test.drop(['id'], axis =1)\ntrain = train.drop(['id'], axis =1)\n\ntrain.shape, test.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.26Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.describe().T","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"duplicates = train.duplicated()\nprint(f\"Number of duplicates: {duplicates.sum()}\")\n\ntrain = train.drop_duplicates()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in train.columns:\n    pct_missing = np.mean(train[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in test.columns:\n    pct_missing = np.mean(test[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Optimize memory","metadata":{}},{"cell_type":"code","source":"train = optimize_memory_usage(train)\ntest = optimize_memory_usage(test)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Numerical features","metadata":{}},{"cell_type":"markdown","source":"### Hist  ","metadata":{}},{"cell_type":"code","source":"plot_numerical_features(train)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.262Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Box","metadata":{}},{"cell_type":"code","source":"plot_numerical_boxplots(train)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.262Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q-Q","metadata":{}},{"cell_type":"code","source":"plot_qq_plot(train)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.262Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation matrix","metadata":{}},{"cell_type":"code","source":"plot_correlation_matrix(train)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.263Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Categorical features","metadata":{}},{"cell_type":"code","source":"plot_categorical_features(train)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.264Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Some notice","metadata":{}},{"cell_type":"code","source":"from statsmodels.graphics.mosaicplot import mosaic\n\nplt.figure(figsize=(16, 10))\nmosaic(train, ['Publication_Time', 'Genre'], title='Allocation of time')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nmosaic(train, ['Publication_Time', 'Episode_Sentiment'], title='Allocation of Sentiment time')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.boxplot(x='Genre', y='Listening_Time_minutes', data=train)\nplt.xticks(rotation=90)\nplt.title('Listening time by genre')\nplt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# New features","metadata":{}},{"cell_type":"code","source":"train['Episode_Length_minutes'] = train['Episode_Length_minutes'].fillna(train['Episode_Length_minutes'].median())\ntrain['Guest_Popularity_percentage'] = train['Guest_Popularity_percentage'].fillna(train['Guest_Popularity_percentage'].median())\ntrain['Number_of_Ads'] = train['Number_of_Ads'].fillna(train['Number_of_Ads'].median())\n\ntest['Episode_Length_minutes'] = test['Episode_Length_minutes'].fillna(test['Episode_Length_minutes'].median())\ntest['Guest_Popularity_percentage'] = test['Guest_Popularity_percentage'].fillna(test['Guest_Popularity_percentage'].median())\ntest['Number_of_Ads'] = test['Number_of_Ads'].fillna(test['Number_of_Ads'].median())","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# for col in train.select_dtypes(include=['number']).columns:\n#     train[col] = winsorize(train[col], limits=[0.05, 0.05])\nQ1 = train['Number_of_Ads'].quantile(0.25)\nQ3 = train['Number_of_Ads'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower = Q1 - 1.5 * IQR\nupper = Q3 + 1.5 * IQR\n\ntrain = train[\n    (train['Number_of_Ads'] >= lower) &\n    (train['Number_of_Ads'] <= upper)\n]\ntrain.shape, test.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def categorize_variable(df, column, labels):\n    \n    if len(labels) != 3:\n        raise ValueError(\"3 type\")\n    \n    bins = [-float('inf'), \n            df[column].quantile(0.25), \n            df[column].quantile(0.75), \n            float('inf')]\n    \n    df[f'{column}_group'] = pd.cut(df[column], bins=bins, labels=labels)\n    return df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"categorize_variable(train, 'Episode_Length_minutes', [\"short\", \"mean\", 'long'])\ncategorize_variable(train, 'Host_Popularity_percentage', [\"low\", \"normal\", 'high'])\ncategorize_variable(train, 'Guest_Popularity_percentage', [\"low_important\", \"mean\", 'top'])\n\ncategorize_variable(test, 'Episode_Length_minutes', [\"short\", \"mean\", 'long'])\ncategorize_variable(test, 'Host_Popularity_percentage', [\"low\", \"normal\", 'high'])\ncategorize_variable(test, 'Guest_Popularity_percentage', [\"low_important\", \"mean\", 'top'])\n\ntrain.shape, test.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.266Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in train.columns:\n    pct_missing = np.mean(train[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.267Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# X,y make transform","metadata":{}},{"cell_type":"markdown","source":"## Target Encode","metadata":{}},{"cell_type":"code","source":"col = ['Podcast_Name', 'Episode_Title', 'Genre', 'Publication_Day',\n       'Publication_Time', 'Episode_Sentiment', 'Episode_Length_minutes_group',\n      'Host_Popularity_percentage_group','Guest_Popularity_percentage_group']\ncol_num = ['Episode_Length_minutes', 'Host_Popularity_percentage',\n       'Guest_Popularity_percentage', 'Number_of_Ads']\n\nTE = TargetEncoder(cols=col)\ntrain[col] = TE.fit_transform(train[col], train['Listening_Time_minutes'])\ntest[col] = TE.transform(test[col])\n\ntrain.shape, test.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train.drop(columns=['Listening_Time_minutes'])\ny = train['Listening_Time_minutes']\nprint('before threshold:',X.shape, y.shape)\n\nX = variance_threshold(X,0.01)\nlist_name = (X.columns)\ntest = test[list_name]\n\nprint('after threshold:',X.shape, y.shape)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.267Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX[X.select_dtypes(include=[np.number]).columns] = scaler.fit_transform(X[X.select_dtypes(include=[np.number]).columns])\ntest[X.select_dtypes(include=[np.number]).columns] = scaler.transform(test[X.select_dtypes(include=[np.number]).columns])\n\nX.shape, y.shape, test.shape","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Parametrs","metadata":{}},{"cell_type":"code","source":"catboost_params = [\n    {'iterations': 2000, 'depth': 8, 'learning_rate': 0.02, 'l2_leaf_reg': 5, 'loss_function': 'RMSE'},\n    {'iterations': 1500, 'depth': 10, 'learning_rate': 0.01, 'l2_leaf_reg': 3, 'loss_function': 'RMSE'},\n    {'iterations': 1000, 'depth': 12, 'learning_rate': 0.005, 'l2_leaf_reg': 1, 'loss_function': 'RMSE'},\n    {'iterations': 2500, 'depth': 7, 'learning_rate': 0.03, 'l2_leaf_reg': 7,  'loss_function': 'RMSE'},\n    {'iterations': 1800, 'depth': 9, 'learning_rate': 0.025,'l2_leaf_reg': 4,  'loss_function': 'RMSE'},\n    {'iterations': 1200, 'depth': 6, 'learning_rate': 0.04, 'l2_leaf_reg': 6, 'loss_function': 'RMSE'},\n    {'iterations': 1600, 'depth': 11, 'learning_rate': 0.015, 'l2_leaf_reg': 2, 'loss_function': 'RMSE'}\n]\n\nxgb_params = [\n    {'n_estimators': 2000, 'max_depth': 8, 'learning_rate': 0.01,  'eval_metric': 'rmse'},\n    {'n_estimators': 1500, 'max_depth': 10, 'learning_rate': 0.02, 'eval_metric': 'rmse'},\n    {'n_estimators': 1000, 'max_depth': 12, 'learning_rate': 0.05,  'eval_metric': 'rmse'},\n    {'n_estimators': 2500, 'max_depth': 7, 'learning_rate': 0.03,  'eval_metric': 'rmse'},\n    {'n_estimators': 2200, 'max_depth': 9, 'learning_rate': 0.015,  'eval_metric': 'rmse'},\n    {'n_estimators': 1800, 'max_depth': 6, 'learning_rate': 0.04, 'eval_metric': 'rmse'},\n    {'n_estimators': 1300, 'max_depth': 11, 'learning_rate': 0.025, 'eval_metric': 'rmse'}\n]\n\nlgbm_params = [\n    {'n_estimators': 2000, 'max_depth': 8, 'learning_rate': 0.01, 'metric': 'rmse'},\n    {'n_estimators': 1500, 'max_depth': 10, 'learning_rate': 0.02, 'metric': 'rmse'},\n    {'n_estimators': 1000, 'max_depth': 12, 'learning_rate': 0.05, 'metric': 'rmse'},\n    {'n_estimators': 2500, 'max_depth': 7, 'learning_rate': 0.03,  'metric': 'rmse'},\n    {'n_estimators': 1800, 'max_depth': 9, 'learning_rate': 0.018, 'metric': 'rmse'},\n    {'n_estimators': 1700, 'max_depth': 6, 'learning_rate': 0.04, 'metric': 'rmse'},\n    {'n_estimators': 1600, 'max_depth': 11, 'learning_rate': 0.022, 'metric': 'rmse'}\n]\n\nhgb_params = [\n    {'max_iter': 2000, 'learning_rate': 0.01, 'max_depth': 8,  'loss': 'squared_error'},\n    {'max_iter': 1500, 'learning_rate': 0.02, 'max_depth': 10, 'loss': 'squared_error'},\n    {'max_iter': 1000, 'learning_rate': 0.05, 'max_depth': 12, 'loss': 'squared_error'},\n    {'max_iter': 2500, 'learning_rate': 0.03, 'max_depth': 7,  'loss': 'squared_error'},\n    {'max_iter': 2200, 'learning_rate': 0.018, 'max_depth': 9, 'loss': 'squared_error'},\n    {'max_iter': 1800, 'learning_rate': 0.02, 'max_depth': 6, 'loss': 'squared_error'},\n    {'max_iter': 1600, 'learning_rate': 0.015, 'max_depth': 11, 'loss': 'squared_error'}\n]","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.268Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fit","metadata":{}},{"cell_type":"code","source":"def create_ensemble(X, y, test, n_folds=5):\n    \n    FOLDS = KFold(n_splits=5, shuffle=True, random_state=42)\n    \n    all_oof = {}\n    all_predictions = {}\n    \n    models = []\n    \n    for i, params in enumerate(catboost_params, 1):\n        models.append((f'cat_{i}', CatBoostRegressor(**params, verbose=0, thread_count=1)))\n    \n    for i, params in enumerate(xgb_params, 1):\n        models.append((f'xgb_{i}', xgb.XGBRegressor(**params, n_jobs=1)))\n    \n    for i, params in enumerate(lgbm_params, 1):\n        models.append((f'lgb_{i}', LGBMRegressor(**params, verbose=-1, n_jobs=1)))\n    \n    for i, params in enumerate(hgb_params, 1):\n        models.append((f'hgb_{i}', HistGradientBoostingRegressor(**params)))\n    \n    for name, model in models:\n        try:\n            print(f\"\\nTraining {name}...\")\n            oof = np.zeros(len(X))\n            pred = np.zeros(len(test))\n            \n            for fold, (trn_idx, val_idx) in enumerate(FOLDS.split(X, y)):\n                X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n                X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n                \n                model.fit(X_train, y_train)\n                oof[val_idx] = model.predict(X_val)\n                pred += model.predict(test) / FOLDS.n_splits\n                \n                fold_rmse = np.sqrt(mean_squared_error(y_val, oof[val_idx]))\n                print(f'{name} - Fold {fold} RMSE: {fold_rmse:.4f}')\n            \n            all_oof[name] = oof\n            all_predictions[name] = pred\n            \n            full_rmse = np.sqrt(mean_squared_error(y, oof))\n            print(f'{name} - Full OOF RMSE: {full_rmse:.4f}')\n            \n        except Exception as e:\n            print(f\"Error training {name}: {str(e)}\")\n            continue\n    \n    oof_df = pd.DataFrame(all_oof)\n    predictions_df = pd.DataFrame(all_predictions)\n    \n    oof_df['target'] = y.values\n    \n    model_info = {\n        'model_names': [name for name, _ in models],\n        'num_models': len(models),\n        'features_used': list(X.columns)\n    }\n    \n    return oof_df, predictions_df, model_info","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"oof_results, test_predictions, model_info = create_ensemble(X, y, test)\n    \noof_results.to_csv('oof_predictions.csv', index=False)\ntest_predictions.to_csv('test_predictions.csv', index=False)\n\nprint(\"\\nModeling completed successfully!\")\nprint(f\"Trained {model_info['num_models']} models\")\nprint(\"OOF predictions shape:\", oof_results.shape)\nprint(\"Test predictions shape:\", test_predictions.shape)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Blend","metadata":{}},{"cell_type":"code","source":"X = oof_results.drop(columns=['target'])\ny = oof_results['target']\n\nFOLDs = KFold(n_splits=5, shuffle=True,random_state=42)\n\noof_blend = np.zeros(len(X))\npredictions_blend = np.zeros(len(test_predictions))\nfor fold_, (trn_idx, val_idx) in enumerate(FOLDs.split(X,y)):\n    X.iloc[trn_idx], y.iloc[trn_idx]\n    X.iloc[val_idx], y.iloc[val_idx]\n    xgb_params = {'n_estimators': 2500, 'max_depth': 7, 'learning_rate': 0.03,  'eval_metric': 'rmse'}\n    blend =  xgb.XGBRegressor(**xgb_params, n_jobs=1)\n    \n    blend.fit(X.iloc[trn_idx],y.iloc[trn_idx])\n    \n    oof_blend[val_idx] = blend.predict(X.iloc[val_idx])\n    predictions_blend += blend.predict(test_predictions)/FOLDs.n_splits\n    blend_score = mean_absolute_error(oof_blend, y)\n    \n    print('Fold', fold_, '-- Blend oof MAE is ---',blend_score)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.269Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv('/kaggle/input/playground-series-s5e4/sample_submission.csv')\nsample['Listening_Time_minutes'] = test_predictions.mean(axis=1)\nsample.to_csv('submission.csv', index=False)\nsample.head(10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-22T17:11:22.269Z"}},"outputs":[],"execution_count":null}]}