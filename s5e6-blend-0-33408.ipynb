{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91717,"databundleVersionId":12184666,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/klyushnik/s5e6-blend-0-33408?scriptVersionId=246113058\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:45:31.495565Z","iopub.execute_input":"2025-06-16T12:45:31.495848Z","iopub.status.idle":"2025-06-16T12:45:31.922087Z","shell.execute_reply.started":"2025-06-16T12:45:31.495818Z","shell.execute_reply":"2025-06-16T12:45:31.921294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Description","metadata":{}},{"cell_type":"markdown","source":"![Optimal-Timing-for-Applying-NPK-Fertilizer.jpg](https://risso-chemical.com/wp-content/uploads/2024/09/Optimal-Timing-for-Applying-NPK-Fertilizer.jpg)\n\n## Overview\nWelcome to the 2025 Kaggle Playground Series! This competition aims to provide an engaging and approachable dataset for the community to practice their machine learning skills. Each month, we anticipate a new competition, and this month, we focus on predicting the optimal fertilizers for various weather, soil conditions, and crops.\n\n## Your Goal\nYour objective is to select the best fertilizer for different weather, soil conditions, and crops based on the provided dataset.\n\n## Evaluation\nSubmissions are evaluated according to the **Mean Average Precision @ 3 (MAP@3)**, which is calculated using the following formula:\n\n\\$\n\\text{MAP@3} = \\frac{1}{N} \\sum_{i=1}^{N} P@k\n\\$\n\nWhere:\n- \\$ N \\$ is the number of observations,\n- \\$ P@k \\$ is the precision at cutoff \\$ k \\$,\n- \\$ k \\$ is the number of predictions per observation,\n- The indicator function equals 1 if the item at rank \\$ k \\$ is a relevant (correct) label, and zero otherwise.\n\nOnce a correct label has been scored for an observation, that label is no longer considered relevant for that observation, and additional predictions of that label are skipped in the calculation. For example, if the correct label is A for an observation, the following predictions all score an average precision of 1.0:\n- [A, B, C, D, E]\n- [A, A, A, A, A]\n- [A, B, A, C, A]\n\n## Submission File\nFor each `id` in the test set, you may predict up to 3 Fertilizer Name values, with the predictions space delimited. The file should contain a header and have the following format:\n","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\nfrom matplotlib.pyplot import figure\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.stats import mstats\nfrom scipy.stats.mstats import winsorize\nfrom scipy.optimize import minimize\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import (\n    LabelEncoder,\n    OneHotEncoder,\n    OrdinalEncoder,\n    FunctionTransformer,\n    KBinsDiscretizer,\n    StandardScaler\n)\nfrom sklearn.feature_selection import (\n    VarianceThreshold,\n    SelectKBest,\n    chi2,\n    SequentialFeatureSelector,\n)\nfrom sklearn.model_selection import (\n    StratifiedKFold,\n    KFold,\n    train_test_split,\n    cross_validate,\n)\nfrom sklearn.linear_model import (\n    LogisticRegression,\n)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import (\n    RandomForestClassifier,\n    GradientBoostingClassifier,\n    AdaBoostClassifier,\n    HistGradientBoostingClassifier,\n)\nfrom sklearn.metrics import (\n    accuracy_score,\n    f1_score,\n    precision_score,\n    recall_score,\n    make_scorer,\n)\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import (\n    BatchNormalization,\n    Flatten,\n    Dense,\n    Dropout,\n    Activation,\n)\nfrom keras import backend as K\nimport keras_tuner\nfrom keras_tuner import RandomSearch, Hyperband\n\nimport optuna\nfrom optuna.samplers import CmaEsSampler\nfrom optuna.pruners import MedianPruner\nimport optuna.visualization as vis\n\nfrom catboost import CatBoostClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom mlxtend.classifier import StackingClassifier, StackingCVClassifier\nfrom category_encoders import TargetEncoder, MEstimateEncoder\n\nimport requests\nimport holidays\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nimport warnings\nimport re\nimport time\nimport logging\nfrom functools import partial\nfrom itertools import combinations\nfrom IPython.display import Image\n\n# Visualization settings\nplt.style.use('ggplot')\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (12, 8)\nsns.set_context(\"notebook\", font_scale=1.2)\nsns.set_style(\"whitegrid\")\n\n# Pandas settings\npd.options.mode.chained_assignment = None\n\n# Logging configuration\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Warnings configuration\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:45:31.923908Z","iopub.execute_input":"2025-06-16T12:45:31.92433Z","iopub.status.idle":"2025-06-16T12:45:38.911683Z","shell.execute_reply.started":"2025-06-16T12:45:31.924306Z","shell.execute_reply":"2025-06-16T12:45:38.910683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setting","metadata":{}},{"cell_type":"code","source":"def plot_numerical_features(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        sns.histplot(df[feature], bins=30, kde=True, ax=axes[i], color='skyblue', edgecolor='black')\n        axes[i].set_title(f'Distribution of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel(feature, fontsize=14)\n        axes[i].set_ylabel('Frequency', fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n\n        mean_value = df[feature].mean()\n        axes[i].axvline(mean_value, color='red', linestyle='--', label='Mean')\n        axes[i].legend()\n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_numerical_boxplots(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        sns.boxplot(x=df[feature], ax=axes[i], color='lightgreen')\n        axes[i].set_title(f'Boxplot of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel(feature, fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n\n        median_value = df[feature].median()\n        axes[i].axvline(median_value, color='orange', linestyle='--', label='Median')\n        axes[i].legend()\n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_qq_plot(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        stats.probplot(df[feature], dist=\"norm\", plot=axes[i])\n        axes[i].set_title(f'QQ Plot of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel('Theoretical Quantiles', fontsize=14)\n        axes[i].set_ylabel('Sample Quantiles', fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha= 0.7)  \n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_correlation_matrix(df, method='spearman'):\n    num_df = df.select_dtypes(include=[np.number])\n    \n    corr = num_df.corr(method=method)\n    plt.figure(figsize=(14, 10))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8}, linewidths=.5)\n    plt.title(f'Correlation Matrix ({method.capitalize()} Correlation)', fontsize=18, fontweight='bold')\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.show()\n\ndef plot_pairplot(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    sns.pairplot(df[num_features], diag_kind='kde', plot_kws={'alpha': 0.6, 'edgecolor': 'k'}, height=2.5)\n    plt.suptitle('Pairplot of Numerical Features', y=1.02, fontsize=18, fontweight='bold')\n    plt.show()\n\ndef plot_categorical_features(df, ncols=2, top_n=None):\n    cat_features = df.select_dtypes(include=[object]).columns\n    nrows = (len(cat_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(cat_features):\n        if top_n is not None:\n            top_categories = df[feature].value_counts().nlargest(top_n).index\n            sns.countplot(data=df[df[feature].isin(top_categories)], y=feature, ax=axes[i], palette='viridis', order=top_categories)\n        else:\n            sns.countplot(data=df, y=feature, ax=axes[i], palette='viridis')\n        \n        axes[i].set_title(f'Count of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel('Count', fontsize=14)\n        axes[i].set_ylabel(feature, fontsize=14)\n        axes[i].tick_params(axis='y', rotation=0)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n    for j in range(i + 1, len(axes)):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.show()\n\ndef PolynomialFeatures_labeled(input_df,power):\n   \n    poly = preprocessing.PolynomialFeatures(power)\n    output_nparray = poly.fit_transform(input_df)\n    powers_nparray = poly.powers_\n\n    input_feature_names = list(input_df.columns)\n    target_feature_names = [\"Constant Term\"]\n    for feature_distillation in powers_nparray[1:]:\n        intermediary_label = \"\"\n        final_label = \"\"\n        for i in range(len(input_feature_names)):\n            if feature_distillation[i] == 0:\n                continue\n            else:\n                variable = input_feature_names[i]\n                power = feature_distillation[i]\n                intermediary_label = \"%s+%d\" % (variable,power)\n                if final_label == \"\":         #If the final label isn't yet specified\n                    final_label = intermediary_label\n                else:\n                    final_label = final_label + \"x\" + intermediary_label\n        target_feature_names.append(final_label)\n    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n    return output_df\n\ndef variance_threshold(df,th):\n    var_thres=VarianceThreshold(threshold=th)\n    var_thres.fit(df)\n    new_cols = var_thres.get_support()\n    return df.iloc[:,new_cols]\n   \ndef optimize_memory_usage(df, print_size=True):\n    \"\"\"\n    Optimizes memory usage in a DataFrame by downcasting numeric columns.\n\n    Parameters:\n        df (pd.DataFrame): The DataFrame to optimize.\n        print_size (bool): If True, prints memory usage before and after optimization.\n\n    Returns:\n        pd.DataFrame: The optimized DataFrame.\n    \"\"\"\n    # Types for optimization.\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    \n    # Memory usage size before optimize (Mb).\n    before_size = df.memory_usage().sum() / 1024**2\n    \n    for column in df.columns:\n        column_type = df[column].dtype\n        \n        if column_type in numerics:\n            try:\n                if str(column_type).startswith('int'):\n                    df[column] = pd.to_numeric(df[column], downcast='integer')\n                else:\n                    df[column] = pd.to_numeric(df[column], downcast='float')\n                logger.info(f\"Optimized column {column}: {column_type} -> {df[column].dtype}\")\n            except Exception as e:\n                logger.error(f\"Failed to optimize column {column}: {e}\")\n    \n    # Memory usage size after optimize (Mb).\n    after_size = df.memory_usage().sum() / 1024**2\n    \n    if print_size:\n        print(\n            'Memory usage size: before {:5.4f} Mb - after {:5.4f} Mb ({:.1f}%).'.format(\n                before_size, after_size, 100 * (before_size - after_size) / before_size\n            )\n        )\n    \n    return df\n\ndef categorize_variable(df, column, labels):\n    \n    if len(labels) != 3:\n        raise ValueError(\"3 type\")\n    \n    bins = [-float('inf'), \n            df[column].quantile(0.25), \n            df[column].quantile(0.75), \n            float('inf')]\n    \n    df[f'{column}_group'] = pd.cut(df[column], bins=bins, labels=labels)\n    return df\n\ndef replace_outliers_with_mean(df, threshold=3):\n\n    df_clean = df.copy()\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    \n    for col in numeric_cols:\n        \n        z_scores = np.abs(stats.zscore(df[col], nan_policy='omit')) \n        \n        mean_val = df[col][z_scores <= threshold].mean()\n        \n        df_clean[col] = np.where(z_scores > threshold, mean_val, df[col])\n        \n    return df_clean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:45:38.912587Z","iopub.execute_input":"2025-06-16T12:45:38.913364Z","iopub.status.idle":"2025-06-16T12:45:38.944733Z","shell.execute_reply.started":"2025-06-16T12:45:38.913338Z","shell.execute_reply":"2025-06-16T12:45:38.943847Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s5e6/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e6/test.csv')\n\ndisplay(train.shape, test.shape)\n\ndisplay(train.info())\n\ntest = test.drop(['id'], axis =1)\ntrain = train.drop(['id'], axis =1)\n\ndisplay(train.head(5))\n\ndisplay(train.describe().T)\n\nduplicates = train.duplicated()\nprint(f\"Number of duplicates: {duplicates.sum()}\")\n\nfor col in train.columns:\n    pct_missing = np.mean(train[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\ntrain = optimize_memory_usage(train)\ntest = optimize_memory_usage(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:45:38.945678Z","iopub.execute_input":"2025-06-16T12:45:38.946018Z","iopub.status.idle":"2025-06-16T12:45:40.868877Z","shell.execute_reply.started":"2025-06-16T12:45:38.945987Z","shell.execute_reply":"2025-06-16T12:45:40.868105Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Numerical feature","metadata":{}},{"cell_type":"markdown","source":"### Hit","metadata":{}},{"cell_type":"code","source":"plot_numerical_features(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:45:40.869733Z","iopub.execute_input":"2025-06-16T12:45:40.870013Z","iopub.status.idle":"2025-06-16T12:45:59.993476Z","shell.execute_reply.started":"2025-06-16T12:45:40.86999Z","shell.execute_reply":"2025-06-16T12:45:59.992226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Boxplot","metadata":{}},{"cell_type":"code","source":"plot_numerical_boxplots(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:45:59.994622Z","iopub.execute_input":"2025-06-16T12:45:59.994926Z","iopub.status.idle":"2025-06-16T12:46:01.891582Z","shell.execute_reply.started":"2025-06-16T12:45:59.994901Z","shell.execute_reply":"2025-06-16T12:46:01.890558Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q-Q plot","metadata":{}},{"cell_type":"code","source":"plot_qq_plot(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:46:01.89262Z","iopub.execute_input":"2025-06-16T12:46:01.892908Z","iopub.status.idle":"2025-06-16T12:46:13.96218Z","shell.execute_reply.started":"2025-06-16T12:46:01.892884Z","shell.execute_reply":"2025-06-16T12:46:13.961115Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### correlation_matrix","metadata":{}},{"cell_type":"code","source":"plot_correlation_matrix(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:46:13.96564Z","iopub.execute_input":"2025-06-16T12:46:13.966034Z","iopub.status.idle":"2025-06-16T12:46:14.980648Z","shell.execute_reply.started":"2025-06-16T12:46:13.966005Z","shell.execute_reply":"2025-06-16T12:46:14.979694Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### VIF","metadata":{}},{"cell_type":"code","source":"X = sm.add_constant(train.select_dtypes(include=[np.number]).iloc [:, 1:])\n\nVIFs = pd.DataFrame()\nVIFs['Variable'] = X.columns\nVIFs['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nprint(VIFs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:46:14.981715Z","iopub.execute_input":"2025-06-16T12:46:14.982632Z","iopub.status.idle":"2025-06-16T12:46:15.942548Z","shell.execute_reply.started":"2025-06-16T12:46:14.982607Z","shell.execute_reply":"2025-06-16T12:46:15.941727Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Categorical features","metadata":{}},{"cell_type":"code","source":"plot_categorical_features(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:46:15.943868Z","iopub.execute_input":"2025-06-16T12:46:15.944268Z","iopub.status.idle":"2025-06-16T12:46:17.884826Z","shell.execute_reply.started":"2025-06-16T12:46:15.944239Z","shell.execute_reply":"2025-06-16T12:46:17.883774Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# New features","metadata":{}},{"cell_type":"code","source":"categorize_variable(train, 'Temparature', [\"cold\", \"mean\", \"hot\"])\ncategorize_variable(test, 'Temparature', [\"cold\", \"mean\", \"hot\"])\ncategorize_variable(train, 'Humidity', [\"dry\", \"normal\", \"humid\"])\ncategorize_variable(test, 'Humidity', [\"dry\", \"normal\", \"humid\"])\ncategorize_variable(train, 'Moisture', [\"low\", \"medium\", \"high\"])\ncategorize_variable(test, 'Moisture', [\"low\", \"medium\", \"high\"])\ncategorize_variable(train, 'Nitrogen', [\"low\", \"medium\", \"high\"])\ncategorize_variable(test, 'Nitrogen', [\"low\", \"medium\", \"high\"])\ncategorize_variable(train, 'Potassium', [\"low\", \"medium\", \"high\"])\ncategorize_variable(test, 'Potassium', [\"low\", \"medium\", \"high\"])\ncategorize_variable(train, 'Phosphorous', [\"low\", \"medium\", \"high\"])\ncategorize_variable(test, 'Phosphorous', [\"low\", \"medium\", \"high\"])\n\ntrain.shape, test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:46:17.885784Z","iopub.execute_input":"2025-06-16T12:46:17.886096Z","iopub.status.idle":"2025-06-16T12:46:18.242405Z","shell.execute_reply.started":"2025-06-16T12:46:17.886073Z","shell.execute_reply":"2025-06-16T12:46:18.241641Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# X,y make transform","metadata":{}},{"cell_type":"code","source":"col = ['Soil Type', 'Crop Type']\n\nfor c in col:\n    train[c] = train[c].astype('category')\n    test[c] = test[c].astype('category')\n\nle = LabelEncoder()\ntrain['Fertilizer Name'] = le.fit_transform(train['Fertilizer Name'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:46:18.243283Z","iopub.execute_input":"2025-06-16T12:46:18.243574Z","iopub.status.idle":"2025-06-16T12:46:18.510509Z","shell.execute_reply.started":"2025-06-16T12:46:18.243542Z","shell.execute_reply":"2025-06-16T12:46:18.509779Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train.drop(columns=['Fertilizer Name'])\ny = train['Fertilizer Name']\n\nX.shape, y.shape, test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:46:18.511475Z","iopub.execute_input":"2025-06-16T12:46:18.511826Z","iopub.status.idle":"2025-06-16T12:46:18.535351Z","shell.execute_reply.started":"2025-06-16T12:46:18.511797Z","shell.execute_reply":"2025-06-16T12:46:18.534364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX[X.select_dtypes(include=[np.number]).columns] = scaler.fit_transform(X[X.select_dtypes(include=[np.number]).columns])\ntest[X.select_dtypes(include=[np.number]).columns] = scaler.transform(test[X.select_dtypes(include=[np.number]).columns])\n\nX.shape, y.shape, test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:46:18.536308Z","iopub.execute_input":"2025-06-16T12:46:18.536666Z","iopub.status.idle":"2025-06-16T12:46:18.76313Z","shell.execute_reply.started":"2025-06-16T12:46:18.53664Z","shell.execute_reply":"2025-06-16T12:46:18.76208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Set model","metadata":{}},{"cell_type":"code","source":"def mapk(y_true, y_pred, k=3):\n    \"\"\"\n    Correct MAP@3 implementation according to competition rules:\n    - Stops after first correct prediction\n    - Only counts first occurrence of correct label\n    \"\"\"\n    ap = []\n    for true, preds in zip(y_true, y_pred):\n        score = 0.0\n        for i, pred in enumerate(preds[:k]):\n            if pred == true:\n                score = 1.0 / (i + 1.0)\n                break\n        ap.append(score)\n    return np.mean(ap)\n\ndef create_ensemble(X, y, test, n_folds=5, \n                   catboost_params=None, xgb_params=None, lgbm_params=None):\n    \"\"\"\n    Create ensemble with customizable parameters for each model type.\n    \n    Parameters:\n    -----------\n    X : pd.DataFrame\n        Training features\n    y : pd.Series\n        Training target\n    test : pd.DataFrame\n        Test features\n    n_folds : int\n        Number of cross-validation folds\n    catboost_params : list of dict\n        List of parameter dictionaries for CatBoost models\n    xgb_params : list of dict\n        List of parameter dictionaries for XGBoost models\n    lgbm_params : list of dict\n        List of parameter dictionaries for LightGBM models\n        \n    Returns:\n    --------\n    tuple: (all_oof, all_test_preds, model_info)\n    \"\"\"\n    FOLDS = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    # Prepare categorical features\n    cat_cols = ['Soil Type', 'Crop Type', 'Temparature_group', 'Humidity_group',\n               'Moisture_group', 'Nitrogen_group', 'Potassium_group', 'Phosphorous_group']\n    \n    # Convert to categorical dtype\n    for col in cat_cols:\n        X[col] = X[col].astype('category')\n        test[col] = test[col].astype('category')\n    \n    # Default parameters if none provided\n    if catboost_params is None:\n        catboost_params = [{\n            'iterations': 1000,\n            'learning_rate': 0.05,\n            'depth': 6,\n            'l2_leaf_reg': 3,\n            'border_count': 64,\n            'verbose': 0,\n            'loss_function': 'MultiClass',\n            'random_state': 42\n        }]\n    \n    if xgb_params is None:\n        xgb_params = [{\n            'n_estimators': 500,\n            'learning_rate': 0.05,\n            'max_depth': 6,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'gamma': 0.1,\n            'min_child_weight': 3,\n            'objective': 'multi:softprob',\n            'random_state': 42,\n            'enable_categorical': True\n        }]\n    \n    if lgbm_params is None:\n        lgbm_params = [{\n            'n_estimators': 500,\n            'learning_rate': 0.05,\n            'max_depth': -1,\n            'num_leaves': 31,\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'objective': 'multiclass',\n            'random_state': 42\n        }]\n    \n    all_oof = {}\n    all_test_preds = {}\n    models_info = []\n    num_classes = len(np.unique(y))\n    \n    # Create model instances with parameters\n    models = []\n    \n    # CatBoost models\n    for i, params in enumerate(catboost_params, 1):\n        model_name = f'cat_{i}'\n        params['loss_function'] = params.get('loss_function', 'MultiClass')\n        #params['num_class'] = num_classes\n        models.append((model_name, CatBoostClassifier(\n            **params,\n            cat_features=cat_cols\n        )))\n        models_info.append({\n            'name': model_name,\n            'type': 'catboost',\n            'params': params\n        })\n    \n    # XGBoost models\n    for i, params in enumerate(xgb_params, 1):\n        model_name = f'xgb_{i}'\n        params['objective'] = params.get('objective', 'multi:softprob')\n        params['num_class'] = num_classes\n        models.append((model_name, xgb.XGBClassifier(\n            **params,\n            enable_categorical=True\n        )))\n        models_info.append({\n            'name': model_name,\n            'type': 'xgboost',\n            'params': params\n        })\n    \n    # LightGBM models\n    for i, params in enumerate(lgbm_params, 1):\n        model_name = f'lgb_{i}'\n        params['objective'] = params.get('objective', 'multiclass')\n        params['num_class'] = num_classes\n        models.append((model_name, LGBMClassifier(\n            **params,\n            categorical_feature=cat_cols\n        )))\n        models_info.append({\n            'name': model_name,\n            'type': 'lightgbm',\n            'params': params\n        })\n    \n    # Train models\n    for name, model in models:\n        print(f\"\\nTraining {name}...\")\n        oof_preds = np.zeros((len(X), num_classes))\n        test_preds = np.zeros((len(test), num_classes))\n        \n        for fold, (trn_idx, val_idx) in enumerate(FOLDS.split(X, y)):\n            X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n            X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n            \n            try:\n                if name.startswith('cat_'):\n                    model.fit(X_train, y_train, eval_set=(X_val, y_val), verbose=0)\n                else:\n                    model.fit(X_train, y_train)\n                \n                oof_preds[val_idx] = model.predict_proba(X_val)\n                test_preds += model.predict_proba(test) / FOLDS.n_splits\n                \n                fold_top3 = np.argsort(oof_preds[val_idx], axis=1)[:, -3:][:, ::-1]\n                fold_map3 = mapk(y_val.values, fold_top3)\n                print(f'{name} - Fold {fold+1} MAP@3: {fold_map3:.4f}')\n            \n            except Exception as e:\n                print(f\"Error in {name} Fold {fold+1}: {str(e)}\")\n                continue\n        \n        all_oof[name] = oof_preds\n        all_test_preds[name] = test_preds\n        \n        full_top3 = np.argsort(oof_preds, axis=1)[:, -3:][:, ::-1]\n        full_map3 = mapk(y.values, full_top3)\n        print(f'{name} - Full OOF MAP@3: {full_map3:.4f}')\n    \n    return all_oof, all_test_preds, models_info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:46:18.764047Z","iopub.execute_input":"2025-06-16T12:46:18.764317Z","iopub.status.idle":"2025-06-16T12:46:18.782843Z","shell.execute_reply.started":"2025-06-16T12:46:18.764294Z","shell.execute_reply":"2025-06-16T12:46:18.781659Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fit","metadata":{}},{"cell_type":"code","source":"my_cat_params = [\n    {\n        'iterations': 400,\n        'depth': 6,\n        'learning_rate': 0.05,\n        'l2_leaf_reg': 3,\n        'border_count': 32,\n        'early_stopping_rounds': 50\n    },\n    {\n        'iterations': 400,\n        'depth': 8,\n        'learning_rate': 0.03,\n        'l2_leaf_reg': 5,\n        'border_count': 64,\n        'early_stopping_rounds': 100\n    }\n]\n\nmy_xgb_params = [\n    {\n        'n_estimators': 400,\n        'max_depth': 6,\n        'learning_rate': 0.01,\n        'subsample': 0.8,\n        'colsample_bytree': 0.8,\n        'gamma': 0\n    },\n    {\n        'n_estimators': 400,\n        'max_depth': 8,\n        'learning_rate': 0.03,\n        'subsample': 0.9,\n        'colsample_bytree': 0.9,\n        'gamma': 0.1\n    }\n]\n\nmy_lgbm_params = [\n    {\n        'n_estimators': 400,\n        'max_depth': 5,\n        'learning_rate': 0.01,\n        'num_leaves': 31,\n        'feature_fraction': 0.8\n    },\n    {\n        'n_estimators': 400,\n        'max_depth': 8,\n        'learning_rate': 0.03,\n        'num_leaves': 63,\n        'feature_fraction': 0.9\n    }\n]\n\noof_results, test_predictions, models_info = create_ensemble(\n    X, y, test,\n    catboost_params=my_cat_params,\n    xgb_params=my_xgb_params,\n    lgbm_params=my_lgbm_params\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:46:18.783924Z","iopub.execute_input":"2025-06-16T12:46:18.784235Z","iopub.status.idle":"2025-06-16T12:48:47.900627Z","shell.execute_reply.started":"2025-06-16T12:46:18.78421Z","shell.execute_reply":"2025-06-16T12:48:47.899576Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Blend","metadata":{}},{"cell_type":"code","source":"def blend_predictions(oof_results, test_predictions, y_true):\n\n    model_names = list(oof_results.keys())\n    num_classes = oof_results[model_names[0]].shape[1]\n    \n    oof_top3 = {\n        name: np.argsort(preds, axis=1)[:, -3:][:, ::-1] \n        for name, preds in oof_results.items()\n    }\n    \n    model_scores = {\n        name: mapk(y_true, top3) \n        for name, top3 in oof_top3.items()\n    }\n    \n    print(\"\\nIndividual Model Scores (MAP@3):\")\n    for name, score in model_scores.items():\n        print(f\"{name}: {score:.4f}\")\n    \n    def objective(weights):\n        weights = np.array(weights)\n        weights = weights / np.sum(weights)\n        \n        blended = np.zeros_like(oof_results[model_names[0]])\n        for i, name in enumerate(model_names):\n            blended += weights[i] * oof_results[name]\n        \n        top3 = np.argsort(blended, axis=1)[:, -3:][:, ::-1]\n        return mapk(y_true, top3)  \n    \n    bounds = [(0, 1) for _ in model_names]\n    initial_weights = np.array([1/len(model_names)] * len(model_names))\n    \n    result = minimize(\n        lambda w: -objective(w),  \n        initial_weights,\n        method='SLSQP',\n        bounds=bounds,\n        constraints={'type': 'eq', 'fun': lambda w: np.sum(w) - 1},\n        options={'maxiter': 100}\n    )\n    \n    if not result.success:\n        print(\"\\nOptimization warning:\", result.message)\n        optimal_weights = initial_weights\n    else:\n        optimal_weights = result.x / np.sum(result.x)  \n    \n    print(\"\\nOptimized Weights:\")\n    for name, weight in zip(model_names, optimal_weights):\n        print(f\"{name}: {weight:.4f} (original score: {model_scores[name]:.4f})\")\n    \n    final_oof = np.zeros_like(oof_results[model_names[0]])\n    for i, name in enumerate(model_names):\n        final_oof += optimal_weights[i] * oof_results[name]\n    \n    final_top3 = np.argsort(final_oof, axis=1)[:, -3:][:, ::-1]\n    final_score = mapk(y_true, final_top3)\n    print(f\"\\nFinal Blended OOF MAP@3: {final_score:.4f}\")\n    \n    final_test_pred = np.zeros_like(test_predictions[model_names[0]])\n    for i, name in enumerate(model_names):\n        final_test_pred += optimal_weights[i] * test_predictions[name]\n    \n    return final_test_pred, optimal_weights\n\n\nblended_test_pred, optimal_weights = blend_predictions(oof_results, test_predictions, y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:48:47.901735Z","iopub.execute_input":"2025-06-16T12:48:47.902054Z","iopub.status.idle":"2025-06-16T12:49:03.819079Z","shell.execute_reply.started":"2025-06-16T12:48:47.902031Z","shell.execute_reply":"2025-06-16T12:49:03.818101Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"def create_submission(blended_preds, le, sample_sub_path, output_path='submission.csv'):\n    # Get top-3 predictions\n    top3_indices = np.argsort(blended_preds, axis=1)[:, -3:][:, ::-1]\n    \n    # Decode fertilizer names\n    fertilizer_names = []\n    for row in top3_indices:\n        names = le.inverse_transform(row)\n        fertilizer_names.append(' '.join(names))\n    \n    # Load sample submission\n    sample_sub = pd.read_csv(sample_sub_path)\n    \n    # Create submission\n    submission = sample_sub[['id']].copy()\n    submission['Fertilizer Name'] = fertilizer_names\n    \n    # Save\n    submission.to_csv(output_path, index=False)\n    print(f\"Submission saved to {output_path}\")\n    print(\"\\nSubmission head:\")\n    print(submission.head())\n    \n    return submission\n\n# Usage\nsubmission = create_submission(\n    blended_test_pred, \n    le,\n    '/kaggle/input/playground-series-s5e6/sample_submission.csv'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-16T12:49:03.82003Z","iopub.execute_input":"2025-06-16T12:49:03.820297Z","iopub.status.idle":"2025-06-16T12:49:28.767065Z","shell.execute_reply.started":"2025-06-16T12:49:03.820278Z","shell.execute_reply":"2025-06-16T12:49:28.766018Z"}},"outputs":[],"execution_count":null}]}