{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91716,"databundleVersionId":11893428,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":10522392,"sourceType":"datasetVersion","datasetId":6512323}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/klyushnik/s5e5-blend-work-0-05795?scriptVersionId=238865696\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:21.752034Z","iopub.execute_input":"2025-05-10T06:17:21.752494Z","iopub.status.idle":"2025-05-10T06:17:22.062836Z","shell.execute_reply.started":"2025-05-10T06:17:21.752468Z","shell.execute_reply":"2025-05-10T06:17:22.062129Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e5/sample_submission.csv\n/kaggle/input/playground-series-s5e5/train.csv\n/kaggle/input/playground-series-s5e5/test.csv\n/kaggle/input/calories-burnt-prediction/calories.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Description","metadata":{}},{"cell_type":"markdown","source":"![cals.jpg](https://www.saxoncrossfit.com/wp-content/uploads/2019/11/cals.jpg)\n\n**My name is Alexander and I present to your attention my vision of the dataset, model selection and results.**\n\n**Your Goal:** Your goal is to predict how many calories were burned during a workout.\n\n**Evaluation**\n\nThe evaluation metric for this competition is Root Mean Squared Logarithmic Error.\n\nThe RMSLE is calculated as:\n\n$$\n\\text{RMSLE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} \\left( \\log(y_i + 1) - \\log(\\hat{y}_i + 1) \\right)^2}\n$$","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport matplotlib\nfrom matplotlib.pyplot import figure\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.optimize import minimize\nfrom scipy.stats import mstats\nfrom scipy.stats.mstats import winsorize\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import (\n    LabelEncoder,\n    QuantileTransformer,\n    StandardScaler,\n    PowerTransformer,\n    MaxAbsScaler,\n    MinMaxScaler,\n    RobustScaler,\n    PolynomialFeatures,\n    OrdinalEncoder,\n    OneHotEncoder,\n    FunctionTransformer,\n    KBinsDiscretizer,\n)\nfrom sklearn.feature_selection import (\n    VarianceThreshold,\n    SelectKBest,\n    f_regression,\n    SequentialFeatureSelector,\n)\nfrom sklearn.model_selection import (\n    StratifiedKFold,\n    KFold,\n    StratifiedGroupKFold,\n    RepeatedStratifiedKFold,\n    RepeatedKFold,\n    cross_validate,\n    train_test_split,\n    TimeSeriesSplit,\n)\nfrom sklearn.linear_model import (\n    SGDOneClassSVM,\n    LinearRegression,\n    Ridge,\n    Lasso,\n    ElasticNet,\n)\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.ensemble import (\n    HistGradientBoostingRegressor,\n    ExtraTreesRegressor,\n    GradientBoostingRegressor,\n    IsolationForest,\n    BaggingRegressor,\n    RandomForestRegressor,\n)\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    mean_squared_error,\n    mean_absolute_error,\n    mean_absolute_percentage_error, \n    r2_score,\n    make_scorer\n)\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import clone_model\nimport keras\nfrom keras_tuner import RandomSearch\nfrom keras import layers\nfrom keras.layers import (\n    BatchNormalization,\n    Flatten,\n    Dense,\n    Dropout,\n    Activation,\n)\nfrom tensorflow.keras.models import Sequential\nfrom keras import backend as K\nimport keras_tuner\nfrom keras_tuner import Hyperband\nfrom functools import partial\n\nimport optuna\nfrom optuna.samplers import CmaEsSampler\nfrom optuna.pruners import MedianPruner\nimport optuna.visualization as vis\n\nfrom catboost import CatBoostRegressor\nimport xgboost as xgb\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingRegressor, StackingCVRegressor\nfrom category_encoders import TargetEncoder, MEstimateEncoder\n#from cuml.preprocessing import TargetEncoder\n\nimport requests\nimport holidays\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nfrom category_encoders import CatBoostEncoder, LeaveOneOutEncoder\n\nimport warnings\nimport re\nimport time\nimport logging\nfrom functools import partial\nfrom itertools import combinations\nfrom IPython.display import Image\n\nfrom functools import partial\n\n# Visualization settings\nplt.style.use('ggplot')\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (12, 8)\nsns.set_context(\"notebook\", font_scale=1.2)\nsns.set_style(\"whitegrid\")\n\n# Pandas settings\npd.options.mode.chained_assignment = None\n\n# Logging configuration\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Warnings configuration\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:22.063959Z","iopub.execute_input":"2025-05-10T06:17:22.064305Z","iopub.status.idle":"2025-05-10T06:17:45.762183Z","shell.execute_reply.started":"2025-05-10T06:17:22.064285Z","shell.execute_reply":"2025-05-10T06:17:45.761512Z"}},"outputs":[{"name":"stderr","text":"2025-05-10 06:17:25.247022: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746857845.499125      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746857845.569325      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Setting","metadata":{}},{"cell_type":"code","source":"def plot_numerical_features(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        sns.histplot(df[feature], bins=30, kde=True, ax=axes[i], color='skyblue', edgecolor='black')\n        axes[i].set_title(f'Distribution of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel(feature, fontsize=14)\n        axes[i].set_ylabel('Frequency', fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n\n        mean_value = df[feature].mean()\n        axes[i].axvline(mean_value, color='red', linestyle='--', label='Mean')\n        axes[i].legend()\n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_numerical_boxplots(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        sns.boxplot(x=df[feature], ax=axes[i], color='lightgreen')\n        axes[i].set_title(f'Boxplot of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel(feature, fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n\n        median_value = df[feature].median()\n        axes[i].axvline(median_value, color='orange', linestyle='--', label='Median')\n        axes[i].legend()\n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_qq_plot(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    ncols = 2\n    nrows = (len(num_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(num_features):\n        stats.probplot(df[feature], dist=\"norm\", plot=axes[i])\n        axes[i].set_title(f'QQ Plot of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel('Theoretical Quantiles', fontsize=14)\n        axes[i].set_ylabel('Sample Quantiles', fontsize=14)\n        axes[i].grid(True, linestyle='--', alpha= 0.7)  \n\n    plt.tight_layout()\n    plt.show()\n\ndef plot_correlation_matrix(df, method='spearman'):\n    num_df = df.select_dtypes(include=[np.number])\n    \n    corr = num_df.corr(method=method)\n    plt.figure(figsize=(14, 10))\n    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8}, linewidths=.5)\n    plt.title(f'Correlation Matrix ({method.capitalize()} Correlation)', fontsize=18, fontweight='bold')\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.show()\n\ndef plot_pairplot(df):\n    num_features = df.select_dtypes(include=[np.number]).columns\n    sns.pairplot(df[num_features], diag_kind='kde', plot_kws={'alpha': 0.6, 'edgecolor': 'k'}, height=2.5)\n    plt.suptitle('Pairplot of Numerical Features', y=1.02, fontsize=18, fontweight='bold')\n    plt.show()\n\ndef plot_categorical_features(df, ncols=2, top_n=None):\n    cat_features = df.select_dtypes(include=[object]).columns\n    nrows = (len(cat_features) + ncols - 1) // ncols\n\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, 6 * nrows))\n    axes = axes.flatten()\n\n    for i, feature in enumerate(cat_features):\n        if top_n is not None:\n            top_categories = df[feature].value_counts().nlargest(top_n).index\n            sns.countplot(data=df[df[feature].isin(top_categories)], y=feature, ax=axes[i], palette='viridis', order=top_categories)\n        else:\n            sns.countplot(data=df, y=feature, ax=axes[i], palette='viridis')\n        \n        axes[i].set_title(f'Count of {feature}', fontsize=18, fontweight='bold')\n        axes[i].set_xlabel('Count', fontsize=14)\n        axes[i].set_ylabel(feature, fontsize=14)\n        axes[i].tick_params(axis='y', rotation=0)\n        axes[i].grid(True, linestyle='--', alpha=0.7)  \n    for j in range(i + 1, len(axes)):\n        fig.delaxes(axes[j])\n\n    plt.tight_layout()\n    plt.show()\n\ndef PolynomialFeatures_labeled(input_df,power):\n   \n    poly = preprocessing.PolynomialFeatures(power)\n    output_nparray = poly.fit_transform(input_df)\n    powers_nparray = poly.powers_\n\n    input_feature_names = list(input_df.columns)\n    target_feature_names = [\"Constant Term\"]\n    for feature_distillation in powers_nparray[1:]:\n        intermediary_label = \"\"\n        final_label = \"\"\n        for i in range(len(input_feature_names)):\n            if feature_distillation[i] == 0:\n                continue\n            else:\n                variable = input_feature_names[i]\n                power = feature_distillation[i]\n                intermediary_label = \"%s+%d\" % (variable,power)\n                if final_label == \"\":         #If the final label isn't yet specified\n                    final_label = intermediary_label\n                else:\n                    final_label = final_label + \"x\" + intermediary_label\n        target_feature_names.append(final_label)\n    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n    return output_df\n\ndef variance_threshold(df,th):\n    var_thres=VarianceThreshold(threshold=th)\n    var_thres.fit(df)\n    new_cols = var_thres.get_support()\n    return df.iloc[:,new_cols]\n   \ndef optimize_memory_usage(df, print_size=True):\n    \"\"\"\n    Optimizes memory usage in a DataFrame by downcasting numeric columns.\n\n    Parameters:\n        df (pd.DataFrame): The DataFrame to optimize.\n        print_size (bool): If True, prints memory usage before and after optimization.\n\n    Returns:\n        pd.DataFrame: The optimized DataFrame.\n    \"\"\"\n    # Types for optimization.\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    \n    # Memory usage size before optimize (Mb).\n    before_size = df.memory_usage().sum() / 1024**2\n    \n    for column in df.columns:\n        column_type = df[column].dtype\n        \n        if column_type in numerics:\n            try:\n                if str(column_type).startswith('int'):\n                    df[column] = pd.to_numeric(df[column], downcast='integer')\n                else:\n                    df[column] = pd.to_numeric(df[column], downcast='float')\n                logger.info(f\"Optimized column {column}: {column_type} -> {df[column].dtype}\")\n            except Exception as e:\n                logger.error(f\"Failed to optimize column {column}: {e}\")\n    \n    # Memory usage size after optimize (Mb).\n    after_size = df.memory_usage().sum() / 1024**2\n    \n    if print_size:\n        print(\n            'Memory usage size: before {:5.4f} Mb - after {:5.4f} Mb ({:.1f}%).'.format(\n                before_size, after_size, 100 * (before_size - after_size) / before_size\n            )\n        )\n    \n    return df\n\ndef rmsle(y_true, y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    \n    if np.any(y_true < 0) or np.any(y_pred < 0):\n        raise ValueError(\"y_true and y_pred must not contain negative values!\")\n    \n    log_true = np.log1p(y_true)\n    log_pred = np.log1p(y_pred)\n    \n    squared_log_errors = (log_true - log_pred) ** 2\n    mean_squared_log_error = np.mean(squared_log_errors)\n    return np.sqrt(mean_squared_log_error)\n\ndef categorize_variable(df, column, labels):\n    \n    if len(labels) != 3:\n        raise ValueError(\"3 type\")\n    \n    bins = [-float('inf'), \n            df[column].quantile(0.25), \n            df[column].quantile(0.75), \n            float('inf')]\n    \n    df[f'{column}_group'] = pd.cut(df[column], bins=bins, labels=labels)\n    return df\n\ndef replace_outliers_with_mean(df, threshold=3):\n\n    df_clean = df.copy()\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    \n    for col in numeric_cols:\n        \n        z_scores = np.abs(stats.zscore(df[col], nan_policy='omit')) \n        \n        mean_val = df[col][z_scores <= threshold].mean()\n        \n        df_clean[col] = np.where(z_scores > threshold, mean_val, df[col])\n        \n    return df_clean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:45.762972Z","iopub.execute_input":"2025-05-10T06:17:45.763688Z","iopub.status.idle":"2025-05-10T06:17:45.793077Z","shell.execute_reply.started":"2025-05-10T06:17:45.763663Z","shell.execute_reply":"2025-05-10T06:17:45.792276Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"markdown","source":"## Load ","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s5e5/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e5/test.csv')\n\ntrain.shape, test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:45.79511Z","iopub.execute_input":"2025-05-10T06:17:45.795516Z","iopub.status.idle":"2025-05-10T06:17:47.139973Z","shell.execute_reply.started":"2025-05-10T06:17:45.795486Z","shell.execute_reply":"2025-05-10T06:17:47.139039Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"((750000, 9), (250000, 8))"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"calories = pd.read_csv('/kaggle/input/calories-burnt-prediction/calories.csv', index_col=\"User_ID\")\n\ncalories.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:47.141083Z","iopub.execute_input":"2025-05-10T06:17:47.141389Z","iopub.status.idle":"2025-05-10T06:17:47.195938Z","shell.execute_reply.started":"2025-05-10T06:17:47.141363Z","shell.execute_reply":"2025-05-10T06:17:47.194941Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 15000 entries, 14733363 to 11751526\nData columns (total 8 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Gender      15000 non-null  object \n 1   Age         15000 non-null  int64  \n 2   Height      15000 non-null  float64\n 3   Weight      15000 non-null  float64\n 4   Duration    15000 non-null  float64\n 5   Heart_Rate  15000 non-null  float64\n 6   Body_Temp   15000 non-null  float64\n 7   Calories    15000 non-null  float64\ndtypes: float64(6), int64(1), object(1)\nmemory usage: 1.0+ MB\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"calories = calories.rename(columns={\"Gender\": \"Sex\"})\n\ntrain = pd.concat([train, calories])\n\ntrain.shape, test.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:47.197023Z","iopub.execute_input":"2025-05-10T06:17:47.197332Z","iopub.status.idle":"2025-05-10T06:17:47.245096Z","shell.execute_reply.started":"2025-05-10T06:17:47.197304Z","shell.execute_reply":"2025-05-10T06:17:47.244286Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"((765000, 9), (250000, 8))"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"test = test.drop(['id'], axis =1)\ntrain = train.drop(['id'], axis =1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:47.246357Z","iopub.execute_input":"2025-05-10T06:17:47.246636Z","iopub.status.idle":"2025-05-10T06:17:47.285095Z","shell.execute_reply.started":"2025-05-10T06:17:47.246609Z","shell.execute_reply":"2025-05-10T06:17:47.284309Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"train_df = train.copy()\ntest_df = test.copy()\n\ntrain_df = train_df.drop(columns=['Calories', 'Sex'])\ntest_df = test_df.drop(columns=['Sex'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:47.285846Z","iopub.execute_input":"2025-05-10T06:17:47.286084Z","iopub.status.idle":"2025-05-10T06:17:47.344446Z","shell.execute_reply.started":"2025-05-10T06:17:47.286064Z","shell.execute_reply":"2025-05-10T06:17:47.343536Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Info","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:47.345338Z","iopub.execute_input":"2025-05-10T06:17:47.345555Z","iopub.status.idle":"2025-05-10T06:17:47.435635Z","shell.execute_reply.started":"2025-05-10T06:17:47.345539Z","shell.execute_reply":"2025-05-10T06:17:47.434946Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 765000 entries, 0 to 11751526\nData columns (total 8 columns):\n #   Column      Non-Null Count   Dtype  \n---  ------      --------------   -----  \n 0   Sex         765000 non-null  object \n 1   Age         765000 non-null  int64  \n 2   Height      765000 non-null  float64\n 3   Weight      765000 non-null  float64\n 4   Duration    765000 non-null  float64\n 5   Heart_Rate  765000 non-null  float64\n 6   Body_Temp   765000 non-null  float64\n 7   Calories    765000 non-null  float64\ndtypes: float64(6), int64(1), object(1)\nmemory usage: 52.5+ MB\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"train.head(5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:47.438444Z","iopub.execute_input":"2025-05-10T06:17:47.439136Z","iopub.status.idle":"2025-05-10T06:17:47.462544Z","shell.execute_reply.started":"2025-05-10T06:17:47.439114Z","shell.execute_reply":"2025-05-10T06:17:47.461739Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"      Sex  Age  Height  Weight  Duration  Heart_Rate  Body_Temp  Calories\n0    male   36   189.0    82.0      26.0       101.0       41.0     150.0\n1  female   64   163.0    60.0       8.0        85.0       39.7      34.0\n2  female   51   161.0    64.0       7.0        84.0       39.8      29.0\n3    male   20   192.0    90.0      25.0       105.0       40.7     140.0\n4  female   38   166.0    61.0      25.0       102.0       40.6     146.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>Height</th>\n      <th>Weight</th>\n      <th>Duration</th>\n      <th>Heart_Rate</th>\n      <th>Body_Temp</th>\n      <th>Calories</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>male</td>\n      <td>36</td>\n      <td>189.0</td>\n      <td>82.0</td>\n      <td>26.0</td>\n      <td>101.0</td>\n      <td>41.0</td>\n      <td>150.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>female</td>\n      <td>64</td>\n      <td>163.0</td>\n      <td>60.0</td>\n      <td>8.0</td>\n      <td>85.0</td>\n      <td>39.7</td>\n      <td>34.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>female</td>\n      <td>51</td>\n      <td>161.0</td>\n      <td>64.0</td>\n      <td>7.0</td>\n      <td>84.0</td>\n      <td>39.8</td>\n      <td>29.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>male</td>\n      <td>20</td>\n      <td>192.0</td>\n      <td>90.0</td>\n      <td>25.0</td>\n      <td>105.0</td>\n      <td>40.7</td>\n      <td>140.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>female</td>\n      <td>38</td>\n      <td>166.0</td>\n      <td>61.0</td>\n      <td>25.0</td>\n      <td>102.0</td>\n      <td>40.6</td>\n      <td>146.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"train.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:47.463385Z","iopub.execute_input":"2025-05-10T06:17:47.463623Z","iopub.status.idle":"2025-05-10T06:17:47.718455Z","shell.execute_reply.started":"2025-05-10T06:17:47.463604Z","shell.execute_reply":"2025-05-10T06:17:47.717794Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"               count        mean        std    min    25%    50%    75%    max\nAge         765000.0   41.447255  15.213677   20.0   28.0   40.0   52.0   79.0\nHeight      765000.0  174.693126  12.854173  123.0  164.0  174.0  185.0  222.0\nWeight      765000.0   75.142162  14.004122   36.0   63.0   74.0   87.0  132.0\nDuration    765000.0   15.423163   8.353421    1.0    8.0   15.0   23.0   30.0\nHeart_Rate  765000.0   95.484672   9.452476   67.0   88.0   95.0  103.0  128.0\nBody_Temp   765000.0   40.036041   0.779863   37.1   39.6   40.3   40.7   41.5\nCalories    765000.0   88.307424  62.396760    1.0   34.0   77.0  136.0  314.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Age</th>\n      <td>765000.0</td>\n      <td>41.447255</td>\n      <td>15.213677</td>\n      <td>20.0</td>\n      <td>28.0</td>\n      <td>40.0</td>\n      <td>52.0</td>\n      <td>79.0</td>\n    </tr>\n    <tr>\n      <th>Height</th>\n      <td>765000.0</td>\n      <td>174.693126</td>\n      <td>12.854173</td>\n      <td>123.0</td>\n      <td>164.0</td>\n      <td>174.0</td>\n      <td>185.0</td>\n      <td>222.0</td>\n    </tr>\n    <tr>\n      <th>Weight</th>\n      <td>765000.0</td>\n      <td>75.142162</td>\n      <td>14.004122</td>\n      <td>36.0</td>\n      <td>63.0</td>\n      <td>74.0</td>\n      <td>87.0</td>\n      <td>132.0</td>\n    </tr>\n    <tr>\n      <th>Duration</th>\n      <td>765000.0</td>\n      <td>15.423163</td>\n      <td>8.353421</td>\n      <td>1.0</td>\n      <td>8.0</td>\n      <td>15.0</td>\n      <td>23.0</td>\n      <td>30.0</td>\n    </tr>\n    <tr>\n      <th>Heart_Rate</th>\n      <td>765000.0</td>\n      <td>95.484672</td>\n      <td>9.452476</td>\n      <td>67.0</td>\n      <td>88.0</td>\n      <td>95.0</td>\n      <td>103.0</td>\n      <td>128.0</td>\n    </tr>\n    <tr>\n      <th>Body_Temp</th>\n      <td>765000.0</td>\n      <td>40.036041</td>\n      <td>0.779863</td>\n      <td>37.1</td>\n      <td>39.6</td>\n      <td>40.3</td>\n      <td>40.7</td>\n      <td>41.5</td>\n    </tr>\n    <tr>\n      <th>Calories</th>\n      <td>765000.0</td>\n      <td>88.307424</td>\n      <td>62.396760</td>\n      <td>1.0</td>\n      <td>34.0</td>\n      <td>77.0</td>\n      <td>136.0</td>\n      <td>314.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"**On average, a 40-year-old person who is 175 cm tall and weighs 75 kg exercises for 15 minutes and burns 88 calories.**","metadata":{}},{"cell_type":"code","source":"duplicates = train.duplicated()\nprint(f\"Number of duplicates: {duplicates.sum()}\")\n\ntrain = train.drop_duplicates()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:47.719247Z","iopub.execute_input":"2025-05-10T06:17:47.719476Z","iopub.status.idle":"2025-05-10T06:17:48.096383Z","shell.execute_reply.started":"2025-05-10T06:17:47.719452Z","shell.execute_reply":"2025-05-10T06:17:48.095595Z"}},"outputs":[{"name":"stdout","text":"Number of duplicates: 2893\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"for col in train.columns:\n    pct_missing = np.mean(train[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:48.097266Z","iopub.execute_input":"2025-05-10T06:17:48.097952Z","iopub.status.idle":"2025-05-10T06:17:48.156826Z","shell.execute_reply.started":"2025-05-10T06:17:48.09793Z","shell.execute_reply":"2025-05-10T06:17:48.155967Z"}},"outputs":[{"name":"stdout","text":"Sex - 0%\nAge - 0%\nHeight - 0%\nWeight - 0%\nDuration - 0%\nHeart_Rate - 0%\nBody_Temp - 0%\nCalories - 0%\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Optimize memory","metadata":{}},{"cell_type":"code","source":"train = optimize_memory_usage(train)\ntest = optimize_memory_usage(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:48.157597Z","iopub.execute_input":"2025-05-10T06:17:48.157844Z","iopub.status.idle":"2025-05-10T06:17:48.280671Z","shell.execute_reply.started":"2025-05-10T06:17:48.157825Z","shell.execute_reply":"2025-05-10T06:17:48.279915Z"}},"outputs":[{"name":"stdout","text":"Memory usage size: before 52.3297 Mb - after 29.7989 Mb (43.1%).\nMemory usage size: before 13.3516 Mb - after 6.9143 Mb (48.2%).\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"markdown","source":"## Numerical feature","metadata":{}},{"cell_type":"markdown","source":"### Hist","metadata":{}},{"cell_type":"code","source":"plot_numerical_features(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-10T06:17:48.281777Z","iopub.execute_input":"2025-05-10T06:17:48.282007Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Box","metadata":{}},{"cell_type":"code","source":"plot_numerical_boxplots(train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Q_Q","metadata":{}},{"cell_type":"code","source":"plot_qq_plot(train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Correlation matrix","metadata":{}},{"cell_type":"code","source":"plot_correlation_matrix(train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VIF","metadata":{}},{"cell_type":"code","source":"X = sm.add_constant(train.select_dtypes(include=[np.number]).iloc [:, 1:])\n\nVIFs = pd.DataFrame()\nVIFs['Variable'] = X.columns\nVIFs['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nprint(VIFs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Categorical features","metadata":{}},{"cell_type":"code","source":"plot_categorical_features(train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# New features","metadata":{}},{"cell_type":"code","source":"categorize_variable(train, 'Age', [\"young\", \"middle age\", 'old'])\ncategorize_variable(train, 'Height', [\"short\", \"middle\", 'high'])\ncategorize_variable(train, 'Weight', [\"thin\", \"normal\", 'fat'])\ncategorize_variable(train, 'Duration', [\"short\", \"mean\", 'long'])\ncategorize_variable(train, 'Heart_Rate', [\"low\", \"normal\", 'high'])\n\n\ncategorize_variable(test, 'Age', [\"young\", \"middle age\", 'old'])\ncategorize_variable(test, 'Height', [\"short\", \"middle\", 'high'])\ncategorize_variable(test, 'Weight', [\"thin\", \"normal\", 'fat'])\ncategorize_variable(test, 'Duration', [\"short\", \"mean\", 'long'])\ncategorize_variable(test, 'Heart_Rate', [\"low\", \"normal\", 'high'])\n\ntrain['BMI'] = train['Weight'] / (train['Height'] ** 2)\ntest['BMI'] = test['Weight'] / (test['Height'] ** 2)\n\n\ntrain.shape, test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['Sex'] = train['Sex'].astype(str)\ntrain['Age_group'] = train['Age_group'].astype(str)\ntest['Sex'] = test['Sex'].astype(str)\ntest['Age_group'] = test['Age_group'].astype(str)\n\ntrain['Sex_Age'] = train['Sex'] + '_' + train['Age_group']\ntest['Sex_Age'] = test['Sex'] + '_' + test['Age_group']\n\nencoders = {\n    'ME': MEstimateEncoder(cols=['Sex', 'Age_group', 'Sex_Age'], m=5.0),\n    'CatBoost': CatBoostEncoder(cols=['Sex', 'Age_group', 'Sex_Age']),\n    'LOO': LeaveOneOutEncoder(cols=['Sex', 'Age_group', 'Sex_Age'])\n}\n\nencoded_dfs = {}\ntest_encoded_dfs = {}\n\nfor name, encoder in encoders.items():\n    encode_cols = ['Sex', 'Age_group', 'Sex_Age']\n    train_encode = train[encode_cols].astype(str)\n    test_encode = test[encode_cols].astype(str)\n    \n    encoded = encoder.fit_transform(train_encode, train['Calories'])\n    encoded_dfs[f'{name}_encoded'] = encoded.add_prefix(f'{name}_')\n    \n    test_encoded = encoder.transform(test_encode)\n    test_encoded_dfs[f'{name}_encoded_test'] = test_encoded.add_prefix(f'{name}_')\n\ntrain = pd.concat([train] + list(encoded_dfs.values()), axis=1)\ntest = pd.concat([test] + list(test_encoded_dfs.values()), axis=1)\n\ntrain.shape, test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for col in ['Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']:\n    train[f'{col}_boxcox'], _ = stats.boxcox(train[col] + 1)  \n    test[f'{col}_boxcox'], _ = stats.boxcox(test[col] + 1)\n\n\ntrain['Duration_sin'] = np.sin(2 * np.pi * train['Duration']/24)\ntrain['Duration_cos'] = np.cos(2 * np.pi * train['Duration']/24)\ntest['Duration_sin'] = np.sin(2 * np.pi * test['Duration']/24)\ntest['Duration_cos'] = np.cos(2 * np.pi * test['Duration']/24)\n\nscalers = {\n    'standard': StandardScaler(),\n    'robust': RobustScaler(),\n    'minmax': MinMaxScaler()\n}\n\nfor name, scaler in scalers.items():\n    num_cols = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp', 'BMI']\n    train[[f'{name}_{col}' for col in num_cols]] = scaler.fit_transform(train[num_cols])\n    test[[f'{name}_{col}' for col in num_cols]] = scaler.transform(test[num_cols])\n\ntrain.shape, test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train['VO2max_approx'] = train['Duration'] * train['Heart_Rate'] / (train['Weight']**0.5)\ntest['VO2max_approx'] = test['Duration'] * test['Heart_Rate'] / (test['Weight']**0.5)\n\ntrain['Weight_Duration'] = train['Weight'] * train['Duration']\ntrain['Height_Weight'] = train['Height'] * train['Weight']\ntrain['HR_BodyTemp'] = train['Heart_Rate'] * train['Body_Temp']\n\ntest['Weight_Duration'] = test['Weight'] * test['Duration']\ntest['Height_Weight'] = test['Height'] * test['Weight']\ntest['HR_BodyTemp'] = test['Heart_Rate'] * test['Body_Temp']\n\ntrain.shape, test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# X,y make transform","metadata":{}},{"cell_type":"markdown","source":"## Target Encode","metadata":{}},{"cell_type":"code","source":"col = ['Age_group', 'Height_group', 'Weight_group', 'Duration_group',\n       'Heart_Rate_group', 'Sex', 'Sex_Age']\n\nTE = MEstimateEncoder(cols=col, m=5.0)\n\ntrain[col] = TE.fit_transform(train[col], train['Calories'])\ntest[col] = TE.transform(test[col])\n\ntrain.shape, test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Log and transform","metadata":{}},{"cell_type":"code","source":"transform = PowerTransformer(method='yeo-johnson')\ntransform = QuantileTransformer(n_quantiles=10, random_state=0)\n\nfor i in test_df.select_dtypes(include=[np.number]).columns:\n    train_df[i+' +log'] = (train_df[i]+1).transform(np.log)\n    test_df[i+' +log'] =(test_df[i]+1).transform(np.log)\n\n    train_df[i+' +log1'] = (train_df[i]+1).transform(np.log1p)\n    test_df[i+' +log1'] =(test_df[i]+1).transform(np.log1p)\n    \n    train_df[i+' +y_j'] = transform.fit_transform(train_df[[i]])\n    test_df[i+' +y_j'] = transform.fit_transform(test_df[[i]])\n    \n    train_df[i+' +q_t'] = transform.fit_transform(train_df[[i]])\n    test_df[i+' +q_t'] = transform.fit_transform(test_df[[i]])\n    \n    train_df[i+' +sqrt'] = (train_df[i]+1).transform(np.sqrt)\n    test_df[i+' +sqrt'] =(test_df[i]+1).transform(np.sqrt)\n    \ntrain_df.shape, test_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Polynomial Features","metadata":{}},{"cell_type":"code","source":"train_df = PolynomialFeatures_labeled(train_df, 2)\ntest_df = PolynomialFeatures_labeled(test_df, 2)\n\ntrain_df.shape, test_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Threshold","metadata":{}},{"cell_type":"code","source":"train_df = variance_threshold(train_df,0.03)\nlist_name = (train_df.columns)\ntest_df = test_df[list_name]\n\ntrain_df.shape, test_df.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Principal component analysis (PCA)","metadata":{}},{"cell_type":"code","source":"n_components = 4\n\npca = PCA(n_components=n_components)\n\npca_components = pca.fit_transform(train_df)\npca_components_test = pca.transform(test_df)\n\npca_df = pd.DataFrame(pca_components, columns=[f'PCA_{i+1}' for i in range(n_components)])\ntrain = pd.concat([train, pca_df], axis=1)\n\npca_df_test = pd.DataFrame(pca_components_test, columns=[f'PCA_{i+1}' for i in range(n_components)])\ntest = pd.concat([test, pca_df_test], axis=1)\n\ntrain.shape, test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"total_missing = train.isnull().sum().sum()\nprint(total_missing)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train.dropna(inplace=True)\ntest.dropna(inplace=True)\n\ntrain.shape, test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Split data, threshold","metadata":{}},{"cell_type":"code","source":"X = train.drop(columns=['Calories'])\ny = train['Calories']\nprint('before threshold:',X.shape, y.shape)\n\nX = variance_threshold(X,0.03)\nlist_name = (X.columns)\ntest = test[list_name]\n\nprint('after threshold:',X.shape, y.shape, test.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\n\nX[X.select_dtypes(include=[np.number]).columns] = scaler.fit_transform(X[X.select_dtypes(include=[np.number]).columns])\ntest[X.select_dtypes(include=[np.number]).columns] = scaler.transform(test[X.select_dtypes(include=[np.number]).columns])\n\nX.shape, y.shape, test.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## Parametrs","metadata":{}},{"cell_type":"code","source":"catboost_params = [ \n    \n                    {'iterations': 3000, 'depth': 6, 'learning_rate': 0.05, 'l2_leaf_reg': 3,\n                     'loss_function': 'RMSE', 'border_count': 32, 'bagging_temperature': 1,\n                     'random_strength': 1},\n    \n                   {'iterations': 4000, 'depth': 8, 'learning_rate': 0.03, 'l2_leaf_reg': 5,\n                    'loss_function': 'RMSE', 'border_count': 64, 'bagging_temperature': 0.5,\n                    'random_strength': 2},\n    \n                   {'iterations': 2900, 'depth': 4, 'learning_rate': 0.1, 'l2_leaf_reg': 10,\n                    'loss_function': 'RMSE', 'border_count': 32, 'bagging_temperature': 0.8,\n                    'random_strength': 1.5}, \n    \n                   {'iterations': 2300, 'depth': 7, 'learning_rate': 0.02, 'l2_leaf_reg': 7,\n                    'loss_function': 'RMSE', 'border_count': 64, 'bagging_temperature': 1,\n                    'random_strength': 1},\n    \n                   {'iterations': 3500, 'depth': 5, 'learning_rate': 0.04, 'l2_leaf_reg': 4, \n                    'loss_function': 'RMSE','border_count': 48, 'bagging_temperature': 0.7, \n                    'random_strength': 0.8, 'grow_policy': 'SymmetricTree'}, \n]\n\nxgb_params = [ \n    \n                {'n_estimators': 2000, 'max_depth': 6, 'learning_rate': 0.01,\n                'subsample': 0.8, 'eval_metric': 'rmse', 'colsample_bytree': 0.8, \n                'gamma': 0, 'min_child_weight': 1},\n              \n              {'n_estimators': 3000, 'max_depth': 8, 'learning_rate': 0.03,\n               'subsample': 0.9, 'eval_metric': 'rmse', 'colsample_bytree': 0.9, \n               'gamma': 0.1, 'min_child_weight': 2},\n    \n              {'n_estimators': 1900, 'max_depth': 4, 'learning_rate': 0.05,\n               'subsample': 0.7, 'eval_metric': 'rmse', 'colsample_bytree': 0.7,\n               'gamma': 0.2, 'min_child_weight': 3}, \n    \n              {'n_estimators': 4000, 'max_depth': 3, 'learning_rate': 0.005, \n               'subsample': 0.6, 'eval_metric': 'rmse', 'colsample_bytree': 0.6,\n               'gamma': 0.3, 'min_child_weight': 1},\n    \n              {'n_estimators': 2400, 'max_depth': 10, 'learning_rate': 0.02, \n               'subsample': 0.75, 'eval_metric': 'rmse', 'colsample_bytree': 0.75, \n               'gamma': 0.4, 'min_child_weight': 2},\n\n]\n\n\nlgbm_params = [ \n    \n                {'n_estimators': 2000, 'max_depth': 5, 'learning_rate': 0.01,\n                 'num_leaves': 31, 'metric': 'rmse', 'min_data_in_leaf': 20, \n                 'feature_fraction': 0.8, 'bagging_fraction': 0.8},\n               \n               {'n_estimators': 3000, 'max_depth': 8, 'learning_rate': 0.03, \n                'num_leaves': 63, 'metric': 'rmse', 'min_data_in_leaf': 15, \n                'feature_fraction': 0.9, 'bagging_fraction': 0.9},\n               \n               {'n_estimators': 5000, 'max_depth': 3, 'learning_rate': 0.05, \n                'num_leaves': 15, 'metric': 'rmse', 'min_data_in_leaf': 10, \n                'feature_fraction': 0.7, 'bagging_fraction': 0.7},\n               \n               {'n_estimators': 2300, 'max_depth': 12, 'learning_rate': 0.005, \n                'num_leaves': 127, 'metric': 'rmse', 'min_data_in_leaf': 5,\n                'feature_fraction': 0.6, 'bagging_fraction': 0.6},\n               \n               {'n_estimators': 2500, 'max_depth': 6, 'learning_rate': 0.02, \n                'num_leaves': 255, 'metric': 'rmse', 'min_data_in_leaf': 25, \n                'feature_fraction': 0.75, 'bagging_fraction': 0.75}, \n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model","metadata":{}},{"cell_type":"code","source":"def create_ensemble(X, y, test, n_folds=5, use_log_transform=True):\n    FOLDS = KFold(n_splits=5, shuffle=True, random_state=42)\n    \n    if use_log_transform:\n        y_transformed = np.log1p(y)\n        print(\"Applied log1p transformation to target variable\")\n    else:\n        y_transformed = y.copy()\n    \n    all_oof = {}\n    all_predictions = {}\n    models = []\n    \n    for i, params in enumerate(catboost_params, 1):\n        models.append((f'cat_{i}', CatBoostRegressor(**params, verbose=0, thread_count=1)))\n    \n    for i, params in enumerate(xgb_params, 1):\n        models.append((f'xgb_{i}', xgb.XGBRegressor(**params, n_jobs=1)))\n    \n    for i, params in enumerate(lgbm_params, 1):\n        models.append((f'lgb_{i}', LGBMRegressor(**params, verbose=-1, n_jobs=1)))\n\n    for i, params in enumerate(hgbm_params, 1):\n        models.append((f'hgb_{i}', HistGradientBoostingRegressor(**params)))\n    \n    for name, model in models:\n        try:\n            print(f\"\\nTraining {name}...\")\n            oof = np.zeros(len(X))\n            pred = np.zeros(len(test))\n            \n            for fold, (trn_idx, val_idx) in enumerate(FOLDS.split(X, y_transformed)):\n                X_train, y_train = X.iloc[trn_idx], y_transformed.iloc[trn_idx]\n                X_val, y_val = X.iloc[val_idx], y_transformed.iloc[val_idx]\n                \n                model.fit(X_train, y_train)\n                oof[val_idx] = model.predict(X_val)\n                pred += model.predict(test) / FOLDS.n_splits\n                \n                if use_log_transform:\n                    fold_rmsle = rmsle(np.expm1(y_val), np.expm1(oof[val_idx]))\n                else:\n                    fold_rmsle = rmsle(y_val, oof[val_idx])\n                print(f'{name} - Fold {fold} RMSLE: {fold_rmsle:.4f}')\n            \n            all_oof[name] = oof\n            all_predictions[name] = pred\n            \n            if use_log_transform:\n                full_rmsle = rmsle(y, np.expm1(oof))\n            else:\n                full_rmsle = rmsle(y, oof)\n            print(f'{name} - Full OOF RMSLE: {full_rmsle:.4f}')\n            \n        except Exception as e:\n            print(f\"Error training {name}: {str(e)}\")\n            continue\n    \n    oof_df = pd.DataFrame(all_oof)\n    predictions_df = pd.DataFrame(all_predictions)\n    \n    if use_log_transform:\n        oof_df['target'] = y.values\n        oof_df['target_transformed'] = y_transformed.values\n    else:\n        oof_df['target'] = y.values\n    \n    model_info = {\n        'model_names': [name for name, _ in models],\n        'num_models': len(models),\n        'features_used': list(X.columns),\n        'used_log_transform': use_log_transform\n    }\n    \n    if use_log_transform:\n        for col in predictions_df.columns:\n            predictions_df[col] = np.expm1(predictions_df[col])\n    \n    return oof_df, predictions_df, model_info","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Fit","metadata":{}},{"cell_type":"code","source":"oof_results, test_predictions, model_info = create_ensemble(X, y, test)\n    \noof_results.to_csv('oof_predictions.csv', index=False)\ntest_predictions.to_csv('test_predictions.csv', index=False)\n\nprint(\"\\nModeling completed successfully!\")\nprint(f\"Trained {model_info['num_models']} models\")\nprint(\"OOF predictions shape:\", oof_results.shape)\nprint(\"Test predictions shape:\", test_predictions.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Blend","metadata":{}},{"cell_type":"code","source":"\nmodel_columns = [col for col in oof_results.columns if col not in ['target', 'target_transformed']]\nmodel_scores = {name: rmsle(oof_results['target'], oof_results[name]) \n               for name in model_columns}\n\ninitial_weights = np.array([1/score for score in model_scores.values()])\ninitial_weights /= initial_weights.sum()\n\ndef objective(weights, alpha=0.01):\n    combined = sum(w*oof_results[model] for w, model in zip(weights, model_scores.keys()))\n    rmsle_val = rmsle(oof_results['target'], combined)\n    penalty = alpha * np.sum(weights**2)  # L2 регуляризация\n    return rmsle_val + penalty\n\nconstraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\nbounds = [(0,1)] * len(model_scores)\n\nresult = minimize(\n    objective,\n    initial_weights,\n    method='SLSQP',\n    bounds=bounds,\n    constraints=constraints,\n    options={'maxiter': 1000}\n)\n\nif not result.success:\n    print(\"Optimization warning:\", result.message)\n    optimal_weights = initial_weights\nelse:\n    optimal_weights = result.x\n\nprint(\"\\nOptimized weights:\")\nfor name, w in zip(model_scores.keys(), optimal_weights):\n    print(f\"{name}: {w:.4f} (RMSLE: {model_scores[name]:.4f})\")\n\noptimal_pred = sum(w*test_predictions[model] for w, model in zip(optimal_weights, model_scores.keys()))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv('/kaggle/input/playground-series-s5e5/sample_submission.csv')\nsample['Calories'] = optimal_pred\nsample.to_csv('submission.csv', index=False)\nsample.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}