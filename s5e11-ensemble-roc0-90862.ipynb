{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":91722,"databundleVersionId":14262372,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/klyushnik/s5e11-ensemble-roc-0-92268?scriptVersionId=281027553\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:21.819735Z","iopub.execute_input":"2025-11-20T13:46:21.820225Z","iopub.status.idle":"2025-11-20T13:46:22.125265Z","shell.execute_reply.started":"2025-11-20T13:46:21.820198Z","shell.execute_reply":"2025-11-20T13:46:22.124647Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/playground-series-s5e11/sample_submission.csv\n/kaggle/input/playground-series-s5e11/train.csv\n/kaggle/input/playground-series-s5e11/test.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Welcome!","metadata":{}},{"cell_type":"markdown","source":"![What-Is-Loan-Repayment-Why-It-Is-Important-How-It-Works.jpg](https://www.wintwealth.com/blog/wp-content/uploads/2023/01/What-Is-Loan-Repayment-Why-It-Is-Important-How-It-Works.jpg)","metadata":{}},{"cell_type":"markdown","source":"**Welcome to my code! Here I will present my vision and solution for the dataset, where the primary metric is Root Mean Squared Error.**\n\n**Enjoy watching, and please vote!**\n\n**My GitHub: https://github.com/Alexsandrrus!**","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\nimport matplotlib\nfrom matplotlib.pyplot import figure\n\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nfrom scipy.optimize import minimize\nfrom scipy.stats import mstats\nfrom scipy.stats.mstats import winsorize\n\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import (\n    LabelEncoder,\n    QuantileTransformer,\n    StandardScaler,\n    PowerTransformer,\n    MaxAbsScaler,\n    MinMaxScaler,\n    RobustScaler,\n    PolynomialFeatures,\n    OrdinalEncoder,\n    OneHotEncoder,\n    FunctionTransformer,\n    KBinsDiscretizer,\n)\nfrom sklearn.feature_selection import (\n    VarianceThreshold,\n    SelectKBest,\n    f_classif,  \n    SequentialFeatureSelector,\n    SelectFromModel\n)\nfrom sklearn.model_selection import (\n    StratifiedKFold,  \n    KFold,\n    StratifiedGroupKFold,\n    RepeatedStratifiedKFold,\n    RepeatedKFold,\n    cross_validate,\n    train_test_split,\n    TimeSeriesSplit,\n    cross_val_score\n)\nfrom sklearn.linear_model import (\n    SGDOneClassSVM,  \n    LogisticRegression,  \n    RidgeClassifier,\n    Ridge\n)\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (\n    HistGradientBoostingClassifier,  \n    ExtraTreesClassifier,  \n    GradientBoostingClassifier,  \n    IsolationForest,  \n    BaggingClassifier,  \n    RandomForestClassifier, \n    AdaBoostClassifier  \n)\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import (\n    accuracy_score,  \n    precision_score,  \n    recall_score,  \n    f1_score,  \n    classification_report, \n    confusion_matrix,  \n    roc_auc_score,  \n    make_scorer\n)\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.impute import KNNImputer, SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nimport optuna\nfrom optuna.samplers import CmaEsSampler\nfrom optuna.pruners import MedianPruner\nimport optuna.visualization as vis\n\nfrom catboost import CatBoostClassifier \nimport xgboost as xgb\nfrom xgboost import XGBClassifier  \nfrom lightgbm import LGBMClassifier  \nfrom mlxtend.classifier import StackingClassifier, StackingCVClassifier \n\nfrom category_encoders import TargetEncoder, MEstimateEncoder\n# from cuml.preprocessing import TargetEncoder  \n\nimport requests\nimport holidays\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nfrom category_encoders import CatBoostEncoder, LeaveOneOutEncoder\n\nimport warnings\nimport re\nimport time\nimport logging\nfrom functools import partial\nfrom itertools import combinations\nfrom IPython.display import Image\n\nfrom functools import partial\n\n# Visualization settings\nplt.style.use('ggplot')\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (12, 8)\nsns.set_context(\"notebook\", font_scale=1.2)\nsns.set_style(\"whitegrid\")\n\n# Pandas settings\npd.options.mode.chained_assignment = None\n\n# Logging configuration\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Warnings configuration\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:25.307355Z","iopub.execute_input":"2025-11-20T13:46:25.308122Z","iopub.status.idle":"2025-11-20T13:46:27.780788Z","shell.execute_reply.started":"2025-11-20T13:46:25.308091Z","shell.execute_reply":"2025-11-20T13:46:27.779925Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"def PolynomialFeatures_labeled(input_df,power):\n   \n    poly = preprocessing.PolynomialFeatures(power)\n    output_nparray = poly.fit_transform(input_df)\n    powers_nparray = poly.powers_\n\n    input_feature_names = list(input_df.columns)\n    target_feature_names = [\"Constant Term\"]\n    for feature_distillation in powers_nparray[1:]:\n        intermediary_label = \"\"\n        final_label = \"\"\n        for i in range(len(input_feature_names)):\n            if feature_distillation[i] == 0:\n                continue\n            else:\n                variable = input_feature_names[i]\n                power = feature_distillation[i]\n                intermediary_label = \"%s+%d\" % (variable,power)\n                if final_label == \"\":         #If the final label isn't yet specified\n                    final_label = intermediary_label\n                else:\n                    final_label = final_label + \"x\" + intermediary_label\n        target_feature_names.append(final_label)\n    output_df = pd.DataFrame(output_nparray, columns = target_feature_names)\n    return output_df\n\ndef variance_threshold(df,th):\n    var_thres=VarianceThreshold(threshold=th)\n    var_thres.fit(df)\n    new_cols = var_thres.get_support()\n    return df.iloc[:,new_cols]\n   \ndef optimize_memory_usage(df, print_size=True):\n    \"\"\"\n    Optimizes memory usage in a DataFrame by downcasting numeric columns.\n\n    Parameters:\n        df (pd.DataFrame): The DataFrame to optimize.\n        print_size (bool): If True, prints memory usage before and after optimization.\n\n    Returns:\n        pd.DataFrame: The optimized DataFrame.\n    \"\"\"\n    # Types for optimization.\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    \n    # Memory usage size before optimize (Mb).\n    before_size = df.memory_usage().sum() / 1024**2\n    \n    for column in df.columns:\n        column_type = df[column].dtype\n        \n        if column_type in numerics:\n            try:\n                if str(column_type).startswith('int'):\n                    df[column] = pd.to_numeric(df[column], downcast='integer')\n                else:\n                    df[column] = pd.to_numeric(df[column], downcast='float')\n                logger.info(f\"Optimized column {column}: {column_type} -> {df[column].dtype}\")\n            except Exception as e:\n                logger.error(f\"Failed to optimize column {column}: {e}\")\n    \n    # Memory usage size after optimize (Mb).\n    after_size = df.memory_usage().sum() / 1024**2\n    \n    if print_size:\n        print(\n            'Memory usage size: before {:5.4f} Mb - after {:5.4f} Mb ({:.1f}%).'.format(\n                before_size, after_size, 100 * (before_size - after_size) / before_size\n            )\n        )\n    \n    return df\n\ndef categorize_variable(df, column, labels):\n    \n    if len(labels) != 3:\n        raise ValueError(\"3 type\")\n    \n    bins = [-float('inf'), \n            df[column].quantile(0.25), \n            df[column].quantile(0.75), \n            float('inf')]\n    \n    df[f'{column}_group'] = pd.cut(df[column], bins=bins, labels=labels)\n    return df\n\ndef replace_outliers_with_mean(df, threshold=3):\n\n    df_clean = df.copy()\n    \n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    \n    for col in numeric_cols:\n        \n        z_scores = np.abs(stats.zscore(df[col], nan_policy='omit')) \n        \n        mean_val = df[col][z_scores <= threshold].mean()\n        \n        df_clean[col] = np.where(z_scores > threshold, mean_val, df[col])\n        \n    return df_clean","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:27.781973Z","iopub.execute_input":"2025-11-20T13:46:27.782543Z","iopub.status.idle":"2025-11-20T13:46:27.794415Z","shell.execute_reply.started":"2025-11-20T13:46:27.782521Z","shell.execute_reply":"2025-11-20T13:46:27.793583Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/playground-series-s5e11/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e11/test.csv')\n\ndisplay(train.shape, test.shape)\ndisplay(train.info(), test.info())\n\ndisplay(train.describe().T)\ndisplay(test.describe().T)\n\nduplicates = train.duplicated()\nprint(f\"Number of duplicates: {duplicates.sum()}\")\n\nduplicates = test.duplicated()\nprint(f\"Number of duplicates: {duplicates.sum()}\")\n\ntrain = train.drop_duplicates()\n\nfor col in train.columns:\n    pct_missing = np.mean(train[col].isnull())\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\ndisplay(train.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:30.17686Z","iopub.execute_input":"2025-11-20T13:46:30.17764Z","iopub.status.idle":"2025-11-20T13:46:32.536601Z","shell.execute_reply.started":"2025-11-20T13:46:30.177613Z","shell.execute_reply":"2025-11-20T13:46:32.535846Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"(593994, 13)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(254569, 12)"},"metadata":{}},{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 593994 entries, 0 to 593993\nData columns (total 13 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   id                    593994 non-null  int64  \n 1   annual_income         593994 non-null  float64\n 2   debt_to_income_ratio  593994 non-null  float64\n 3   credit_score          593994 non-null  int64  \n 4   loan_amount           593994 non-null  float64\n 5   interest_rate         593994 non-null  float64\n 6   gender                593994 non-null  object \n 7   marital_status        593994 non-null  object \n 8   education_level       593994 non-null  object \n 9   employment_status     593994 non-null  object \n 10  loan_purpose          593994 non-null  object \n 11  grade_subgrade        593994 non-null  object \n 12  loan_paid_back        593994 non-null  float64\ndtypes: float64(5), int64(2), object(6)\nmemory usage: 58.9+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 254569 entries, 0 to 254568\nData columns (total 12 columns):\n #   Column                Non-Null Count   Dtype  \n---  ------                --------------   -----  \n 0   id                    254569 non-null  int64  \n 1   annual_income         254569 non-null  float64\n 2   debt_to_income_ratio  254569 non-null  float64\n 3   credit_score          254569 non-null  int64  \n 4   loan_amount           254569 non-null  float64\n 5   interest_rate         254569 non-null  float64\n 6   gender                254569 non-null  object \n 7   marital_status        254569 non-null  object \n 8   education_level       254569 non-null  object \n 9   employment_status     254569 non-null  object \n 10  loan_purpose          254569 non-null  object \n 11  grade_subgrade        254569 non-null  object \ndtypes: float64(4), int64(2), object(6)\nmemory usage: 23.3+ MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                         count           mean            std       min  \\\nid                    593994.0  296996.500000  171471.442235     0.000   \nannual_income         593994.0   48212.202976   26711.942078  6002.430   \ndebt_to_income_ratio  593994.0       0.120696       0.068573     0.011   \ncredit_score          593994.0     680.916009      55.424956   395.000   \nloan_amount           593994.0   15020.297629    6926.530568   500.090   \ninterest_rate         593994.0      12.356345       2.008959     3.200   \nloan_paid_back        593994.0       0.798820       0.400883     0.000   \n\n                             25%         50%         75%         max  \nid                    148498.250  296996.500  445494.750  593993.000  \nannual_income          27934.400   46557.680   60981.320  393381.740  \ndebt_to_income_ratio       0.072       0.096       0.156       0.627  \ncredit_score             646.000     682.000     719.000     849.000  \nloan_amount            10279.620   15000.220   18858.580   48959.950  \ninterest_rate             10.990      12.370      13.680      20.990  \nloan_paid_back             1.000       1.000       1.000       1.000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id</th>\n      <td>593994.0</td>\n      <td>296996.500000</td>\n      <td>171471.442235</td>\n      <td>0.000</td>\n      <td>148498.250</td>\n      <td>296996.500</td>\n      <td>445494.750</td>\n      <td>593993.000</td>\n    </tr>\n    <tr>\n      <th>annual_income</th>\n      <td>593994.0</td>\n      <td>48212.202976</td>\n      <td>26711.942078</td>\n      <td>6002.430</td>\n      <td>27934.400</td>\n      <td>46557.680</td>\n      <td>60981.320</td>\n      <td>393381.740</td>\n    </tr>\n    <tr>\n      <th>debt_to_income_ratio</th>\n      <td>593994.0</td>\n      <td>0.120696</td>\n      <td>0.068573</td>\n      <td>0.011</td>\n      <td>0.072</td>\n      <td>0.096</td>\n      <td>0.156</td>\n      <td>0.627</td>\n    </tr>\n    <tr>\n      <th>credit_score</th>\n      <td>593994.0</td>\n      <td>680.916009</td>\n      <td>55.424956</td>\n      <td>395.000</td>\n      <td>646.000</td>\n      <td>682.000</td>\n      <td>719.000</td>\n      <td>849.000</td>\n    </tr>\n    <tr>\n      <th>loan_amount</th>\n      <td>593994.0</td>\n      <td>15020.297629</td>\n      <td>6926.530568</td>\n      <td>500.090</td>\n      <td>10279.620</td>\n      <td>15000.220</td>\n      <td>18858.580</td>\n      <td>48959.950</td>\n    </tr>\n    <tr>\n      <th>interest_rate</th>\n      <td>593994.0</td>\n      <td>12.356345</td>\n      <td>2.008959</td>\n      <td>3.200</td>\n      <td>10.990</td>\n      <td>12.370</td>\n      <td>13.680</td>\n      <td>20.990</td>\n    </tr>\n    <tr>\n      <th>loan_paid_back</th>\n      <td>593994.0</td>\n      <td>0.798820</td>\n      <td>0.400883</td>\n      <td>0.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n      <td>1.000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"                         count           mean           std         min  \\\nid                    254569.0  721278.000000  73487.884676  593994.000   \nannual_income         254569.0   48233.080193  26719.658580    6011.770   \ndebt_to_income_ratio  254569.0       0.120583      0.068582       0.011   \ncredit_score          254569.0     681.037691     55.624118     395.000   \nloan_amount           254569.0   15016.753484   6922.165182     500.050   \ninterest_rate         254569.0      12.352323      2.017602       3.200   \n\n                             25%         50%         75%         max  \nid                    657636.000  721278.000  784920.000  848562.000  \nannual_income          27950.300   46528.980   61149.440  380653.940  \ndebt_to_income_ratio       0.072       0.096       0.156       0.627  \ncredit_score             646.000     683.000     719.000     849.000  \nloan_amount            10248.580   15000.220   18831.460   48959.260  \ninterest_rate             10.980      12.370      13.690      21.290  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>mean</th>\n      <th>std</th>\n      <th>min</th>\n      <th>25%</th>\n      <th>50%</th>\n      <th>75%</th>\n      <th>max</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>id</th>\n      <td>254569.0</td>\n      <td>721278.000000</td>\n      <td>73487.884676</td>\n      <td>593994.000</td>\n      <td>657636.000</td>\n      <td>721278.000</td>\n      <td>784920.000</td>\n      <td>848562.000</td>\n    </tr>\n    <tr>\n      <th>annual_income</th>\n      <td>254569.0</td>\n      <td>48233.080193</td>\n      <td>26719.658580</td>\n      <td>6011.770</td>\n      <td>27950.300</td>\n      <td>46528.980</td>\n      <td>61149.440</td>\n      <td>380653.940</td>\n    </tr>\n    <tr>\n      <th>debt_to_income_ratio</th>\n      <td>254569.0</td>\n      <td>0.120583</td>\n      <td>0.068582</td>\n      <td>0.011</td>\n      <td>0.072</td>\n      <td>0.096</td>\n      <td>0.156</td>\n      <td>0.627</td>\n    </tr>\n    <tr>\n      <th>credit_score</th>\n      <td>254569.0</td>\n      <td>681.037691</td>\n      <td>55.624118</td>\n      <td>395.000</td>\n      <td>646.000</td>\n      <td>683.000</td>\n      <td>719.000</td>\n      <td>849.000</td>\n    </tr>\n    <tr>\n      <th>loan_amount</th>\n      <td>254569.0</td>\n      <td>15016.753484</td>\n      <td>6922.165182</td>\n      <td>500.050</td>\n      <td>10248.580</td>\n      <td>15000.220</td>\n      <td>18831.460</td>\n      <td>48959.260</td>\n    </tr>\n    <tr>\n      <th>interest_rate</th>\n      <td>254569.0</td>\n      <td>12.352323</td>\n      <td>2.017602</td>\n      <td>3.200</td>\n      <td>10.980</td>\n      <td>12.370</td>\n      <td>13.690</td>\n      <td>21.290</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Number of duplicates: 0\nNumber of duplicates: 0\nid - 0%\nannual_income - 0%\ndebt_to_income_ratio - 0%\ncredit_score - 0%\nloan_amount - 0%\ninterest_rate - 0%\ngender - 0%\nmarital_status - 0%\neducation_level - 0%\nemployment_status - 0%\nloan_purpose - 0%\ngrade_subgrade - 0%\nloan_paid_back - 0%\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   id  annual_income  debt_to_income_ratio  credit_score  loan_amount  \\\n0   0       29367.99                 0.084           736      2528.42   \n1   1       22108.02                 0.166           636      4593.10   \n2   2       49566.20                 0.097           694     17005.15   \n3   3       46858.25                 0.065           533      4682.48   \n4   4       25496.70                 0.053           665     12184.43   \n\n   interest_rate  gender marital_status education_level employment_status  \\\n0          13.67  Female         Single     High School     Self-employed   \n1          12.92    Male        Married        Master's          Employed   \n2           9.76    Male         Single     High School          Employed   \n3          16.10  Female         Single     High School          Employed   \n4          10.21    Male        Married     High School          Employed   \n\n         loan_purpose grade_subgrade  loan_paid_back  \n0               Other             C3             1.0  \n1  Debt consolidation             D3             0.0  \n2  Debt consolidation             C5             1.0  \n3  Debt consolidation             F1             1.0  \n4               Other             D1             1.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>annual_income</th>\n      <th>debt_to_income_ratio</th>\n      <th>credit_score</th>\n      <th>loan_amount</th>\n      <th>interest_rate</th>\n      <th>gender</th>\n      <th>marital_status</th>\n      <th>education_level</th>\n      <th>employment_status</th>\n      <th>loan_purpose</th>\n      <th>grade_subgrade</th>\n      <th>loan_paid_back</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>29367.99</td>\n      <td>0.084</td>\n      <td>736</td>\n      <td>2528.42</td>\n      <td>13.67</td>\n      <td>Female</td>\n      <td>Single</td>\n      <td>High School</td>\n      <td>Self-employed</td>\n      <td>Other</td>\n      <td>C3</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>22108.02</td>\n      <td>0.166</td>\n      <td>636</td>\n      <td>4593.10</td>\n      <td>12.92</td>\n      <td>Male</td>\n      <td>Married</td>\n      <td>Master's</td>\n      <td>Employed</td>\n      <td>Debt consolidation</td>\n      <td>D3</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>49566.20</td>\n      <td>0.097</td>\n      <td>694</td>\n      <td>17005.15</td>\n      <td>9.76</td>\n      <td>Male</td>\n      <td>Single</td>\n      <td>High School</td>\n      <td>Employed</td>\n      <td>Debt consolidation</td>\n      <td>C5</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>46858.25</td>\n      <td>0.065</td>\n      <td>533</td>\n      <td>4682.48</td>\n      <td>16.10</td>\n      <td>Female</td>\n      <td>Single</td>\n      <td>High School</td>\n      <td>Employed</td>\n      <td>Debt consolidation</td>\n      <td>F1</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>25496.70</td>\n      <td>0.053</td>\n      <td>665</td>\n      <td>12184.43</td>\n      <td>10.21</td>\n      <td>Male</td>\n      <td>Married</td>\n      <td>High School</td>\n      <td>Employed</td>\n      <td>Other</td>\n      <td>D1</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"for col in train.select_dtypes(include=[np.number]).columns:\n    if np.any(np.isinf(train[col])):\n        print(f\"Found infinite values in {col}, replacing with NaN\")\n        train[col] = train[col].replace([np.inf, -np.inf], np.nan)\n        \nfor col in test.select_dtypes(include=[np.number]).columns:\n    if np.any(np.isinf(test[col])):\n        print(f\"Found infinite values in test {col}, replacing with NaN\")\n        test[col] = test[col].replace([np.inf, -np.inf], np.nan)\n\nfor col in train.columns:\n    pct_missing = np.mean(train[col].isnull())\n    if pct_missing > 0:\n        print(f'{col} - {round(pct_missing*100)}%')\n\nfor col in train.columns:\n    if train[col].isnull().any():\n        if train[col].dtype in ['int64', 'float64']:\n            fill_value = train[col].median()\n            train[col] = train[col].fillna(fill_value)\n            test[col] = test[col].fillna(fill_value)\n            print(f\"Filled {col} with median: {fill_value}\")\n        else:\n            fill_value = train[col].mode()[0] if len(train[col].mode()) > 0 else 'Unknown'\n            train[col] = train[col].fillna(fill_value)\n            test[col] = test[col].fillna(fill_value)\n            print(f\"Filled {col} with mode: {fill_value}\")\n\nprint(\"\\n‚úÖ Final check for missing and infinite values:\")\nprint(f\"Missing values in train: {train.isnull().sum().sum()}\")\nprint(f\"Missing values in test: {test.isnull().sum().sum()}\")\nprint(f\"Infinite values in train: {np.any(np.isinf(train.select_dtypes(include=[np.number])))}\")\nprint(f\"Infinite values in test: {np.any(np.isinf(test.select_dtypes(include=[np.number])))}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:34.353063Z","iopub.execute_input":"2025-11-20T13:46:34.35335Z","iopub.status.idle":"2025-11-20T13:46:34.947503Z","shell.execute_reply.started":"2025-11-20T13:46:34.353319Z","shell.execute_reply":"2025-11-20T13:46:34.946786Z"}},"outputs":[{"name":"stdout","text":"\n‚úÖ Final check for missing and infinite values:\nMissing values in train: 0\nMissing values in test: 0\nInfinite values in train: False\nInfinite values in test: False\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Optimize memory","metadata":{}},{"cell_type":"code","source":"train = optimize_memory_usage(train)\ntest = optimize_memory_usage(test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:39.095439Z","iopub.execute_input":"2025-11-20T13:46:39.096068Z","iopub.status.idle":"2025-11-20T13:46:39.228261Z","shell.execute_reply.started":"2025-11-20T13:46:39.096041Z","shell.execute_reply":"2025-11-20T13:46:39.227446Z"}},"outputs":[{"name":"stderr","text":"INFO:__main__:Optimized column id: int64 -> int32\nINFO:__main__:Optimized column annual_income: float64 -> float64\nINFO:__main__:Optimized column debt_to_income_ratio: float64 -> float32\nINFO:__main__:Optimized column credit_score: int64 -> int16\nINFO:__main__:Optimized column loan_amount: float64 -> float64\nINFO:__main__:Optimized column interest_rate: float64 -> float32\nINFO:__main__:Optimized column loan_paid_back: float64 -> float32\nINFO:__main__:Optimized column id: int64 -> int32\nINFO:__main__:Optimized column annual_income: float64 -> float64\nINFO:__main__:Optimized column debt_to_income_ratio: float64 -> float32\nINFO:__main__:Optimized column credit_score: int64 -> int16\nINFO:__main__:Optimized column loan_amount: float64 -> float64\nINFO:__main__:Optimized column interest_rate: float64 -> float32\n","output_type":"stream"},{"name":"stdout","text":"Memory usage size: before 58.9137 Mb - after 46.4512 Mb (21.2%).\nMemory usage size: before 23.3066 Mb - after 18.9366 Mb (18.7%).\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Setting style for beautiful EDA\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nmatplotlib.rcParams['figure.figsize'] = (15, 10)\n\n# Create copies of data for analysis\ntrain_eda = train.copy()\ntest_eda = test.copy()\n\nprint(\"üéØ COMPREHENSIVE EDA OF CREDIT DATA\")\nprint(\"=\" * 70)\n\n# 1. BASIC DATA INFORMATION\nprint(\"\\nüìä 1. BASIC DATASET INFORMATION\")\nprint(f\"Training data size: {train_eda.shape}\")\nprint(f\"Test data size: {test_eda.shape}\")\nprint(f\"Train/test ratio: {len(train_eda)/len(test_eda):.2f}:1\")\n\n# 2. TARGET VARIABLE ANALYSIS\nprint(\"\\nüéØ 2. TARGET VARIABLE ANALYSIS (loan_paid_back)\")\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Target variable distribution\npaid_counts = train_eda['loan_paid_back'].value_counts()\ncolors = ['#ff6b6b', '#51cf66']\naxes[0].pie(paid_counts.values, labels=['Not Paid (0)', 'Paid Back (1)'], \n           autopct='%1.1f%%', colors=colors, startangle=90)\naxes[0].set_title('Target Variable Distribution\\n(loan_paid_back)', fontsize=14, fontweight='bold')\n\n# Class balance\nsns.countplot(data=train_eda, x='loan_paid_back', ax=axes[1], palette=colors)\naxes[1].set_title('Class Balance', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Loan Paid Back')\naxes[1].set_ylabel('Count')\nfor i, count in enumerate(paid_counts.values):\n    axes[1].text(i, count + 1000, f'{count:,}', ha='center', va='bottom', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"üî∏ Number of paid loans: {paid_counts[1]:,} ({paid_counts[1]/len(train_eda)*100:.1f}%)\")\nprint(f\"üî∏ Number of defaulted loans: {paid_counts[0]:,} ({paid_counts[0]/len(train_eda)*100:.1f}%)\")\n\n# 3. NUMERICAL VARIABLES DISTRIBUTION\nprint(\"\\nüìà 3. NUMERICAL VARIABLES DISTRIBUTION\")\nnumeric_cols = ['annual_income', 'debt_to_income_ratio', 'credit_score', \n                'loan_amount', 'interest_rate']\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(numeric_cols):\n    # Histogram with KDE\n    sns.histplot(data=train_eda, x=col, kde=True, ax=axes[i], alpha=0.7, color='skyblue')\n    axes[i].axvline(train_eda[col].mean(), color='red', linestyle='--', linewidth=2, \n                   label=f'Mean: {train_eda[col].mean():.2f}')\n    axes[i].axvline(train_eda[col].median(), color='green', linestyle='--', linewidth=2, \n                   label=f'Median: {train_eda[col].median():.2f}')\n    axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Frequency')\n    axes[i].legend()\n    \n    # Add boxplot to the last subplot\n    if i == len(numeric_cols) - 1:\n        sns.boxplot(data=train_eda[numeric_cols], ax=axes[i+1], palette=\"Set3\")\n        axes[i+1].set_title('Boxplot of All Numerical Variables', fontsize=12, fontweight='bold')\n        axes[i+1].tick_params(axis='x', rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n# 4. CORRELATION ANALYSIS\nprint(\"\\nüîó 4. CORRELATION MATRIX\")\n# Calculate correlations only for numerical columns\nnumeric_data = train_eda[numeric_cols + ['loan_paid_back']]\ncorrelation_matrix = numeric_data.corr()\n\nplt.figure(figsize=(12, 10))\nmask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n            square=True, fmt='.3f', cbar_kws={'shrink': 0.8}, mask=mask)\nplt.title('Correlation Matrix of Numerical Variables', fontsize=16, fontweight='bold')\nplt.tight_layout()\nplt.show()\n\n# Display top correlations with target variable\ntarget_correlations = correlation_matrix['loan_paid_back'].sort_values(ascending=False)\nprint(\"\\nüî∏ Correlations with target variable (loan_paid_back):\")\nfor feature, corr in target_correlations.items():\n    if feature != 'loan_paid_back':\n        print(f\"   {feature}: {corr:.4f}\")\n\n# 5. CATEGORICAL VARIABLES ANALYSIS\nprint(\"\\nüìä 5. CATEGORICAL VARIABLES ANALYSIS\")\ncategorical_cols = ['gender', 'marital_status', 'education_level', \n                   'employment_status', 'loan_purpose', 'grade_subgrade']\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(categorical_cols):\n    # Calculate percentages\n    value_counts = train_eda[col].value_counts()\n    percentages = (value_counts / len(train_eda) * 100).round(1)\n    \n    # Create bar plot\n    bars = axes[i].bar(range(len(value_counts)), value_counts.values, color=sns.color_palette(\"Set3\"))\n    axes[i].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Count')\n    axes[i].tick_params(axis='x', rotation=45)\n    \n    # Add percentage labels\n    for j, (bar, count, perc) in enumerate(zip(bars, value_counts.values, percentages.values)):\n        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n                    f'{perc}%', ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# 6. RELATIONSHIP WITH TARGET VARIABLE\nprint(\"\\nüîç 6. RELATIONSHIP WITH TARGET VARIABLE\")\n\n# For numerical variables\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(numeric_cols):\n    sns.boxplot(data=train_eda, x='loan_paid_back', y=col, ax=axes[i], palette=colors)\n    axes[i].set_title(f'{col} vs Loan Paid Back', fontsize=12, fontweight='bold')\n    axes[i].set_xlabel('Loan Paid Back')\n    axes[i].set_ylabel(col)\n\nplt.tight_layout()\nplt.show()\n\n# For categorical variables\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(categorical_cols):\n    # Calculate repayment rates by categories\n    paid_rate = train_eda.groupby(col)['loan_paid_back'].mean().sort_values(ascending=False)\n    \n    bars = axes[i].bar(range(len(paid_rate)), paid_rate.values * 100, color=sns.color_palette(\"viridis\", len(paid_rate)))\n    axes[i].set_title(f'Repayment Rate by {col}', fontsize=12, fontweight='bold')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Repayment Rate (%)')\n    axes[i].tick_params(axis='x', rotation=45)\n    axes[i].set_ylim(0, 100)\n    \n    # Add labels\n    for j, (bar, rate) in enumerate(zip(bars, paid_rate.values)):\n        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n                    f'{rate*100:.1f}%', ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# 7. OUTLIERS ANALYSIS\nprint(\"\\nüìä 7. OUTLIERS ANALYSIS IN NUMERICAL VARIABLES\")\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(numeric_cols):\n    # IQR method for outlier detection\n    Q1 = train_eda[col].quantile(0.25)\n    Q3 = train_eda[col].quantile(0.75)\n    IQR = Q3 - Q1\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    \n    outliers = train_eda[(train_eda[col] < lower_bound) | (train_eda[col] > upper_bound)]\n    outlier_percentage = (len(outliers) / len(train_eda)) * 100\n    \n    # Boxplot\n    sns.boxplot(data=train_eda, y=col, ax=axes[i], color='lightblue')\n    axes[i].set_title(f'{col}\\nOutliers: {outlier_percentage:.2f}%', fontsize=12, fontweight='bold')\n    axes[i].set_ylabel(col)\n\nplt.tight_layout()\nplt.show()\n\n# 8. COMPARISON OF TRAIN AND TEST DISTRIBUTIONS\nprint(\"\\nüî¨ 8. COMPARISON OF TRAIN AND TEST DISTRIBUTIONS\")\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 12))\naxes = axes.ravel()\n\nfor i, col in enumerate(numeric_cols):\n    # Compare distributions\n    sns.kdeplot(data=train_eda[col], label='Train', ax=axes[i], linewidth=2)\n    sns.kdeplot(data=test_eda[col], label='Test', ax=axes[i], linewidth=2)\n    axes[i].set_title(f'Distribution Comparison: {col}', fontsize=12, fontweight='bold')\n    axes[i].set_xlabel(col)\n    axes[i].set_ylabel('Density')\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n# 9. DETAILED STATISTICAL ANALYSIS\nprint(\"\\nüìã 9. DETAILED STATISTICAL SUMMARY\")\n\n# Function for skewness analysis\ndef analyze_skewness(series, name):\n    skew_val = series.skew()\n    if abs(skew_val) < 0.5:\n        skew_type = \"‚âà normal\"\n    elif abs(skew_val) < 1:\n        skew_type = \"moderate\"\n    else:\n        skew_type = \"strong\"\n    return skew_val, skew_type\n\nprint(\"\\nüî∏ Distribution skewness analysis:\")\nfor col in numeric_cols:\n    skew_val, skew_type = analyze_skewness(train_eda[col], col)\n    print(f\"   {col}: {skew_val:.3f} ({skew_type})\")\n\n# 10. ANALYSIS OF VARIABLE RELATIONSHIPS\nprint(\"\\nüîó 10. ANALYSIS OF KEY RELATIONSHIPS\")\n\n# Interesting variable pairs for analysis\ninteresting_pairs = [\n    ('annual_income', 'loan_amount'),\n    ('credit_score', 'interest_rate'),\n    ('debt_to_income_ratio', 'loan_paid_back'),\n    ('annual_income', 'credit_score')\n]\n\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\naxes = axes.ravel()\n\nfor i, (x_col, y_col) in enumerate(interesting_pairs):\n    if y_col == 'loan_paid_back':\n        sns.boxplot(data=train_eda, x=y_col, y=x_col, ax=axes[i], palette=colors)\n    else:\n        sns.scatterplot(data=train_eda, x=x_col, y=y_col, hue='loan_paid_back', \n                       alpha=0.6, ax=axes[i], palette=colors)\n    axes[i].set_title(f'{x_col} vs {y_col}', fontsize=12, fontweight='bold')\n    axes[i].set_xlabel(x_col)\n    axes[i].set_ylabel(y_col)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"‚úÖ EDA ANALYSIS COMPLETED\")\nprint(\"=\"*70)\n\n# Display key insights\nprint(\"\\nüéØ KEY INSIGHTS:\")\nprint(\"1. üìä Data is well balanced (~80% loans repaid)\")\nprint(\"2. üîç Highest correlation with target: credit_score, interest_rate\")\nprint(\"3. üìà Numerical variables distributions are close to normal\")\nprint(\"4. üéØ Test and train distributions are very similar - good sign\")\nprint(\"5. üìä Categorical variables have reasonable distribution\")\nprint(\"6. üöÄ Minimal number of outliers in the data\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# New features","metadata":{}},{"cell_type":"code","source":"def create_financial_features(df):\n    df = df.copy()\n    \n    df['income_to_loan_ratio'] = df['annual_income'] / df['loan_amount']\n    df['debt_burden'] = df['annual_income'] * df['debt_to_income_ratio']\n    df['loan_to_income_ratio'] = df['loan_amount'] / df['annual_income']\n    df['affordability_score'] = (df['annual_income'] * (1 - df['debt_to_income_ratio'])) / df['loan_amount']\n    \n    df['credit_income_interaction'] = df['credit_score'] * df['annual_income'] / 10000\n    df['risk_score'] = df['debt_to_income_ratio'] * df['interest_rate']\n    df['financial_stability'] = (df['credit_score'] / 100) * (1 - df['debt_to_income_ratio'])\n    \n    return df\n\ntrain = create_financial_features(train)\ntest = create_financial_features(test)\n\ndisplay(train.shape, test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:45.729289Z","iopub.execute_input":"2025-11-20T13:46:45.729994Z","iopub.status.idle":"2025-11-20T13:46:45.811505Z","shell.execute_reply.started":"2025-11-20T13:46:45.729971Z","shell.execute_reply":"2025-11-20T13:46:45.810795Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"(593994, 20)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(254569, 19)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"def extract_grade_features(df):\n    df = df.copy()\n    \n    df['grade_letter'] = df['grade_subgrade'].str[0]\n    df['grade_number'] = df['grade_subgrade'].str[1].astype(int)\n    \n    grade_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7}\n    df['grade_numeric'] = df['grade_letter'].map(grade_mapping)\n    \n    df['combined_grade_score'] = df['grade_numeric'] * 10 + df['grade_number']\n    \n    return df\n\ntrain = extract_grade_features(train)\ntest = extract_grade_features(test)\n\ndisplay(train.shape, test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:46.967321Z","iopub.execute_input":"2025-11-20T13:46:46.967939Z","iopub.status.idle":"2025-11-20T13:46:47.530074Z","shell.execute_reply.started":"2025-11-20T13:46:46.967912Z","shell.execute_reply":"2025-11-20T13:46:47.529506Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"(593994, 24)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(254569, 23)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"def create_risk_features(df):\n    df = df.copy()\n    \n    df['disposable_income'] = df['annual_income'] * (1 - df['debt_to_income_ratio'])\n    df['monthly_loan_payment'] = (df['loan_amount'] * df['interest_rate'] / 100) / 12\n    df['payment_to_income_ratio'] = df['monthly_loan_payment'] / (df['annual_income'] / 12)\n    \n    df['credit_debt_ratio'] = df['credit_score'] / (df['debt_to_income_ratio'] * 1000 + 1)\n    df['income_adequacy'] = df['annual_income'] / (df['loan_amount'] + 1)\n    \n    df['high_risk_flag'] = ((df['debt_to_income_ratio'] > 0.4) | \n                           (df['credit_score'] < 600) | \n                           (df['interest_rate'] > 15)).astype(int)\n    \n    df['medium_risk_flag'] = ((df['debt_to_income_ratio'] > 0.2) & \n                             (df['debt_to_income_ratio'] <= 0.4) & \n                             (df['credit_score'] >= 600)).astype(int)\n    \n    return df\n\ntrain = create_risk_features(train)\ntest = create_risk_features(test)\n\ndisplay(train.shape, test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:48.133687Z","iopub.execute_input":"2025-11-20T13:46:48.134259Z","iopub.status.idle":"2025-11-20T13:46:48.395062Z","shell.execute_reply.started":"2025-11-20T13:46:48.134237Z","shell.execute_reply":"2025-11-20T13:46:48.394438Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"(593994, 31)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(254569, 30)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def create_categorical_features(df):\n    df = df.copy()\n    \n    df['employment_education'] = df['employment_status'] + '_' + df['education_level']\n    df['gender_marital'] = df['gender'] + '_' + df['marital_status']\n    df['purpose_grade'] = df['loan_purpose'] + '_' + df['grade_letter']\n    \n    df['is_debt_consolidation'] = (df['loan_purpose'] == 'Debt consolidation').astype(int)\n    df['is_employed'] = (df['employment_status'] == 'Employed').astype(int)\n    df['is_high_education'] = df['education_level'].isin(['Master\\'s', 'Doctorate']).astype(int)\n    df['is_married'] = (df['marital_status'] == 'Married').astype(int)\n    \n    purpose_counts = df['loan_purpose'].value_counts()\n    df['purpose_frequency'] = df['loan_purpose'].map(purpose_counts)\n    \n    education_rank = {'High School': 1, 'Bachelor\\'s': 2, 'Master\\'s': 3, 'Doctorate': 4}\n    df['education_rank'] = df['education_level'].map(education_rank)\n    \n    return df\n\ntrain = create_categorical_features(train)\ntest = create_categorical_features(test)\n\ndisplay(train.shape, test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:49.285515Z","iopub.execute_input":"2025-11-20T13:46:49.285941Z","iopub.status.idle":"2025-11-20T13:46:50.353666Z","shell.execute_reply.started":"2025-11-20T13:46:49.285915Z","shell.execute_reply":"2025-11-20T13:46:50.352988Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"(593994, 40)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(254569, 39)"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def create_creative_features(df):\n    df = df.copy()\n    \n    df['financial_health_score'] = (\n        (df['credit_score'] / 850) * 0.4 +\n        (1 - df['debt_to_income_ratio']) * 0.3 +\n        (1 / (1 + df['loan_to_income_ratio'])) * 0.3\n    )\n    \n    conditions = [\n        (df['credit_score'] >= 700) & (df['debt_to_income_ratio'] <= 0.2),\n        (df['credit_score'] >= 650) & (df['debt_to_income_ratio'] <= 0.3),\n        (df['credit_score'] >= 600) & (df['debt_to_income_ratio'] <= 0.4),\n        (df['credit_score'] < 600) | (df['debt_to_income_ratio'] > 0.4)\n    ]\n    risk_levels = [1, 2, 3, 4]  \n    df['risk_level'] = np.select(conditions, risk_levels, default=3)\n    \n    df['id_recency'] = df['id'].rank(pct=True)\n    \n    df['comprehensive_score'] = (\n        df['credit_score'] * 0.5 + \n        (1 - df['debt_to_income_ratio']) * 200 + \n        (df['annual_income'] / df['loan_amount']) * 50\n    )\n    \n    return df\n\ntrain = create_creative_features(train)\ntest = create_creative_features(test)\n\ndisplay(train.shape, test.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:50.48865Z","iopub.execute_input":"2025-11-20T13:46:50.489287Z","iopub.status.idle":"2025-11-20T13:46:51.041945Z","shell.execute_reply.started":"2025-11-20T13:46:50.489267Z","shell.execute_reply":"2025-11-20T13:46:51.041168Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"(593994, 44)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(254569, 43)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"test = test.drop(['id'], axis =1)\ntrain = train.drop(['id'], axis =1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:52.327419Z","iopub.execute_input":"2025-11-20T13:46:52.32804Z","iopub.status.idle":"2025-11-20T13:46:52.481219Z","shell.execute_reply.started":"2025-11-20T13:46:52.328016Z","shell.execute_reply":"2025-11-20T13:46:52.480506Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Split data & threshold","metadata":{}},{"cell_type":"code","source":"cat_features = train.select_dtypes(include=[object]).columns\ncat_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:46:54.92038Z","iopub.execute_input":"2025-11-20T13:46:54.920721Z","iopub.status.idle":"2025-11-20T13:46:55.008671Z","shell.execute_reply.started":"2025-11-20T13:46:54.920698Z","shell.execute_reply":"2025-11-20T13:46:55.007995Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Index(['gender', 'marital_status', 'education_level', 'employment_status',\n       'loan_purpose', 'grade_subgrade', 'grade_letter',\n       'employment_education', 'gender_marital', 'purpose_grade'],\n      dtype='object')"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"X = train.drop(columns=['loan_paid_back'])\ny = train['loan_paid_back']\n\nprint(\"üîç Checking for infinite values...\")\n\nfor df, name in [(X, 'X'), (test, 'test')]:\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        if np.any(np.isinf(df[col])):\n            print(f\"Found infinite values in {name}.{col}, replacing with NaN\")\n            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n\nprint(\"\\nüéØ Target Encoding categorical features...\")\nfor col in cat_features:\n    if col in X.columns:\n        print(f\"Encoding {col}...\")\n        le = TargetEncoder()\n        \n        le.fit(X[col].astype(str), y)\n        \n        X[col] = le.transform(X[col].astype(str))\n        test[col] = le.transform(test[col].astype(str))\n\nprint(\"\\nüìä Analyzing missing values...\")\n\nmissing_summary = []\nfor col in X.columns:\n    pct_missing = np.mean(X[col].isnull())\n    if pct_missing > 0:\n        missing_summary.append((col, round(pct_missing*100, 2)))\n        \nif missing_summary:\n    print(\"Columns with missing values:\")\n    for col, pct in sorted(missing_summary, key=lambda x: x[1], reverse=True):\n        print(f'  {col} - {pct}%')\nelse:\n    print(\"No missing values found!\")\n\nprint(\"\\nüîÑ Filling missing values...\")\n\nfor col in X.columns:\n    if X[col].isnull().any():\n        if X[col].dtype in ['int64', 'float64']:\n            fill_value = X[col].median()\n            X[col] = X[col].fillna(fill_value)\n            test[col] = test[col].fillna(fill_value)\n            print(f\"Filled numeric {col} with median: {fill_value:.4f}\")\n        else:\n            fill_value = X[col].mode()[0] if len(X[col].mode()) > 0 else 'Unknown'\n            X[col] = X[col].fillna(fill_value)\n            test[col] = test[col].fillna(fill_value)\n            print(f\"Filled categorical {col} with mode: {fill_value}\")\n\nprint(\"\\n‚úÖ Final data quality check:\")\nprint(f\"Missing values in X: {X.isnull().sum().sum()}\")\nprint(f\"Missing values in test: {test.isnull().sum().sum()}\")\n\nnumeric_cols_X = X.select_dtypes(include=[np.number])\nnumeric_cols_test = test.select_dtypes(include=[np.number])\n\nprint(f\"Infinite values in X: {np.any(np.isinf(numeric_cols_X)) if not numeric_cols_X.empty else 'No numeric cols'}\")\nprint(f\"Infinite values in test: {np.any(np.isinf(numeric_cols_test)) if not numeric_cols_test.empty else 'No numeric cols'}\")\n\nprint(\"\\nüéØ Applying variance threshold...\")\n\nX = variance_threshold(X, 0.004)\nselected_features = list(X.columns)\ntest = test[selected_features]\n\nprint(f\"Features after variance threshold: {len(selected_features)}\")\n\nprint(\"\\nüìà Final dataset shapes:\")\nprint(f\"X shape: {X.shape}\")\nprint(f\"y shape: {y.shape}\") \nprint(f\"test shape: {test.shape}\")\n\nprint(\"\\nüìä Data types summary:\")\ndisplay(X.info())\nprint(\"\\nTest set info:\")\ndisplay(test.info())\n\nprint(\"\\nüéØ Target distribution:\")\nprint(y.value_counts(normalize=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T13:49:24.539868Z","iopub.execute_input":"2025-11-20T13:49:24.540667Z","iopub.status.idle":"2025-11-20T13:49:30.410798Z","shell.execute_reply.started":"2025-11-20T13:49:24.540638Z","shell.execute_reply":"2025-11-20T13:49:30.410198Z"}},"outputs":[{"name":"stdout","text":"üîç Checking for infinite values...\n\nüéØ Target Encoding categorical features...\nEncoding gender...\nEncoding marital_status...\nEncoding education_level...\nEncoding employment_status...\nEncoding loan_purpose...\nEncoding grade_subgrade...\nEncoding grade_letter...\nEncoding employment_education...\nEncoding gender_marital...\nEncoding purpose_grade...\n\nüìä Analyzing missing values...\nColumns with missing values:\n  education_rank - 6.35%\n\nüîÑ Filling missing values...\nFilled numeric education_rank with median: 2.0000\n\n‚úÖ Final data quality check:\nMissing values in X: 0\nMissing values in test: 0\nInfinite values in X: False\nInfinite values in test: False\n\nüéØ Applying variance threshold...\nFeatures after variance threshold: 31\n\nüìà Final dataset shapes:\nX shape: (593994, 31)\ny shape: (593994,)\ntest shape: (254569, 31)\n\nüìä Data types summary:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 593994 entries, 0 to 593993\nData columns (total 31 columns):\n #   Column                     Non-Null Count   Dtype  \n---  ------                     --------------   -----  \n 0   annual_income              593994 non-null  float64\n 1   credit_score               593994 non-null  int16  \n 2   loan_amount                593994 non-null  float64\n 3   interest_rate              593994 non-null  float32\n 4   employment_status          593994 non-null  float64\n 5   income_to_loan_ratio       593994 non-null  float64\n 6   debt_burden                593994 non-null  float64\n 7   loan_to_income_ratio       593994 non-null  float64\n 8   affordability_score        593994 non-null  float64\n 9   credit_income_interaction  593994 non-null  float64\n 10  risk_score                 593994 non-null  float32\n 11  financial_stability        593994 non-null  float64\n 12  grade_number               593994 non-null  int64  \n 13  grade_numeric              593994 non-null  int64  \n 14  combined_grade_score       593994 non-null  int64  \n 15  disposable_income          593994 non-null  float64\n 16  monthly_loan_payment       593994 non-null  float64\n 17  credit_debt_ratio          593994 non-null  float32\n 18  income_adequacy            593994 non-null  float64\n 19  high_risk_flag             593994 non-null  int64  \n 20  medium_risk_flag           593994 non-null  int64  \n 21  employment_education       593994 non-null  float64\n 22  is_debt_consolidation      593994 non-null  int64  \n 23  is_employed                593994 non-null  int64  \n 24  is_high_education          593994 non-null  int64  \n 25  is_married                 593994 non-null  int64  \n 26  purpose_frequency          593994 non-null  int64  \n 27  education_rank             593994 non-null  float64\n 28  risk_level                 593994 non-null  int64  \n 29  id_recency                 593994 non-null  float64\n 30  comprehensive_score        593994 non-null  float64\ndtypes: float32(3), float64(16), int16(1), int64(11)\nmemory usage: 130.3 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"name":"stdout","text":"\nTest set info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 254569 entries, 0 to 254568\nData columns (total 31 columns):\n #   Column                     Non-Null Count   Dtype  \n---  ------                     --------------   -----  \n 0   annual_income              254569 non-null  float64\n 1   credit_score               254569 non-null  int16  \n 2   loan_amount                254569 non-null  float64\n 3   interest_rate              254569 non-null  float32\n 4   employment_status          254569 non-null  float64\n 5   income_to_loan_ratio       254569 non-null  float64\n 6   debt_burden                254569 non-null  float64\n 7   loan_to_income_ratio       254569 non-null  float64\n 8   affordability_score        254569 non-null  float64\n 9   credit_income_interaction  254569 non-null  float64\n 10  risk_score                 254569 non-null  float32\n 11  financial_stability        254569 non-null  float64\n 12  grade_number               254569 non-null  int64  \n 13  grade_numeric              254569 non-null  int64  \n 14  combined_grade_score       254569 non-null  int64  \n 15  disposable_income          254569 non-null  float64\n 16  monthly_loan_payment       254569 non-null  float64\n 17  credit_debt_ratio          254569 non-null  float32\n 18  income_adequacy            254569 non-null  float64\n 19  high_risk_flag             254569 non-null  int64  \n 20  medium_risk_flag           254569 non-null  int64  \n 21  employment_education       254569 non-null  float64\n 22  is_debt_consolidation      254569 non-null  int64  \n 23  is_employed                254569 non-null  int64  \n 24  is_high_education          254569 non-null  int64  \n 25  is_married                 254569 non-null  int64  \n 26  purpose_frequency          254569 non-null  int64  \n 27  education_rank             254569 non-null  float64\n 28  risk_level                 254569 non-null  int64  \n 29  id_recency                 254569 non-null  float64\n 30  comprehensive_score        254569 non-null  float64\ndtypes: float32(3), float64(16), int16(1), int64(11)\nmemory usage: 55.8 MB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"None"},"metadata":{}},{"name":"stdout","text":"\nüéØ Target distribution:\nloan_paid_back\n1.0    0.79882\n0.0    0.20118\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Optuna","metadata":{}},{"cell_type":"code","source":"def optimize_catboost_classification(X, y, n_trials=15, cv=5):\n    \n    def objective(trial):\n        params = {\n            'iterations': trial.suggest_int('iterations', 100, 3000),\n            'depth': trial.suggest_int('depth', 4, 10),\n            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 10),\n            'border_count': trial.suggest_int('border_count', 32, 255),\n            'random_strength': trial.suggest_float('random_strength', 0, 2),\n            'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 1),\n            'grow_policy': trial.suggest_categorical('grow_policy', ['SymmetricTree', 'Depthwise']),\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 50),\n            'loss_function': 'Logloss',\n            'eval_metric': 'AUC',\n            'task_type': 'GPU', \n            'verbose': False,\n            'early_stopping_rounds': 100,\n            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.1, 10.0)\n        }\n\n        model = CatBoostClassifier(**params)\n        \n        scores = cross_val_score(model, X, y, cv=cv, \n                               scoring='roc_auc', n_jobs=1)\n        \n        return scores.mean()\n    \n    study = optuna.create_study(direction='maximize')  \n    study.optimize(objective, n_trials=n_trials)\n    \n    return study\n\ncatboost_studies = []\nfor i in range(3):\n    print(f\"\\nRunning CatBoost Classification optimization {i+1}/3\")\n    study = optimize_catboost_classification(X, y, n_trials=15)\n    catboost_studies.append(study)\n    print(f\"Best trial {i+1}:\")\n    print(f\"  ROC-AUC: {study.best_value:.5f}\")\n    print(f\"  Params: {study.best_params}\")\n\ncatboost_best_params = []\n\nfor i, study in enumerate(catboost_studies):\n    params = study.best_params.copy()\n    params['loss_function'] = 'Logloss'\n    params['eval_metric'] = 'AUC'\n    params['verbose'] = False\n    params['task_type'] = 'GPU'\n    catboost_best_params.append(params)\n    print(f\"\\nBest parameters for model {i+1}:\")\n    for key, value in params.items():\n        print(f\"  {key}: {value}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"OPTIMIZATION SUMMARY\")\nprint(\"=\"*50)\nfor i, (study, params) in enumerate(zip(catboost_studies, catboost_best_params)):\n    print(f\"Model {i+1}: ROC-AUC = {study.best_value:.5f}\")\n\nconfig_0 = {\n\n        'iterations': 2921,\n        'depth': 8,\n        'learning_rate': 0.01816721169731064,\n        'l2_leaf_reg': 6.901118902592421,\n        'border_count': 226,\n        'random_strength': 1.2775341874094779,\n        'bagging_temperature': 0.2039594590592117,\n        'grow_policy': 'Depthwise',\n        'min_data_in_leaf': 32,\n        'scale_pos_weight': 2.8174668679789066,\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'verbose': False,\n        'task_type': 'GPU'\n\n}\n\nconfig_1 = {\n\n        'iterations': 2335,\n        'depth': 7,\n        'learning_rate': 0.0536466110605888,\n        'l2_leaf_reg': 9.14873264737344,\n        'border_count': 184,\n        'random_strength': 1.9114396745170756,\n        'bagging_temperature': 0.08999986547803351,\n        'grow_policy': 'SymmetricTree',\n        'min_data_in_leaf': 17,\n        'scale_pos_weight': 7.02365672600217,\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'verbose': False,\n        'task_type': 'GPU'\n\n}\n\nconfig_2 = {\n    \n        'iterations': 2184,\n        'depth': 4,\n        'learning_rate': 0.06273309004713383,\n        'l2_leaf_reg': 6.373791122546431,\n        'border_count': 212,\n        'random_strength': 0.6760575153006142,\n        'bagging_temperature': 0.0023685021637712238,\n        'grow_policy': 'Depthwise',\n        'min_data_in_leaf': 4,\n        'scale_pos_weight': 2.8285899843487243,\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'verbose': False,\n        'task_type': 'GPU'\n    \n}\n\nconfig_3 = {\n    \n        'iterations': 2844,\n        'depth': 4,\n        'learning_rate': 0.0911136296079596,\n        'l2_leaf_reg': 9.992781300877866,\n        'border_count': 255,\n        'random_strength': 0.018385667495696006,\n        'bagging_temperature': 0.6553300237177059,\n        'grow_policy': 'Depthwise',\n        'min_data_in_leaf': 1,\n        'scale_pos_weight': 1.952645400055282,\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'verbose': False,\n        'task_type': 'GPU'\n    \n}\n\ncatboost_best_params.append(config_0)\ncatboost_best_params.append(config_1)\ncatboost_best_params.append(config_2)\ncatboost_best_params.append(config_3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize_xgboost_classification(X, y, n_trials=15, cv=5):\n    \n    def objective(trial):\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n            'gamma': trial.suggest_float('gamma', 0, 1),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n            'reg_lambda': trial.suggest_float('reg_lambda', 0.1, 2),\n            'reg_alpha': trial.suggest_float('reg_alpha', 0, 1),\n            'max_delta_step': trial.suggest_int('max_delta_step', 0, 5),\n            'eval_metric': 'auc',\n            'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n            'max_leaves': trial.suggest_int('max_leaves', 32, 256),\n            'max_bin': trial.suggest_int('max_bin', 128, 256),\n            'tree_method': 'gpu_hist',\n            'predictor': 'gpu_predictor',\n            'sampling_method': trial.suggest_categorical('sampling_method', ['uniform', 'gradient_based']),\n            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.1, 10.0)\n        }\n        \n        model = xgb.XGBClassifier(**params)\n        \n        scores = cross_val_score(model, X, y, cv=cv, \n                               scoring='roc_auc', n_jobs=-1)\n        \n        return scores.mean()\n    \n    study = optuna.create_study(direction='maximize')  \n    study.optimize(objective, n_trials=n_trials)\n    \n    return study\n\n\nxgb_studies = []\nfor i in range(3):\n    print(f\"\\nRunning XGBoost Classification optimization {i+1}/3\")\n    study = optimize_xgboost_classification(X, y, n_trials=15)\n    xgb_studies.append(study)\n    print(f\"Best trial {i+1}:\")\n    print(f\"  ROC-AUC: {study.best_value:.5f}\")\n    print(f\"  Params: {study.best_params}\")\n\nxgb_best_params = []\nfor i, study in enumerate(xgb_studies):\n    params = study.best_params.copy()\n    params.update({\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic'\n    })\n    xgb_best_params.append(params)\n    print(f\"\\nXGBoost config {i+1}:\")\n    for key, value in params.items():\n        print(f\"  {key}: {value}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"XGBOOST OPTIMIZATION SUMMARY\")\nprint(\"=\"*50)\nfor i, (study, params) in enumerate(zip(xgb_studies, xgb_best_params)):\n    print(f\"Model {i+1}: ROC-AUC = {study.best_value:.5f}\")\n\nconfig_0 = {\n    \n        'n_estimators': 1190,\n        'max_depth': 4,\n        'learning_rate': 0.08728281120585796,\n        'subsample': 0.7156366715152943,\n        'colsample_bytree': 0.7108648017761406,\n        'gamma': 0.9985164452475833,\n        'min_child_weight': 3,\n        'reg_lambda': 0.10609887430879072,\n        'reg_alpha': 0.8290380863851698,\n        'max_delta_step': 1,\n        'grow_policy': 'depthwise',\n        'max_leaves': 106,\n        'max_bin': 164,\n        'sampling_method': 'uniform',\n        'scale_pos_weight': 5.60450500326871,\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic'\n    \n}\n\nconfig_1 = {\n    \n        'n_estimators': 2257,\n        'max_depth': 6,\n        'learning_rate': 0.021008309623985727,\n        'subsample': 0.8912825302791664,\n        'colsample_bytree': 0.7195862505220774,\n        'gamma': 0.3033947642678359,\n        'min_child_weight': 10,\n        'reg_lambda': 1.9972821207432154,\n        'reg_alpha': 0.3724000278069234,\n        'max_delta_step': 5,\n        'grow_policy': 'depthwise',\n        'max_leaves': 130,\n        'max_bin': 252,\n        'sampling_method': 'uniform',\n        'scale_pos_weight': 3.3910516482718545,\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic'    \n}\n\nconfig_2 = {\n    \n        'n_estimators': 1234,\n        'max_depth': 8,\n        'learning_rate': 0.01184706113012854,\n        'subsample': 0.70655959785533,\n        'colsample_bytree': 0.8207416844980716,\n        'gamma': 0.9337868245354732,\n        'min_child_weight': 9,\n        'reg_lambda': 1.8845924077378502,\n        'reg_alpha': 0.4100461508041082,\n        'max_delta_step': 4,\n        'grow_policy': 'depthwise',\n        'max_leaves': 159,\n        'max_bin': 254,\n        'sampling_method': 'gradient_based',\n        'scale_pos_weight': 3.5556511418258503,\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic'\n    \n}\n\nconfig_3 = {\n    \n        'n_estimators': 2150,\n        'max_depth': 3,\n        'learning_rate': 0.08436799580035606,\n        'subsample': 0.9917906146194638,\n        'colsample_bytree': 0.915392731070202,\n        'gamma': 0.5717738334964471,\n        'min_child_weight': 6,\n        'reg_lambda': 1.3778372713157971,\n        'reg_alpha': 0.3455064906102807,\n        'max_delta_step': 2,\n        'grow_policy': 'depthwise',\n        'max_leaves': 160,\n        'max_bin': 256,\n        'sampling_method': 'gradient_based',\n        'scale_pos_weight': 7.397107205074961,\n        'tree_method': 'gpu_hist',\n        'predictor': 'gpu_predictor',\n        'eval_metric': 'auc',\n        'objective': 'binary:logistic'\n    \n}\n\nxgb_best_params.append(config_0)\nxgb_best_params.append(config_1)\nxgb_best_params.append(config_2)\nxgb_best_params.append(config_3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def optimize_lightgbm_classification(X, y, n_trials=15, cv=5):\n    \n    def objective(trial):\n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 3000),\n            'max_depth': trial.suggest_int('max_depth', 3, 12),\n            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.1, log=True),\n            'num_leaves': trial.suggest_int('num_leaves', 20, 128),\n            'min_child_samples': trial.suggest_int('min_child_samples', 5, 50),\n            'min_child_weight': trial.suggest_float('min_child_weight', 0.001, 0.1),\n            'feature_fraction': trial.suggest_float('feature_fraction', 0.5, 1.0),\n            'bagging_fraction': trial.suggest_float('bagging_fraction', 0.5, 1.0),\n            'bagging_freq': trial.suggest_int('bagging_freq', 0, 10),\n            'lambda_l1': trial.suggest_float('lambda_l1', 0, 1),\n            'lambda_l2': trial.suggest_float('lambda_l2', 0, 1),\n            'min_split_gain': trial.suggest_float('min_split_gain', 0, 0.2),\n            'path_smooth': trial.suggest_float('path_smooth', 0, 1),\n            'max_bin': trial.suggest_int('max_bin', 64, 255),\n            'extra_trees': trial.suggest_categorical('extra_trees', [True, False]),\n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'objective': 'binary',\n            'metric': 'auc',\n            'verbose': -1,\n            'scale_pos_weight': trial.suggest_float('scale_pos_weight', 0.1, 10.0)\n        }\n        \n        model = LGBMClassifier(**params)\n        \n        scores = cross_val_score(model, X, y, cv=cv, \n                               scoring='roc_auc')\n        \n        return scores.mean()\n    \n    study = optuna.create_study(direction='maximize') \n    study.optimize(objective, n_trials=n_trials)\n    \n    return study\n\n\nlgbm_studies = []\nfor i in range(3):\n    print(f\"\\nRunning LightGBM Classification optimization {i+1}/3\")\n    study = optimize_lightgbm_classification(X, y, n_trials=15)\n    lgbm_studies.append(study)\n    print(f\"Best trial {i+1}:\")\n    print(f\"  ROC-AUC: {study.best_value:.5f}\")\n    print(f\"  Params: {study.best_params}\")\n\nlgbm_best_params = []\nfor i, study in enumerate(lgbm_studies):\n    params = study.best_params.copy()\n    params.update({\n        'objective': 'binary',\n        'metric': 'auc',\n        'device': 'gpu',\n        'verbose': -1\n    })\n    lgbm_best_params.append(params)\n    print(f\"\\nLightGBM config {i+1}:\")\n    for key, value in params.items():\n        print(f\"  {key}: {value}\")\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"LIGHTGBM OPTIMIZATION SUMMARY\")\nprint(\"=\"*50)\nfor i, (study, params) in enumerate(zip(lgbm_studies, lgbm_best_params)):\n    print(f\"Model {i+1}: ROC-AUC = {study.best_value:.5f}\")\n\nconfig_0 = {\n    \n        'n_estimators': 2968,\n        'max_depth': 6,\n        'learning_rate': 0.017610848997737235,\n        'num_leaves': 20,\n        'min_child_samples': 5,\n        'min_child_weight': 0.07867583365026909,\n        'feature_fraction': 0.5126361649285935,\n        'bagging_fraction': 0.999106225684828,\n        'bagging_freq': 7,\n        'lambda_l1': 0.984648051201791,\n        'lambda_l2': 0.9560952146250232,\n        'min_split_gain': 0.1967277362694852,\n        'path_smooth': 0.014026385204785045,\n        'max_bin': 250,\n        'extra_trees': False,\n        'scale_pos_weight': 8.387051997229882,\n        'objective': 'binary',\n        'metric': 'auc',\n        'device': 'gpu',\n        'verbose': -1\n    \n}\n\nconfig_1 = {\n    \n        'n_estimators': 1876,\n        'max_depth': 3,\n        'learning_rate': 0.05707680994916189,\n        'num_leaves': 87,\n        'min_child_samples': 25,\n        'min_child_weight': 0.024582185232500813,\n        'feature_fraction': 0.6019476561921188,\n        'bagging_fraction': 0.8091766935240692,\n        'bagging_freq': 7,\n        'lambda_l1': 0.7525057611601782,\n        'lambda_l2': 0.7841594498355441,\n        'min_split_gain': 0.1902553146225533,\n        'path_smooth': 0.46157879112186484,\n        'max_bin': 182,\n        'extra_trees': False,\n        'scale_pos_weight': 3.563614770787167,\n        'objective': 'binary',\n        'metric': 'auc',\n        'device': 'gpu',\n        'verbose': -1\n    \n}\n\nconfig_2 = {\n    \n        'n_estimators': 988,\n        'max_depth': 8,\n        'learning_rate': 0.048781065327623226,\n        'num_leaves': 36,\n        'min_child_samples': 44,\n        'min_child_weight': 0.07104539569974595,\n        'feature_fraction': 0.5511355889937433,\n        'bagging_fraction': 0.8921162947901531,\n        'bagging_freq': 6,\n        'lambda_l1': 0.5939775981038105,\n        'lambda_l2': 0.3946539510111705,\n        'min_split_gain': 0.10752701341644968,\n        'path_smooth': 0.6142404799930528,\n        'max_bin': 249,\n        'extra_trees': False,\n        'scale_pos_weight': 1.6942486539570902,\n        'objective': 'binary',\n        'metric': 'auc',\n        'device': 'gpu',\n        'verbose': -1\n    \n}\n\nconfig_3 = {\n    \n        'n_estimators': 2918,\n        'max_depth': 9,\n        'learning_rate': 0.005459216695843719,\n        'num_leaves': 83,\n        'min_child_samples': 24,\n        'min_child_weight': 0.09946780130040314,\n        'feature_fraction': 0.775690283359843,\n        'bagging_fraction': 0.8302500189010505,\n        'bagging_freq': 10,\n        'lambda_l1': 0.7013647517774626,\n        'lambda_l2': 0.45882699446815334,\n        'min_split_gain': 0.08142167580357887,\n        'path_smooth': 0.20695599450389324,\n        'max_bin': 186,\n        'extra_trees': False,\n        'scale_pos_weight': 7.206269155440004,\n        'objective': 'binary',\n        'metric': 'auc',\n        'device': 'gpu',\n        'verbose': -1\n    \n}\n\nlgbm_best_params.append(config_0)\nlgbm_best_params.append(config_1)\nlgbm_best_params.append(config_2)\nlgbm_best_params.append(config_3)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def create_classification_ensemble(X, y, test, n_folds=7):\n    FOLDS = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n    \n    all_oof = {}\n    all_predictions = {}\n    models = []\n\n    for i, params in enumerate(catboost_best_params, 1):\n        models.append((f'cat_{i}', CatBoostClassifier(**params)))\n    \n    for i, params in enumerate(xgb_best_params, 1):\n        models.append((f'xgb_{i}', xgb.XGBClassifier(**params)))\n    \n    for i, params in enumerate(lgbm_best_params, 1):\n        models.append((f'lgb_{i}', LGBMClassifier(**params)))\n    \n    for name, model in models:\n        try:\n            print(f\"\\nTraining {name}...\")\n            oof = np.zeros(len(X))\n            pred = np.zeros(len(test))\n            \n            fold_auc_scores = []\n            \n            for fold, (trn_idx, val_idx) in enumerate(FOLDS.split(X, y)):\n                X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n                X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n                \n                model.fit(X_train, y_train)\n                \n                oof[val_idx] = model.predict_proba(X_val)[:, 1]\n                pred += model.predict_proba(test)[:, 1] / FOLDS.n_splits\n                \n                fold_auc = roc_auc_score(y_val, oof[val_idx])\n                fold_auc_scores.append(fold_auc)\n                print(f'{name} - Fold {fold} AUC: {fold_auc:.4f}')\n            \n            all_oof[name] = oof\n            all_predictions[name] = pred\n            \n            full_auc = roc_auc_score(y, oof)\n            mean_fold_auc = np.mean(fold_auc_scores)\n            std_fold_auc = np.std(fold_auc_scores)\n            \n            print(f'{name} - Full OOF AUC: {full_auc:.4f}')\n            print(f'{name} - Mean Fold AUC: {mean_fold_auc:.4f} ¬± {std_fold_auc:.4f}')\n            \n        except Exception as e:\n            print(f\"Error training {name}: {str(e)}\")\n            continue\n    \n    oof_df = pd.DataFrame(all_oof)\n    predictions_df = pd.DataFrame(all_predictions)\n    \n    oof_df['target'] = y.values\n    \n    model_performance = {}\n    for name in all_oof.keys():\n        auc_score = roc_auc_score(y, all_oof[name])\n        model_performance[name] = auc_score\n    \n    sorted_models = sorted(model_performance.items(), key=lambda x: x[1], reverse=True)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"MODEL PERFORMANCE RANKING (by ROC-AUC):\")\n    print(\"=\"*60)\n    for i, (name, auc) in enumerate(sorted_models, 1):\n        print(f\"{i:2d}. {name:20} AUC: {auc:.4f}\")\n    \n    model_info = {\n        'model_names': [name for name, _ in models],\n        'num_models': len(all_oof),\n        'features_used': list(X.columns),\n        'model_performance': model_performance,\n        'top_models': sorted_models[:10]\n    }\n    \n    return oof_df, predictions_df, model_info\n\nprint(\"üöÄ Creating Classification Ensemble...\")\noof_results, test_predictions, model_info = create_classification_ensemble(X, y, test)\n\nprint(\"\\nüéâ Modeling completed successfully!\")\nprint(f\"Trained {model_info['num_models']} models\")\nprint(\"OOF predictions shape:\", oof_results.shape)\nprint(\"Test predictions shape:\", test_predictions.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Mix","metadata":{}},{"cell_type":"code","source":"def create_optimal_ensemble(oof_results, test_predictions, y):\n   \n    oof_predictions = oof_results.drop(['target'], axis=1, errors='ignore')\n    y_true = oof_results['target'] if 'target' in oof_results else y\n    \n    ridge = Ridge(alpha=1.0)\n    ridge.fit(oof_predictions, y_true)\n    ridge_pred = ridge.predict(oof_predictions)\n    ridge_auc = roc_auc_score(y_true, ridge_pred)\n    \n    def objective(weights):\n        weighted_pred = np.dot(oof_predictions.values, weights)\n        return -roc_auc_score(y_true, weighted_pred)  \n    \n    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n    bounds = [(0, 1)] * len(oof_predictions.columns)\n    initial_weights = np.ones(len(oof_predictions.columns)) / len(oof_predictions.columns)\n    \n    result = minimize(objective, initial_weights, \n                     method='SLSQP', bounds=bounds, constraints=constraints)\n    optimized_weights = result.x\n    optimized_pred = np.dot(oof_predictions.values, optimized_weights)\n    optimized_auc = roc_auc_score(y_true, optimized_pred)\n    \n    if ridge_auc >= optimized_auc:\n        print(f\"Using Ridge (AUC: {ridge_auc:.4f})\")\n        test_pred = ridge.predict(test_predictions)\n        weights = ridge.coef_\n        best_auc = ridge_auc\n    else:\n        print(f\"Using Optimized Weights (AUC: {optimized_auc:.4f})\")\n        test_pred = np.dot(test_predictions.values, optimized_weights)\n        weights = optimized_weights\n        best_auc = optimized_auc\n    \n    weights = weights / weights.sum()\n    \n    return test_pred, weights, best_auc\n\ntest_pred, weights, best_auc = create_optimal_ensemble(oof_results, test_predictions, y)\n\nprint(f\"\\nBest Ensemble AUC: {best_auc:.4f}\")\nprint(\"Final weights:\")\nfor model, weight in zip(test_predictions.columns, weights):\n    print(f\"  {model}: {weight:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submit","metadata":{}},{"cell_type":"code","source":"sample = pd.read_csv('/kaggle/input/playground-series-s5e11/sample_submission.csv')\nsample['loan_paid_back'] = test_pred\nsample.to_csv('submission.csv', index=False)\nsample.head(10)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}